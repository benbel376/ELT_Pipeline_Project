

============================== 2022-07-27 21:07:37.125502 | c753ce90-4226-408f-8e2c-36a54a109654 ==============================
[0m21:07:37.125612 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:07:37.133440 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:07:37.134558 [debug] [MainThread]: Tracking: tracking
[0m21:07:37.321875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b694f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b695e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b695b0>]}
[0m21:07:37.460974 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m21:07:37.462984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b7e760>]}
[0m21:07:37.527585 [debug] [MainThread]: Parsing macros/relations.sql
[0m21:07:37.543261 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:07:37.695602 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:07:37.704743 [debug] [MainThread]: Parsing macros/materializations/snapshot_merge.sql
[0m21:07:37.712354 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:07:37.714921 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:07:37.720765 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:07:37.723980 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:07:37.729198 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:07:37.755901 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:07:37.762606 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:07:37.773383 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:07:37.791589 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:07:37.799439 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:07:37.853495 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:07:38.016788 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:07:38.118960 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:07:38.207479 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:07:38.381032 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:07:38.459703 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:07:38.503415 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:07:38.643316 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:07:38.747925 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:07:38.754874 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:07:38.769674 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:07:38.815611 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:07:38.831205 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:07:38.841413 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:07:38.905499 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:07:38.950159 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:07:38.982008 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:07:39.065959 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:07:39.109511 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:07:39.118655 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:07:39.134203 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:07:39.155926 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:07:39.179953 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:07:39.302678 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:07:39.370175 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:07:39.430497 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:07:39.473669 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:07:39.598146 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:07:39.622897 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:07:39.635905 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:07:39.657251 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:07:39.665171 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:07:39.676900 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:07:39.688048 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:07:39.695171 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:07:39.712345 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:07:39.726774 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:07:39.745963 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:07:39.781097 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:07:39.785554 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:07:39.811219 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:07:39.817874 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:07:39.832006 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:07:39.846837 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:07:39.937438 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:07:39.953695 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:07:39.973536 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:07:39.989986 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:07:40.020142 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:07:40.047142 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:07:40.065701 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:07:40.074993 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:07:40.140629 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:07:40.249404 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:07:40.269520 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:07:40.298706 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:07:40.316054 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:07:42.789841 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m21:07:42.945704 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m21:07:42.972925 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:07:43.001028 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:07:43.017967 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:07:43.488917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6a9eee0>]}
[0m21:07:43.516397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6619a30>]}
[0m21:07:43.517794 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:07:43.519899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6a26fd0>]}
[0m21:07:43.526597 [info ] [MainThread]: 
[0m21:07:43.529093 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:07:43.534439 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:07:43.596812 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:07:43.597741 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:07:43.598476 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:07:43.620927 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:07:43.626854 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:07:43.649010 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:07:43.711653 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:07:43.712799 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:07:43.713697 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:07:43.726782 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m21:07:43.729289 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:07:43.730189 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:07:43.757106 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.03 seconds
[0m21:07:43.765747 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:07:43.772362 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:07:43.790453 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.791379 [debug] [MainThread]: On master: BEGIN
[0m21:07:43.797666 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:07:43.813454 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:07:43.815368 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.816748 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:07:43.853589 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:07:43.867461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6a9ed00>]}
[0m21:07:43.872833 [debug] [MainThread]: On master: ROLLBACK
[0m21:07:43.874456 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.875381 [debug] [MainThread]: On master: BEGIN
[0m21:07:43.884905 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:07:43.886641 [debug] [MainThread]: On master: COMMIT
[0m21:07:43.892079 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.893849 [debug] [MainThread]: On master: COMMIT
[0m21:07:43.899499 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:07:43.906314 [debug] [MainThread]: On master: Close
[0m21:07:43.917659 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:07:43.919299 [info ] [MainThread]: 
[0m21:07:43.968515 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:07:43.969906 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:07:43.974976 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:07:43.975920 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:07:43.976853 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:07:43.989039 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:07:43.990884 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:43.991899 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:07:44.360302 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:07:44.366988 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:07:44.367928 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:07:44.368834 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:07:44.383186 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:07:44.384649 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:07:44.385768 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."warehouse"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:07:44.408225 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from "warehouse"."warehouse"."source"
                          ^

[0m21:07:44.409251 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:07:44.411395 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:44.412458 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:07:44.414413 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from "warehouse"."warehouse"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:07:44.415711 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff44ce910>]}
[0m21:07:44.423606 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.44s]
[0m21:07:44.434923 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:07:44.436070 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:07:44.444036 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:07:44.450951 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.451891 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:07:44.461019 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:07:44.486059 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:07:44.487577 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:44.497355 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:07:44.543609 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:07:44.561434 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.566882 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:07:44.567656 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:07:44.587775 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:07:44.588842 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.589588 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:07:44.595609 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.0 seconds
[0m21:07:44.679904 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.692861 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:07:44.695142 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:07:44.762432 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.763329 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:07:44.781549 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.02 seconds
[0m21:07:45.211528 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:07:45.218537 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:45.228422 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:07:45.245909 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:07:45.280322 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:45.281410 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:07:45.302072 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:07:45.305997 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:45.307053 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:07:45.320343 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff65928b0>]}
[0m21:07:45.321984 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.87s]
[0m21:07:45.336723 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:07:45.337996 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:07:45.345320 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:07:45.353047 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:07:45.358689 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:07:45.372331 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:07:45.375525 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:07:45.380528 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:07:45.385990 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:07:45.390165 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.391161 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:07:45.397286 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:07:45.438242 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:07:45.445511 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:45.446640 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:07:45.671627 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:07:45.677851 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.678848 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:07:45.681322 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:07:45.696420 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:07:45.697384 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.698175 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:07:45.706028 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.0 seconds
[0m21:07:45.718274 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.719178 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:07:45.731888 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:07:45.755824 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:07:45.756807 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.757658 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:07:45.778728 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:07:45.799449 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.800625 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:07:45.809268 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:07:45.818271 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:45.824318 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:07:45.829411 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff44ced60>]}
[0m21:07:45.836666 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.44s]
[0m21:07:45.841517 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:07:45.863284 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:07:45.864955 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:45.866194 [debug] [MainThread]: On master: BEGIN
[0m21:07:45.867069 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:07:45.882766 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:07:45.884097 [debug] [MainThread]: On master: COMMIT
[0m21:07:45.888119 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:45.892204 [debug] [MainThread]: On master: COMMIT
[0m21:07:45.901053 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:07:45.908327 [debug] [MainThread]: On master: Close
[0m21:07:45.916110 [info ] [MainThread]: 
[0m21:07:45.917876 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.39 seconds (2.39s).
[0m21:07:45.924605 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:07:45.931959 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:07:45.932887 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:07:46.022111 [info ] [MainThread]: 
[0m21:07:46.026239 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:07:46.053084 [info ] [MainThread]: 
[0m21:07:46.059244 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:07:46.069292 [error] [MainThread]:   relation "warehouse.source" does not exist
[0m21:07:46.078576 [error] [MainThread]:   LINE 8:     select * from "warehouse"."warehouse"."source"
[0m21:07:46.086814 [error] [MainThread]:                             ^
[0m21:07:46.090253 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:07:46.102003 [info ] [MainThread]: 
[0m21:07:46.145309 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:07:46.162754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff658bf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b7e6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff44ced60>]}


============================== 2022-07-27 21:12:17.533748 | c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c ==============================
[0m21:12:17.533882 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:12:17.552806 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:12:17.560519 [debug] [MainThread]: Tracking: tracking
[0m21:12:17.801662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016cb0520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016cb0610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016cb05e0>]}
[0m21:12:18.011181 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m21:12:18.013120 [debug] [MainThread]: Partial parsing: added file: dbt_://models/traffic_models/schema.yml
[0m21:12:18.024887 [debug] [MainThread]: Partial parsing: deleted source source.dbt_.traffic_source.source
[0m21:12:18.095307 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:12:18.157689 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:12:18.198725 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:12:18.334661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016722eb0>]}
[0m21:12:18.366246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016c29b50>]}
[0m21:12:18.367794 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:12:18.370384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016b9a100>]}
[0m21:12:18.378231 [info ] [MainThread]: 
[0m21:12:18.382145 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:12:18.389848 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:12:18.445581 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:12:18.446700 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:12:18.448366 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:12:18.474407 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.03 seconds
[0m21:12:18.479491 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:12:18.503716 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:12:18.609026 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:12:18.616062 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:12:18.628125 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:12:18.670636 [debug] [ThreadPool]: SQL status: BEGIN in 0.04 seconds
[0m21:12:18.671591 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:12:18.672660 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:12:18.683510 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:12:18.706182 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:12:18.713017 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:12:18.743676 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.750703 [debug] [MainThread]: On master: BEGIN
[0m21:12:18.751450 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:12:18.782191 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
[0m21:12:18.785241 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.789834 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:12:18.835736 [debug] [MainThread]: SQL status: SELECT 1 in 0.04 seconds
[0m21:12:18.840005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016b67e80>]}
[0m21:12:18.841331 [debug] [MainThread]: On master: ROLLBACK
[0m21:12:18.850754 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.851702 [debug] [MainThread]: On master: BEGIN
[0m21:12:18.867628 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:12:18.869156 [debug] [MainThread]: On master: COMMIT
[0m21:12:18.874725 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.883465 [debug] [MainThread]: On master: COMMIT
[0m21:12:18.891721 [debug] [MainThread]: SQL status: COMMIT in 0.01 seconds
[0m21:12:18.892796 [debug] [MainThread]: On master: Close
[0m21:12:18.894788 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:12:18.898149 [info ] [MainThread]: 
[0m21:12:18.950762 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:12:18.952075 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:12:18.955627 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:12:18.956668 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:12:18.957540 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:12:18.985039 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:12:18.990134 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:18.992315 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:12:19.273980 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:12:19.275996 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:12:19.277196 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:12:19.278081 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:12:19.295095 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:12:19.296055 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:12:19.297103 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."warehouse"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:12:19.300401 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from "warehouse"."warehouse"."source"
                          ^

[0m21:12:19.302023 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:12:19.309457 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.310959 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:12:19.317565 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from "warehouse"."warehouse"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:12:19.320007 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff014634490>]}
[0m21:12:19.324315 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.37s]
[0m21:12:19.329999 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:12:19.334064 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:12:19.337859 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:12:19.346507 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.350385 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:12:19.351629 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:12:19.387827 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:12:19.397331 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.398294 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:12:19.422842 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:12:19.425687 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.426606 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:12:19.427310 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:12:19.445859 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:12:19.449003 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.451479 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:12:19.458600 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:12:19.515315 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.516474 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:12:19.519163 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:12:19.545612 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.550819 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:12:19.554039 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:12:19.692905 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:12:19.693847 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.694570 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:12:19.722038 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:12:19.785405 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.786338 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:12:19.811969 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:12:19.815776 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.816862 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:12:19.818934 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0166c47f0>]}
[0m21:12:19.820532 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.47s]
[0m21:12:19.831942 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:12:19.835264 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:12:19.839605 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:12:19.842758 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:12:19.844260 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:12:19.845574 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:12:19.847975 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:12:19.849860 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:12:19.851389 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:12:19.854732 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:19.856040 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:12:19.858128 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:12:19.867247 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:12:19.869269 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.870438 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:12:20.142009 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:12:20.143641 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.144584 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:12:20.152237 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:12:20.171412 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:12:20.174068 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.177398 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:12:20.185599 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:12:20.195739 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.196759 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:12:20.199554 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:12:20.206233 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:12:20.207408 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.208190 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:12:20.243881 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:12:20.251177 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.252449 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:12:20.255517 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:12:20.264495 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:20.265488 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:12:20.275699 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff01565c490>]}
[0m21:12:20.277235 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.42s]
[0m21:12:20.285613 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:12:20.290628 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:12:20.292598 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:20.293585 [debug] [MainThread]: On master: BEGIN
[0m21:12:20.294376 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:12:20.353270 [debug] [MainThread]: SQL status: BEGIN in 0.06 seconds
[0m21:12:20.354829 [debug] [MainThread]: On master: COMMIT
[0m21:12:20.358535 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:20.359580 [debug] [MainThread]: On master: COMMIT
[0m21:12:20.361368 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:12:20.362347 [debug] [MainThread]: On master: Close
[0m21:12:20.371104 [info ] [MainThread]: 
[0m21:12:20.374203 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 1.99 seconds (1.99s).
[0m21:12:20.377134 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:12:20.378082 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:12:20.382686 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:12:20.413589 [info ] [MainThread]: 
[0m21:12:20.416482 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:12:20.419449 [info ] [MainThread]: 
[0m21:12:20.422847 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:12:20.426274 [error] [MainThread]:   relation "warehouse.source" does not exist
[0m21:12:20.429625 [error] [MainThread]:   LINE 8:     select * from "warehouse"."warehouse"."source"
[0m21:12:20.432445 [error] [MainThread]:                             ^
[0m21:12:20.434275 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:12:20.438661 [info ] [MainThread]: 
[0m21:12:20.442458 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:12:20.445594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016b79dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff01458cb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff01458c610>]}


============================== 2022-07-27 21:13:01.384874 | 660e18c5-7fc5-4858-8f8b-9220ee6122f7 ==============================
[0m21:13:01.385003 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:13:01.393056 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:13:01.396114 [debug] [MainThread]: Tracking: tracking
[0m21:13:01.624382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09c684c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09c685b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09c68580>]}
[0m21:13:02.015334 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:13:02.023880 [debug] [MainThread]: Partial parsing: deleted source source.dbt_.traffic_source.source
[0m21:13:02.031148 [debug] [MainThread]: Partial parsing: update schema file: dbt_://models/traffic_models/schema.yml
[0m21:13:02.172837 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:13:02.281852 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:13:02.294446 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:13:02.445020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c096dbd60>]}
[0m21:13:02.505326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c096a87f0>]}
[0m21:13:02.506716 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:13:02.516754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09b4cf70>]}
[0m21:13:02.541888 [info ] [MainThread]: 
[0m21:13:02.548473 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:13:02.585475 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:13:02.752009 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:13:02.754800 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:13:02.757765 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:13:02.792066 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.03 seconds
[0m21:13:02.798437 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:13:02.810291 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:13:02.869363 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:13:02.870266 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:13:02.872469 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:13:02.896735 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:13:02.897687 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:13:02.900290 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:13:02.910053 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:13:02.914147 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:13:02.916526 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:13:02.943611 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:02.948037 [debug] [MainThread]: On master: BEGIN
[0m21:13:02.950610 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:13:02.972921 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:13:02.973868 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:02.974605 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:13:03.025240 [debug] [MainThread]: SQL status: SELECT 1 in 0.05 seconds
[0m21:13:03.030443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09aec9a0>]}
[0m21:13:03.031737 [debug] [MainThread]: On master: ROLLBACK
[0m21:13:03.037414 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:03.039261 [debug] [MainThread]: On master: BEGIN
[0m21:13:03.050257 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:13:03.053128 [debug] [MainThread]: On master: COMMIT
[0m21:13:03.054002 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:03.054707 [debug] [MainThread]: On master: COMMIT
[0m21:13:03.063499 [debug] [MainThread]: SQL status: COMMIT in 0.01 seconds
[0m21:13:03.065957 [debug] [MainThread]: On master: Close
[0m21:13:03.075030 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:13:03.096822 [info ] [MainThread]: 
[0m21:13:03.130486 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:13:03.131780 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:13:03.137167 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:13:03.138371 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:13:03.139254 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:13:03.151564 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:13:03.153201 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:03.154799 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:13:03.514738 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:13:03.519432 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:13:03.523092 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:13:03.524120 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:13:03.542654 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:13:03.545078 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:13:03.546280 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:13:03.549749 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:13:03.557473 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:13:03.562969 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:03.564125 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:13:03.566021 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:13:03.578873 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c085e2e20>]}
[0m21:13:03.585107 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.44s]
[0m21:13:03.596421 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:13:03.601889 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:13:03.605585 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:13:03.613610 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.624350 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:13:03.626319 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:13:03.651626 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:13:03.658885 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:03.661739 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:13:03.703769 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:13:03.714299 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.717816 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:13:03.718937 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:13:03.751296 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:13:03.753677 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.755123 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:13:03.761567 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:13:03.817946 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.824443 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:13:03.826729 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:13:03.851813 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.855471 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:13:03.861909 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:13:04.242211 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:13:04.243146 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:04.264584 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:13:04.283383 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:13:04.335963 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:04.337606 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:13:04.361378 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:13:04.365309 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:04.366301 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:13:04.373799 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c085e2700>]}
[0m21:13:04.375331 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.76s]
[0m21:13:04.397113 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:13:04.400971 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:13:04.402912 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:13:04.417865 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:13:04.419340 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:13:04.422628 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:13:04.462873 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:13:04.464875 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:13:04.467561 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:13:04.476379 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.478651 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:13:04.480003 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:13:04.490141 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:13:04.491966 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:04.493972 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:13:04.661896 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:13:04.671454 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.672965 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:13:04.674679 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:13:04.698317 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:13:04.702685 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.709071 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:13:04.717738 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:13:04.729245 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.730834 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:13:04.733323 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:13:04.740991 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:13:04.742072 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.745391 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:13:04.760404 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
[0m21:13:04.775682 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.788632 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:13:04.791030 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:13:04.796056 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:04.797328 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:13:04.808546 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c086153d0>]}
[0m21:13:04.813636 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.33s]
[0m21:13:04.823188 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:13:04.829338 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:13:04.830881 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:04.832424 [debug] [MainThread]: On master: BEGIN
[0m21:13:04.834033 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:13:04.878083 [debug] [MainThread]: SQL status: BEGIN in 0.04 seconds
[0m21:13:04.880643 [debug] [MainThread]: On master: COMMIT
[0m21:13:04.881419 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:04.882110 [debug] [MainThread]: On master: COMMIT
[0m21:13:04.884557 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:13:04.885928 [debug] [MainThread]: On master: Close
[0m21:13:04.893426 [info ] [MainThread]: 
[0m21:13:04.903784 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.35 seconds (2.35s).
[0m21:13:04.909047 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:13:04.909864 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:13:04.912811 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:13:05.008576 [info ] [MainThread]: 
[0m21:13:05.010688 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:13:05.020793 [info ] [MainThread]: 
[0m21:13:05.023205 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:13:05.027019 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:13:05.032420 [error] [MainThread]:   LINE 8:     select * from "warehouse"."traffic_source"."source"
[0m21:13:05.043423 [error] [MainThread]:                             ^
[0m21:13:05.045035 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:13:05.057606 [info ] [MainThread]: 
[0m21:13:05.066359 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:13:05.074371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09b30d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c08546d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c08546a90>]}


============================== 2022-07-27 21:17:00.008397 | 3a2f7873-0cba-4436-946e-fc7cedcbf49b ==============================
[0m21:17:00.008536 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:17:00.018206 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:17:00.024426 [debug] [MainThread]: Tracking: tracking
[0m21:17:00.247309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ee4d5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ee4d6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ee4d670>]}
[0m21:17:00.587397 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:17:00.596495 [debug] [MainThread]: Partial parsing: deleted source source.dbt_.traffic_source.source
[0m21:17:00.599331 [debug] [MainThread]: Partial parsing: update schema file: dbt_://models/traffic_models/schema.yml
[0m21:17:00.755666 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:17:00.815128 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:17:00.828140 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:17:00.937691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e8bf070>]}
[0m21:17:00.967682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e88d850>]}
[0m21:17:00.969426 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:17:00.971219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ed360a0>]}
[0m21:17:00.978013 [info ] [MainThread]: 
[0m21:17:00.980691 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:17:00.985404 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:17:01.026280 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:17:01.032413 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:17:01.033537 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:17:01.049066 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:17:01.055088 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:17:01.086831 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:17:01.185282 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:17:01.194821 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:17:01.196286 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:17:01.225283 [debug] [ThreadPool]: SQL status: BEGIN in 0.03 seconds
[0m21:17:01.228869 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:17:01.237260 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:17:01.249532 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:17:01.254517 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:17:01.261359 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:17:01.312018 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.321087 [debug] [MainThread]: On master: BEGIN
[0m21:17:01.321982 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:17:01.334752 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:17:01.336263 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.337186 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:17:01.364446 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:17:01.368651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e8690a0>]}
[0m21:17:01.370146 [debug] [MainThread]: On master: ROLLBACK
[0m21:17:01.372203 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.373271 [debug] [MainThread]: On master: BEGIN
[0m21:17:01.376546 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:17:01.377761 [debug] [MainThread]: On master: COMMIT
[0m21:17:01.378705 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.380232 [debug] [MainThread]: On master: COMMIT
[0m21:17:01.381810 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:17:01.382937 [debug] [MainThread]: On master: Close
[0m21:17:01.390276 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:17:01.392066 [info ] [MainThread]: 
[0m21:17:01.403220 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:17:01.404657 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:17:01.406865 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:17:01.407794 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:17:01.409066 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:17:01.419667 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:17:01.421585 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:01.422766 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:17:01.671993 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:17:01.685161 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:17:01.686920 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:17:01.687994 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:17:01.717183 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:17:01.718177 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:17:01.718914 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:17:01.722669 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:17:01.732861 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:17:01.736088 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:01.741005 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:17:01.752682 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:17:01.754272 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05c7cea60>]}
[0m21:17:01.755990 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.35s]
[0m21:17:01.761698 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:17:01.766361 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:17:01.768418 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:17:01.781411 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:01.790345 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:17:01.792379 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:17:01.817851 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:17:01.826657 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:01.829229 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:17:01.906227 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:17:01.913875 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:01.916372 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:17:01.920631 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:17:01.935603 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:17:01.941941 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:01.943064 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:17:01.948986 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.0 seconds
[0m21:17:01.999530 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.005161 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:17:02.009045 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:17:02.025972 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.029899 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:17:02.033705 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:17:02.223148 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:17:02.227761 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.230009 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:17:02.242347 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
[0m21:17:02.310678 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.318105 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:17:02.354086 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.03 seconds
[0m21:17:02.358659 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:02.368395 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:17:02.387436 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e847eb0>]}
[0m21:17:02.389277 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.61s]
[0m21:17:02.391936 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:17:02.396518 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:17:02.398156 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:17:02.411660 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:17:02.413293 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:17:02.414595 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:17:02.416821 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:17:02.420248 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:17:02.424049 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:17:02.429488 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.435687 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:17:02.439640 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:17:02.459166 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:17:02.462998 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:02.478556 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:17:02.705778 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:17:02.712326 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.714319 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:17:02.716292 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:17:02.731402 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:17:02.733570 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.734934 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:17:02.743291 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:17:02.759750 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.761245 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:17:02.763445 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:17:02.770916 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:17:02.771995 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.772937 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:17:02.808832 [debug] [Thread-1  ]: SQL status: COMMIT in 0.04 seconds
[0m21:17:02.818637 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.819724 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:17:02.823075 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:17:02.832060 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:02.836100 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:17:02.848975 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05d7fa520>]}
[0m21:17:02.850664 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.42s]
[0m21:17:02.853819 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:17:02.867109 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:17:02.868352 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:02.869331 [debug] [MainThread]: On master: BEGIN
[0m21:17:02.870174 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:17:02.881520 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:17:02.883508 [debug] [MainThread]: On master: COMMIT
[0m21:17:02.884720 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:02.885612 [debug] [MainThread]: On master: COMMIT
[0m21:17:02.887128 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:17:02.888274 [debug] [MainThread]: On master: Close
[0m21:17:02.897980 [info ] [MainThread]: 
[0m21:17:02.899759 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 1.92 seconds (1.92s).
[0m21:17:02.903333 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:17:02.906338 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:17:02.908451 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:17:02.967193 [info ] [MainThread]: 
[0m21:17:02.973246 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:17:02.975025 [info ] [MainThread]: 
[0m21:17:02.976606 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:17:02.980883 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:17:02.984560 [error] [MainThread]:   LINE 8:     select * from "warehouse"."traffic_source"."source"
[0m21:17:02.993170 [error] [MainThread]:                             ^
[0m21:17:02.994718 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:17:02.996241 [info ] [MainThread]: 
[0m21:17:02.997768 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:17:02.999515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ed15e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05c72e220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05c72e9a0>]}


============================== 2022-07-27 21:18:08.486201 | a62dc8d8-f3fd-46a7-b2c1-e2759cd22814 ==============================
[0m21:18:08.486321 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:18:08.512519 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:18:08.514710 [debug] [MainThread]: Tracking: tracking
[0m21:18:08.749342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa120460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa120550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa120520>]}
[0m21:18:09.147596 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:18:09.149689 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/dim_types.sql
[0m21:18:09.217613 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:18:09.357150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f9fc9130>]}
[0m21:18:09.385499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa09a8e0>]}
[0m21:18:09.387200 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:18:09.389032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f9fc8f10>]}
[0m21:18:09.404600 [info ] [MainThread]: 
[0m21:18:09.417965 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:18:09.429240 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:18:09.495026 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:18:09.496140 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:18:09.497091 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:09.515008 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:18:09.529788 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:18:09.546577 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:18:09.651523 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:18:09.652495 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:18:09.656777 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:09.671846 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:18:09.675374 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:18:09.676125 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:18:09.688185 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:18:09.694025 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:18:09.695650 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:18:09.742603 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.745577 [debug] [MainThread]: On master: BEGIN
[0m21:18:09.749878 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:18:09.806291 [debug] [MainThread]: SQL status: BEGIN in 0.06 seconds
[0m21:18:09.807226 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.807925 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:18:09.841271 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:18:09.845202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa137af0>]}
[0m21:18:09.847659 [debug] [MainThread]: On master: ROLLBACK
[0m21:18:09.849417 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.850315 [debug] [MainThread]: On master: BEGIN
[0m21:18:09.853225 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:18:09.854222 [debug] [MainThread]: On master: COMMIT
[0m21:18:09.855413 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.858131 [debug] [MainThread]: On master: COMMIT
[0m21:18:09.860522 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:18:09.862810 [debug] [MainThread]: On master: Close
[0m21:18:09.873631 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:09.875574 [info ] [MainThread]: 
[0m21:18:09.894875 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:18:09.896356 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:18:09.899471 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:18:09.900795 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:18:09.901732 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:18:09.912615 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:18:09.914309 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:09.917839 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:18:10.204590 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:18:10.208478 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:18:10.209403 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:18:10.210214 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:10.233057 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:18:10.233992 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:18:10.235485 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:18:10.242680 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from "warehouse"."source"
                          ^

[0m21:18:10.245295 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:18:10.248827 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.252752 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:18:10.262922 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from "warehouse"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:18:10.264358 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f82c9910>]}
[0m21:18:10.269481 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.37s]
[0m21:18:10.271665 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:18:10.275410 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:18:10.277477 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:18:10.281843 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.284383 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:18:10.286592 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:18:10.305761 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:18:10.310999 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.313530 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:18:10.349161 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:18:10.360062 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.362592 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:18:10.364848 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:10.381635 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:18:10.386115 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.387674 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:18:10.396536 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:18:10.460143 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.469972 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:18:10.481691 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:18:10.511499 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.515155 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:18:10.521079 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:18:10.710601 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:18:10.711713 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.712730 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:18:10.726869 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
[0m21:18:10.780072 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.788904 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:18:10.816207 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:18:10.821116 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.822275 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:18:10.825881 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f8af61f0>]}
[0m21:18:10.827816 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.55s]
[0m21:18:10.840595 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:18:10.843781 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:18:10.845972 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:18:10.850578 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:18:10.869946 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:18:10.874893 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:18:10.880340 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:18:10.881697 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:18:10.883841 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:18:10.890344 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:10.891514 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:18:10.892497 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:18:10.902929 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:18:10.904804 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.905989 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:18:11.098543 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:18:11.100590 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.102397 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:18:11.103346 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:11.125704 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:18:11.132043 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.133988 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:18:11.147332 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:18:11.166252 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.167165 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:18:11.172968 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:18:11.190304 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:18:11.202400 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.205429 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:18:11.269371 [debug] [Thread-1  ]: SQL status: COMMIT in 0.06 seconds
[0m21:18:11.277925 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.278833 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:18:11.280642 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:18:11.288620 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:11.289616 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:18:11.292860 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f82a9d30>]}
[0m21:18:11.298114 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.40s]
[0m21:18:11.302472 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:18:11.309297 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:18:11.310320 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:11.311076 [debug] [MainThread]: On master: BEGIN
[0m21:18:11.311752 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:18:11.329131 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:18:11.330918 [debug] [MainThread]: On master: COMMIT
[0m21:18:11.333136 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:11.333902 [debug] [MainThread]: On master: COMMIT
[0m21:18:11.338814 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:18:11.339777 [debug] [MainThread]: On master: Close
[0m21:18:11.364557 [info ] [MainThread]: 
[0m21:18:11.366600 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 1.95 seconds (1.95s).
[0m21:18:11.372358 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:11.373301 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:18:11.373992 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:18:11.423871 [info ] [MainThread]: 
[0m21:18:11.450902 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:18:11.456720 [info ] [MainThread]: 
[0m21:18:11.458678 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:18:11.465415 [error] [MainThread]:   relation "warehouse.source" does not exist
[0m21:18:11.469878 [error] [MainThread]:   LINE 8:     select * from "warehouse"."source"
[0m21:18:11.471286 [error] [MainThread]:                             ^
[0m21:18:11.472596 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:18:11.473928 [info ] [MainThread]: 
[0m21:18:11.486186 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:18:11.487939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa05a3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa05a3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa05a370>]}


============================== 2022-07-27 21:20:50.147062 | 9b8add4c-8101-4c32-b3f0-892f2e8a0370 ==============================
[0m21:20:50.147178 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:20:50.162022 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:20:50.163893 [debug] [MainThread]: Tracking: tracking
[0m21:20:50.325722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8bf1520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8bf1610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8bf15e0>]}
[0m21:20:50.609331 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:20:50.611186 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/dim_types.sql
[0m21:20:50.738683 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:20:50.913565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8ad01f0>]}
[0m21:20:50.942038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8baa9a0>]}
[0m21:20:50.943734 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:20:50.945860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8ad2fd0>]}
[0m21:20:50.965849 [info ] [MainThread]: 
[0m21:20:50.972908 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:20:50.987882 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:20:51.066193 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:20:51.067131 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:20:51.067847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:20:51.091229 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:20:51.096348 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:20:51.110289 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:20:51.159376 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:20:51.160673 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:20:51.161614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:20:51.180022 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:20:51.181159 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:20:51.188955 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:20:51.206194 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.02 seconds
[0m21:20:51.213291 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:20:51.224375 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:20:51.257069 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.263762 [debug] [MainThread]: On master: BEGIN
[0m21:20:51.269570 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:20:51.289049 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:20:51.293795 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.296839 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:20:51.331827 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:20:51.336105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8c07760>]}
[0m21:20:51.337413 [debug] [MainThread]: On master: ROLLBACK
[0m21:20:51.339185 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.340094 [debug] [MainThread]: On master: BEGIN
[0m21:20:51.342636 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:20:51.349366 [debug] [MainThread]: On master: COMMIT
[0m21:20:51.350901 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.352527 [debug] [MainThread]: On master: COMMIT
[0m21:20:51.356094 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:20:51.357099 [debug] [MainThread]: On master: Close
[0m21:20:51.366724 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:20:51.371111 [info ] [MainThread]: 
[0m21:20:51.438489 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:20:51.440017 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:20:51.442450 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:20:51.443615 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:20:51.444504 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:20:51.458525 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:20:51.460317 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:51.472759 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:20:51.825281 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:20:51.828847 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:51.835753 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:20:51.836568 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:51.855978 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:51.857049 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:51.857958 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:20:52.016590 [debug] [Thread-1  ]: SQL status: SELECT 6 in 0.16 seconds
[0m21:20:52.063777 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:52.075730 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
[0m21:20:52.081196 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:20:52.221370 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:20:52.222479 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:52.223365 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:20:52.248360 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:20:52.267250 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:52.268408 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
[0m21:20:52.270404 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m21:20:52.274465 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.275630 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:20:52.284761 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de85d8190>]}
[0m21:20:52.286519 [info ] [Thread-1  ]: 1 of 5 OK created table model warehouse.dim_types .............................. [[32mSELECT 6[0m in 0.84s]
[0m21:20:52.291066 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:20:52.300849 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:20:52.303364 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:20:52.311145 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.312854 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:20:52.316455 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:20:52.338155 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:20:52.339982 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.341188 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:20:52.383706 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:20:52.388693 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.389928 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:20:52.390863 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:52.411931 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:52.415319 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.418031 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:20:52.430307 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:20:52.454127 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.455008 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:20:52.467326 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:20:52.509337 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.510225 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:20:52.516850 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:20:52.570694 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:20:52.582401 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.583143 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:20:52.614473 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:20:52.622089 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.622972 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:20:52.659675 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.04 seconds
[0m21:20:52.663806 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.664853 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:20:52.666568 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b01340>]}
[0m21:20:52.672760 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.36s]
[0m21:20:52.677248 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:20:52.681870 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:20:52.685755 [info ] [Thread-1  ]: 3 of 5 START table model warehouse.fct_summary ................................. [RUN]
[0m21:20:52.707036 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:20:52.709415 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:20:52.710202 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:20:52.740048 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:20:52.741554 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.742455 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:20:52.770921 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_summary"
[0m21:20:52.773574 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:20:52.774419 [debug] [Thread-1  ]: On model.dbt_.fct_summary: BEGIN
[0m21:20:52.779632 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:52.794999 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:52.796520 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:20:52.798253 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:20:52.802315 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 9:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:20:52.803289 [debug] [Thread-1  ]: On model.dbt_.fct_summary: ROLLBACK
[0m21:20:52.810635 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.814113 [debug] [Thread-1  ]: On model.dbt_.fct_summary: Close
[0m21:20:52.822993 [debug] [Thread-1  ]: Database Error in model fct_summary (models/traffic_models/fct_summary.sql)
  relation "traffic_source.source" does not exist
  LINE 9:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/fct_summary.sql
[0m21:20:52.824283 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de856f100>]}
[0m21:20:52.829493 [error] [Thread-1  ]: 3 of 5 ERROR creating table model warehouse.fct_summary ........................ [[31mERROR[0m in 0.12s]
[0m21:20:52.834051 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:20:52.835108 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:20:52.857301 [info ] [Thread-1  ]: 4 of 5 START table model warehouse.fct_trajectory .............................. [RUN]
[0m21:20:52.865083 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:20:52.866400 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:20:52.868348 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:20:52.883940 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:20:52.885680 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.900068 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:20:52.955069 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_trajectory"
[0m21:20:52.962369 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:20:52.967124 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: BEGIN
[0m21:20:52.970222 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:52.987208 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:52.990606 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:20:52.992040 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:20:53.008227 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 9:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:20:53.016013 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: ROLLBACK
[0m21:20:53.044368 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:53.048434 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: Close
[0m21:20:53.050568 [debug] [Thread-1  ]: Database Error in model fct_trajectory (models/traffic_models/fct_trajectory.sql)
  relation "traffic_source.source" does not exist
  LINE 9:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/fct_trajectory.sql
[0m21:20:53.052022 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8513dc0>]}
[0m21:20:53.053696 [error] [Thread-1  ]: 4 of 5 ERROR creating table model warehouse.fct_trajectory ..................... [[31mERROR[0m in 0.19s]
[0m21:20:53.057669 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:20:53.058997 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:20:53.060634 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:20:53.066496 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.067589 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:20:53.069006 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:20:53.077976 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:20:53.087499 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:53.090713 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:20:53.257935 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:20:53.264810 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.266893 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:20:53.267785 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:53.303728 [debug] [Thread-1  ]: SQL status: BEGIN in 0.04 seconds
[0m21:20:53.304923 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.324356 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:20:53.332263 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:20:53.341664 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.342539 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:20:53.351910 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:20:53.365371 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:20:53.366281 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.366976 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:20:53.391250 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:20:53.415188 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.416068 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:20:53.425702 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.01 seconds
[0m21:20:53.431737 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:53.435042 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:20:53.445872 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8532d60>]}
[0m21:20:53.449490 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.38s]
[0m21:20:53.464942 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:20:53.470223 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:20:53.471867 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:53.473046 [debug] [MainThread]: On master: BEGIN
[0m21:20:53.473991 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:20:53.487916 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:20:53.492874 [debug] [MainThread]: On master: COMMIT
[0m21:20:53.499349 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:53.504065 [debug] [MainThread]: On master: COMMIT
[0m21:20:53.513882 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:20:53.519569 [debug] [MainThread]: On master: Close
[0m21:20:53.527051 [info ] [MainThread]: 
[0m21:20:53.540948 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.56 seconds (2.56s).
[0m21:20:53.556415 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:20:53.561215 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:20:53.611577 [info ] [MainThread]: 
[0m21:20:53.621395 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m21:20:53.622865 [info ] [MainThread]: 
[0m21:20:53.632274 [error] [MainThread]: [33mDatabase Error in model fct_summary (models/traffic_models/fct_summary.sql)[0m
[0m21:20:53.633838 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:20:53.635074 [error] [MainThread]:   LINE 9:     select * from "warehouse"."traffic_source"."source"
[0m21:20:53.636320 [error] [MainThread]:                             ^
[0m21:20:53.637533 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/fct_summary.sql
[0m21:20:53.638769 [info ] [MainThread]: 
[0m21:20:53.640019 [error] [MainThread]: [33mDatabase Error in model fct_trajectory (models/traffic_models/fct_trajectory.sql)[0m
[0m21:20:53.650616 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:20:53.651859 [error] [MainThread]:   LINE 9:     select * from "warehouse"."traffic_source"."source"
[0m21:20:53.653170 [error] [MainThread]:                             ^
[0m21:20:53.654377 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/fct_trajectory.sql
[0m21:20:53.655657 [info ] [MainThread]: 
[0m21:20:53.657004 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 TOTAL=5
[0m21:20:53.658528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b6c460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b6c490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b6c430>]}


============================== 2022-07-27 21:23:24.914759 | a676c8f2-cadb-40cb-afcb-8120b89d9038 ==============================
[0m21:23:24.914901 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:23:24.940533 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:23:24.943326 [debug] [MainThread]: Tracking: tracking
[0m21:23:25.132732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39419b8580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39419b8670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39419b8640>]}
[0m21:23:25.604412 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m21:23:25.606269 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/fct_trajectory.sql
[0m21:23:25.607577 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/fct_summary.sql
[0m21:23:25.663061 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:23:25.741015 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:23:25.793503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f394186aa00>]}
[0m21:23:25.820059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39418f2670>]}
[0m21:23:25.821504 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:23:25.823106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39418f25e0>]}
[0m21:23:25.828820 [info ] [MainThread]: 
[0m21:23:25.831261 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:23:25.835284 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:23:25.876639 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:23:25.877583 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:23:25.878339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:25.957778 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.08 seconds
[0m21:23:25.962667 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:23:25.985518 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:23:26.055464 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:23:26.056584 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:23:26.057460 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:26.071750 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m21:23:26.075061 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:23:26.077918 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:23:26.090082 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.01 seconds
[0m21:23:26.113696 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:23:26.124462 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:23:26.165107 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.166027 [debug] [MainThread]: On master: BEGIN
[0m21:23:26.166729 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:23:26.185494 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:23:26.186429 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.187140 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:23:26.226658 [debug] [MainThread]: SQL status: SELECT 1 in 0.04 seconds
[0m21:23:26.230632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3941931820>]}
[0m21:23:26.231909 [debug] [MainThread]: On master: ROLLBACK
[0m21:23:26.234112 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.235283 [debug] [MainThread]: On master: BEGIN
[0m21:23:26.238716 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:23:26.240186 [debug] [MainThread]: On master: COMMIT
[0m21:23:26.242289 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.243495 [debug] [MainThread]: On master: COMMIT
[0m21:23:26.245067 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:23:26.246024 [debug] [MainThread]: On master: Close
[0m21:23:26.254397 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:23:26.256031 [info ] [MainThread]: 
[0m21:23:26.290953 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:23:26.292353 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:23:26.294603 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:23:26.295578 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:23:26.296486 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:23:26.306861 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:23:26.308447 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:26.320290 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:23:26.688057 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:23:26.702814 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.703768 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:23:26.704522 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:26.725177 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:23:26.727387 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.728886 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:23:26.828681 [debug] [Thread-1  ]: SQL status: SELECT 6 in 0.1 seconds
[0m21:23:26.925458 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.926369 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
[0m21:23:26.932389 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:23:26.946569 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.947466 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
[0m21:23:26.953694 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:23:27.189386 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:23:27.190335 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:27.191062 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:23:27.599219 [debug] [Thread-1  ]: SQL status: COMMIT in 0.41 seconds
[0m21:23:27.624086 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:27.625107 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
[0m21:23:27.767093 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.14 seconds
[0m21:23:27.771300 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:27.772517 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:23:27.782233 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f394035b130>]}
[0m21:23:27.784214 [info ] [Thread-1  ]: 1 of 5 OK created table model warehouse.dim_types .............................. [[32mSELECT 6[0m in 1.49s]
[0m21:23:27.786580 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:23:27.788054 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:23:27.793548 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:23:27.795974 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.804793 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:23:27.805805 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:23:27.825494 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:23:27.829534 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:27.832978 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:23:27.904605 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:23:27.906202 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.916820 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:23:27.918040 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:27.932616 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:23:27.934329 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.937546 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:23:27.944422 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:23:27.966496 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.967390 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:23:27.969542 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:27.978840 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.979826 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:23:27.982130 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:27.991852 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:23:27.992863 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.993581 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:23:28.201033 [debug] [Thread-1  ]: SQL status: COMMIT in 0.21 seconds
[0m21:23:28.212005 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:28.239075 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:23:28.268375 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:23:28.273387 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.274536 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:23:28.283448 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3941888970>]}
[0m21:23:28.285199 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.49s]
[0m21:23:28.291873 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:23:28.296481 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:23:28.300897 [info ] [Thread-1  ]: 3 of 5 START table model warehouse.fct_summary ................................. [RUN]
[0m21:23:28.308053 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:23:28.315320 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:23:28.325112 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:23:28.360623 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:23:28.369246 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.370807 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:23:28.412640 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_summary"
[0m21:23:28.414449 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.415328 [debug] [Thread-1  ]: On model.dbt_.fct_summary: BEGIN
[0m21:23:28.416240 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:28.428577 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:23:28.432485 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.437712 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:23:28.496231 [debug] [Thread-1  ]: SQL status: SELECT 922 in 0.05 seconds
[0m21:23:28.513704 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.515856 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
[0m21:23:28.521670 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:28.535946 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:23:28.540005 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.542571 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:23:28.568195 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:23:28.582361 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.588404 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
[0m21:23:28.593744 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m21:23:28.606960 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.611453 [debug] [Thread-1  ]: On model.dbt_.fct_summary: Close
[0m21:23:28.616634 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39402ec6d0>]}
[0m21:23:28.624675 [info ] [Thread-1  ]: 3 of 5 OK created table model warehouse.fct_summary ............................ [[32mSELECT 922[0m in 0.31s]
[0m21:23:28.629119 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:23:28.635865 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:23:28.645892 [info ] [Thread-1  ]: 4 of 5 START table model warehouse.fct_trajectory .............................. [RUN]
[0m21:23:28.649787 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:23:28.673168 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:23:28.692417 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:23:28.771084 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:23:28.776133 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.777142 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:23:28.823481 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_trajectory"
[0m21:23:28.845351 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:28.846379 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: BEGIN
[0m21:23:28.847215 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:28.865049 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:23:28.866640 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:28.867599 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:23:32.825530 [debug] [Thread-1  ]: SQL status: SELECT 922 in 3.95 seconds
[0m21:23:32.838953 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:32.839837 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
[0m21:23:32.841906 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:32.853719 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:23:32.854854 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:32.855741 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:23:33.154016 [debug] [Thread-1  ]: SQL status: COMMIT in 0.3 seconds
[0m21:23:33.202160 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:33.203228 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
[0m21:23:33.205580 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m21:23:33.214894 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:33.217165 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: Close
[0m21:23:33.221502 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39402a3130>]}
[0m21:23:33.223392 [info ] [Thread-1  ]: 4 of 5 OK created table model warehouse.fct_trajectory ......................... [[32mSELECT 922[0m in 4.57s]
[0m21:23:33.226582 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:23:33.227896 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:23:33.230269 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:23:33.234343 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.235501 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:23:33.237176 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:23:33.268517 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:23:33.270384 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:33.271505 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:23:33.432436 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:23:33.445664 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.446583 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:23:33.447285 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:33.474718 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:23:33.475830 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.481598 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:23:33.489522 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:23:33.515545 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.516494 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:23:33.520366 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:33.552479 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:23:33.553445 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.554179 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:23:33.732420 [debug] [Thread-1  ]: SQL status: COMMIT in 0.18 seconds
[0m21:23:33.744282 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.745194 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:23:33.760606 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.01 seconds
[0m21:23:33.768673 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:33.769661 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:23:33.782408 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39413f3100>]}
[0m21:23:33.789584 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.55s]
[0m21:23:33.796892 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:23:33.802258 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:23:33.803483 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:33.824829 [debug] [MainThread]: On master: BEGIN
[0m21:23:33.825889 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:23:33.851210 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
[0m21:23:33.852396 [debug] [MainThread]: On master: COMMIT
[0m21:23:33.855166 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:33.857985 [debug] [MainThread]: On master: COMMIT
[0m21:23:33.864385 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:23:33.868473 [debug] [MainThread]: On master: Close
[0m21:23:33.873945 [info ] [MainThread]: 
[0m21:23:33.885251 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 8.04 seconds (8.04s).
[0m21:23:33.892634 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:23:33.895804 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:23:33.906055 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:23:33.958805 [info ] [MainThread]: 
[0m21:23:33.967208 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:23:33.972469 [info ] [MainThread]: 
[0m21:23:33.975525 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m21:23:33.981075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39418f2520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3941880e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f394183f610>]}


============================== 2022-07-27 21:26:16.553911 | 016b495a-9ff1-4450-9e2b-8900dbe57597 ==============================
[0m21:26:16.554034 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:26:16.563072 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m21:26:16.564100 [debug] [MainThread]: Tracking: tracking
[0m21:26:16.775196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848610>]}
[0m21:26:16.962694 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:26:16.963876 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:26:16.988387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644732a00>]}
[0m21:26:17.026099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644788e20>]}
[0m21:26:17.027651 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:26:17.029595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644788d30>]}
[0m21:26:17.038026 [info ] [MainThread]: 
[0m21:26:17.043606 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:26:17.047954 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:26:17.114424 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:26:17.119491 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:26:17.120754 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:26:17.134964 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m21:26:17.141351 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:26:17.146175 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:26:17.158169 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
[0m21:26:17.165131 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:26:17.171917 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:26:17.219562 [debug] [MainThread]: Using postgres connection "master"
[0m21:26:17.222779 [debug] [MainThread]: On master: BEGIN
[0m21:26:17.226614 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:26:17.244341 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:26:17.246698 [debug] [MainThread]: Using postgres connection "master"
[0m21:26:17.247600 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:26:17.287318 [debug] [MainThread]: SQL status: SELECT 1 in 0.04 seconds
[0m21:26:17.291398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb64485b7c0>]}
[0m21:26:17.292984 [debug] [MainThread]: On master: ROLLBACK
[0m21:26:17.294615 [debug] [MainThread]: On master: Close
[0m21:26:17.301869 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:26:17.303880 [info ] [MainThread]: 
[0m21:26:17.333778 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:26:17.335354 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:26:17.336384 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:26:17.337253 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:26:17.346380 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:26:17.348249 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.349567 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:26:17.352739 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.361584 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:26:17.363100 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:26:17.365416 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:26:17.366516 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:26:17.367443 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:26:17.386501 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:26:17.393902 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.395200 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:26:17.396241 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.398191 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:26:17.403017 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:26:17.406139 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:26:17.412820 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:26:17.413898 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:26:17.443288 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:26:17.453506 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.454677 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:26:17.461608 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.465248 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:26:17.469185 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:26:17.477417 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:26:17.478534 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:26:17.479470 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:26:17.496569 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:26:17.502535 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.503668 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:26:17.504722 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.506626 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:26:17.511958 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:26:17.514161 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:26:17.516950 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:26:17.518038 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:26:17.535753 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:26:17.537665 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.541515 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:26:17.542924 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.546375 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:26:17.547689 [debug] [Thread-1  ]: Began running node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.550163 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710"
[0m21:26:17.552472 [debug] [Thread-1  ]: Began compiling node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.555552 [debug] [Thread-1  ]: Compiling test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.675162 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710"
[0m21:26:17.677012 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.677958 [debug] [Thread-1  ]: Began executing node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.678768 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.680603 [debug] [Thread-1  ]: Finished running node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.681673 [debug] [Thread-1  ]: Began running node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.683522 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.unique_my_first_dbt_model_id.16e066b321"
[0m21:26:17.684528 [debug] [Thread-1  ]: Began compiling node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.685301 [debug] [Thread-1  ]: Compiling test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.715970 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.unique_my_first_dbt_model_id.16e066b321"
[0m21:26:17.717481 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.718388 [debug] [Thread-1  ]: Began executing node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.720467 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.722282 [debug] [Thread-1  ]: Finished running node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.723324 [debug] [Thread-1  ]: Began running node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.725164 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.not_null_my_second_dbt_model_id.151b76d778"
[0m21:26:17.726248 [debug] [Thread-1  ]: Began compiling node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.727186 [debug] [Thread-1  ]: Compiling test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.745985 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.not_null_my_second_dbt_model_id.151b76d778"
[0m21:26:17.747760 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.749145 [debug] [Thread-1  ]: Began executing node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.750183 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.752074 [debug] [Thread-1  ]: Finished running node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.753353 [debug] [Thread-1  ]: Began running node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.755556 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.unique_my_second_dbt_model_id.57a0f8c493"
[0m21:26:17.756726 [debug] [Thread-1  ]: Began compiling node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.757660 [debug] [Thread-1  ]: Compiling test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.773297 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.unique_my_second_dbt_model_id.57a0f8c493"
[0m21:26:17.774948 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.775995 [debug] [Thread-1  ]: Began executing node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.777049 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.778958 [debug] [Thread-1  ]: Finished running node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.782462 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:26:17.783353 [debug] [MainThread]: Connection 'test.dbt_.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m21:26:17.807238 [info ] [MainThread]: Done.
[0m21:26:17.864839 [debug] [MainThread]: Acquiring new postgres connection "generate_catalog"
[0m21:26:17.865873 [info ] [MainThread]: Building catalog
[0m21:26:17.883317 [debug] [ThreadPool]: Acquiring new postgres connection "warehouse.information_schema"
[0m21:26:17.959344 [debug] [ThreadPool]: Using postgres connection "warehouse.information_schema"
[0m21:26:17.960899 [debug] [ThreadPool]: On warehouse.information_schema: BEGIN
[0m21:26:17.961961 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:26:17.979299 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:26:17.983030 [debug] [ThreadPool]: Using postgres connection "warehouse.information_schema"
[0m21:26:17.985419 [debug] [ThreadPool]: On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        tbl_desc.description as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        col_desc.description as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid
    left outer join pg_catalog.pg_description tbl_desc on (tbl_desc.objoid = tbl.oid and tbl_desc.objsubid = 0)
    left outer join pg_catalog.pg_description col_desc on (col_desc.objoid = tbl.oid and col_desc.objsubid = col.attnum)

    where (upper(sch.nspname) = upper('warehouse') or upper(sch.nspname) = upper('traffic_source'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence in ('p', 'u') -- [p]ermanent table or [u]nlogged table. Exclude [t]emporary tables
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
[0m21:26:18.442086 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.45 seconds
[0m21:26:18.511323 [debug] [ThreadPool]: On warehouse.information_schema: ROLLBACK
[0m21:26:18.513793 [debug] [ThreadPool]: On warehouse.information_schema: Close
[0m21:26:18.561310 [info ] [MainThread]: Catalog written to /home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/target/catalog.json
[0m21:26:18.563307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb643e90ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb643e90d90>]}
[0m21:26:21.692619 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m21:26:21.694461 [debug] [MainThread]: Connection 'warehouse.information_schema' was properly closed.


============================== 2022-07-27 21:26:47.108713 | 501303cb-3088-4b70-9289-bc0960616cda ==============================
[0m21:26:47.108833 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:26:47.127122 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m21:26:47.128143 [debug] [MainThread]: Tracking: tracking
[0m21:26:47.390794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1da0c18670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1da0c18760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1da0c18730>]}
[0m21:26:47.407920 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m21:26:47.413581 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m21:26:47.415170 [info ] [MainThread]: 
[0m21:26:47.423949 [info ] [MainThread]: 
[0m21:26:47.426885 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-07-27 21:29:03.002006 | 9029f63b-320e-413e-9865-cf604216577b ==============================
[0m21:29:03.002127 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:29:03.014055 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:29:03.016650 [debug] [MainThread]: Tracking: tracking
[0m21:29:03.224704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71f154c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71f155b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71f15580>]}
[0m21:29:03.415142 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 0 files added, 0 files changed.
[0m21:29:03.416668 [debug] [MainThread]: Partial parsing: deleted file: dbt_://models/example/my_first_dbt_model.sql
[0m21:29:03.417548 [debug] [MainThread]: Partial parsing: deleted file: dbt_://models/example/my_second_dbt_model.sql
[0m21:29:03.437810 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

[0m21:29:03.461434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e04970>]}
[0m21:29:03.486401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e4f1f0>]}
[0m21:29:03.487863 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:29:03.489547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e4f2e0>]}
[0m21:29:03.494801 [info ] [MainThread]: 
[0m21:29:03.497400 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:29:03.517496 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:29:03.629220 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:29:03.630423 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:29:03.631360 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:29:03.651131 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:29:03.661517 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:29:03.674946 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:29:03.701617 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:29:03.712340 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:29:03.713465 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:29:03.729987 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:29:03.731074 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:29:03.733555 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:29:03.746222 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
[0m21:29:03.754017 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:29:03.761232 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:29:03.802220 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.803146 [debug] [MainThread]: On master: BEGIN
[0m21:29:03.803843 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:29:03.839207 [debug] [MainThread]: SQL status: BEGIN in 0.04 seconds
[0m21:29:03.846546 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.850329 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:29:03.903369 [debug] [MainThread]: SQL status: SELECT 1 in 0.05 seconds
[0m21:29:03.907321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e8e430>]}
[0m21:29:03.909578 [debug] [MainThread]: On master: ROLLBACK
[0m21:29:03.913158 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.914510 [debug] [MainThread]: On master: BEGIN
[0m21:29:03.918550 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:29:03.921133 [debug] [MainThread]: On master: COMMIT
[0m21:29:03.922231 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.924186 [debug] [MainThread]: On master: COMMIT
[0m21:29:03.927829 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:29:03.929393 [debug] [MainThread]: On master: Close
[0m21:29:03.932872 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:29:03.940467 [info ] [MainThread]: 
[0m21:29:03.994250 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:29:03.995664 [info ] [Thread-1  ]: 1 of 3 START table model warehouse.dim_types ................................... [RUN]
[0m21:29:03.998427 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:29:03.999623 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:29:04.000564 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:29:04.017343 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:29:04.018849 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:04.025086 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:29:04.429499 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:29:04.431490 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.433330 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:29:04.434493 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:29:04.447130 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:29:04.449840 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.451067 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:29:04.511420 [debug] [Thread-1  ]: SQL status: SELECT 6 in 0.06 seconds
[0m21:29:04.536357 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.537451 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
[0m21:29:04.539767 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:04.550486 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.551727 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
[0m21:29:04.554059 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:04.657697 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:29:04.658832 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.659733 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:29:04.685823 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:29:04.704878 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.705964 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
[0m21:29:04.720190 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
[0m21:29:04.724520 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:04.725687 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:29:04.734165 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71569d90>]}
[0m21:29:04.735915 [info ] [Thread-1  ]: 1 of 3 OK created table model warehouse.dim_types .............................. [[32mSELECT 6[0m in 0.74s]
[0m21:29:04.740020 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:29:04.752509 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:29:04.758670 [info ] [Thread-1  ]: 2 of 3 START table model warehouse.fct_summary ................................. [RUN]
[0m21:29:04.762474 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:29:04.764717 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:29:04.767117 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:29:04.786741 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:29:04.794474 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:04.797062 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:29:04.835245 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_summary"
[0m21:29:04.837227 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:04.838550 [debug] [Thread-1  ]: On model.dbt_.fct_summary: BEGIN
[0m21:29:04.839537 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:29:04.856937 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:29:04.858705 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:04.862311 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:29:04.925655 [debug] [Thread-1  ]: SQL status: SELECT 922 in 0.06 seconds
[0m21:29:04.954352 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:04.963268 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
[0m21:29:04.970234 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:05.000059 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:05.001026 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
[0m21:29:05.010088 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:29:05.067107 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:29:05.068029 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:05.073685 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:29:05.097148 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:29:05.125135 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:05.126133 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
[0m21:29:05.205353 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.08 seconds
[0m21:29:05.210623 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:05.211795 [debug] [Thread-1  ]: On model.dbt_.fct_summary: Close
[0m21:29:05.215198 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b705245e0>]}
[0m21:29:05.216983 [info ] [Thread-1  ]: 2 of 3 OK created table model warehouse.fct_summary ............................ [[32mSELECT 922[0m in 0.45s]
[0m21:29:05.250056 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:29:05.253783 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:29:05.260207 [info ] [Thread-1  ]: 3 of 3 START table model warehouse.fct_trajectory .............................. [RUN]
[0m21:29:05.268650 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:29:05.270953 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:29:05.275084 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:29:05.295548 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:29:05.303106 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:05.307960 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:29:05.368955 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_trajectory"
[0m21:29:05.370547 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:05.376668 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: BEGIN
[0m21:29:05.377427 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:29:05.406328 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:29:05.407598 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:05.408429 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:29:08.279601 [debug] [Thread-1  ]: SQL status: SELECT 922 in 2.87 seconds
[0m21:29:08.294761 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.295659 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
[0m21:29:08.300470 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:08.311663 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.313166 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
[0m21:29:08.315950 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:08.328321 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:29:08.329398 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.330336 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:29:08.368289 [debug] [Thread-1  ]: SQL status: COMMIT in 0.04 seconds
[0m21:29:08.376276 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.377725 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
[0m21:29:08.435389 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.06 seconds
[0m21:29:08.442629 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:08.444296 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: Close
[0m21:29:08.457247 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b704bc580>]}
[0m21:29:08.458943 [info ] [Thread-1  ]: 3 of 3 OK created table model warehouse.fct_trajectory ......................... [[32mSELECT 922[0m in 3.19s]
[0m21:29:08.476650 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:29:08.488931 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:29:08.490116 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:08.490883 [debug] [MainThread]: On master: BEGIN
[0m21:29:08.491572 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:29:08.508828 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:29:08.509793 [debug] [MainThread]: On master: COMMIT
[0m21:29:08.510515 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:08.511202 [debug] [MainThread]: On master: COMMIT
[0m21:29:08.512656 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:29:08.513618 [debug] [MainThread]: On master: Close
[0m21:29:08.515756 [info ] [MainThread]: 
[0m21:29:08.521781 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 5.02 seconds (5.02s).
[0m21:29:08.524233 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:29:08.526922 [debug] [MainThread]: Connection 'model.dbt_.fct_trajectory' was properly closed.
[0m21:29:08.589151 [info ] [MainThread]: 
[0m21:29:08.595585 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:29:08.597390 [info ] [MainThread]: 
[0m21:29:08.598842 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m21:29:08.600442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e8e220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b7156c160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71ddcd60>]}
2022-07-28 00:32:11.150893 (MainThread): Running with dbt=0.16.1
2022-07-28 00:32:11.514781 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 00:32:11.566477 (MainThread): Tracking: tracking
2022-07-28 00:32:11.698661 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d790e6510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d7913d4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d78f87850>]}
2022-07-28 00:32:11.861962 (MainThread): Partial parsing not enabled
2022-07-28 00:32:11.940918 (MainThread): Parsing macros/core.sql
2022-07-28 00:32:11.962447 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 00:32:11.997903 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 00:32:12.007088 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 00:32:12.147313 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 00:32:12.246055 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 00:32:12.276333 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 00:32:12.286892 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 00:32:12.394453 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 00:32:12.458764 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 00:32:12.489309 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 00:32:12.532735 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 00:32:12.567577 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 00:32:12.767575 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 00:32:12.773897 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 00:32:12.784847 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 00:32:12.795968 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 00:32:12.801033 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 00:32:12.811262 (MainThread): Parsing macros/etc/query.sql
2022-07-28 00:32:12.819732 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 00:32:12.862045 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 00:32:12.866958 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 00:32:12.877254 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 00:32:12.885765 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 00:32:12.895773 (MainThread): Parsing macros/relations.sql
2022-07-28 00:32:12.904275 (MainThread): Parsing macros/adapters.sql
2022-07-28 00:32:13.024957 (MainThread): Parsing macros/catalog.sql
2022-07-28 00:32:13.039806 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 00:32:13.131146 (MainThread): Partial parsing not enabled
2022-07-28 00:32:13.326965 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 00:32:13.327429 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:13.420366 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 00:32:13.420803 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:13.447795 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:32:13.448295 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:14.271731 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 00:32:15.414955 (MainThread): scipy not found, skipping conversion test.
2022-07-28 00:32:15.425105 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 00:32:15.426056 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 00:32:15.436987 (MainThread): 
2022-07-28 00:32:15.438685 (MainThread): Acquiring new postgres connection "master".
2022-07-28 00:32:15.439111 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:15.557077 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 00:32:15.557824 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 00:32:16.609463 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 00:32:16.612300 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 00:32:16.735234 (ThreadPoolExecutor-0_0): Got an error when attempting to open a postgres connection: 'could not connect to server: Connection refused
	Is the server running on host "localhost" (127.0.0.1) and accepting
	TCP/IP connections on port 5432?
could not connect to server: Cannot assign requested address
	Is the server running on host "localhost" (::1) and accepting
	TCP/IP connections on port 5432?
'
2022-07-28 00:32:16.740561 (ThreadPoolExecutor-0_0): Error running SQL: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 00:32:16.741046 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-07-28 00:32:16.741367 (ThreadPoolExecutor-0_0): On list_warehouse: No close available on handle
2022-07-28 00:32:16.741804 (ThreadPoolExecutor-0_0): Error running SQL: macro list_schemas
2022-07-28 00:32:16.742084 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-07-28 00:32:16.747596 (MainThread): Connection 'master' was properly closed.
2022-07-28 00:32:16.749186 (MainThread): Connection 'list_warehouse' was properly closed.
2022-07-28 00:32:16.749678 (MainThread): ERROR: Database Error
  could not connect to server: Connection refused
  	Is the server running on host "localhost" (127.0.0.1) and accepting
  	TCP/IP connections on port 5432?
  could not connect to server: Cannot assign requested address
  	Is the server running on host "localhost" (::1) and accepting
  	TCP/IP connections on port 5432?
  
2022-07-28 00:32:16.752667 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d687631d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d68763990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d68763510>]}
2022-07-28 00:32:16.753418 (MainThread): Flushing usage events
2022-07-28 00:37:52.205092 (MainThread): Running with dbt=0.16.1
2022-07-28 00:37:52.655231 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 00:37:52.702734 (MainThread): Tracking: tracking
2022-07-28 00:37:52.752627 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee0b4490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee172550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee177f10>]}
2022-07-28 00:37:52.895856 (MainThread): Partial parsing not enabled
2022-07-28 00:37:52.927645 (MainThread): Parsing macros/core.sql
2022-07-28 00:37:52.948888 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 00:37:52.991752 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 00:37:53.002290 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 00:37:53.151051 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 00:37:53.236308 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 00:37:53.265645 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 00:37:53.274887 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 00:37:53.371223 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 00:37:53.429718 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 00:37:53.461440 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 00:37:53.517160 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 00:37:53.549208 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 00:37:53.731881 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 00:37:53.737205 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 00:37:53.746953 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 00:37:53.757737 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 00:37:53.762599 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 00:37:53.770843 (MainThread): Parsing macros/etc/query.sql
2022-07-28 00:37:53.776390 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 00:37:53.818348 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 00:37:53.823448 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 00:37:53.833155 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 00:37:53.838758 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 00:37:53.846775 (MainThread): Parsing macros/relations.sql
2022-07-28 00:37:53.854372 (MainThread): Parsing macros/adapters.sql
2022-07-28 00:37:53.976886 (MainThread): Parsing macros/catalog.sql
2022-07-28 00:37:53.994295 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 00:37:54.134882 (MainThread): Partial parsing not enabled
2022-07-28 00:37:54.321484 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:54.322132 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:54.426389 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:54.426990 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:54.454370 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:37:54.454983 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:55.161614 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 00:37:56.276738 (MainThread): scipy not found, skipping conversion test.
2022-07-28 00:37:56.286898 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 00:37:56.289050 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 00:37:56.298397 (MainThread): 
2022-07-28 00:37:56.299754 (MainThread): Acquiring new postgres connection "master".
2022-07-28 00:37:56.300394 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:56.379593 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 00:37:56.380778 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 00:37:56.918092 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 00:37:56.919010 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 00:37:56.950243 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-07-28 00:37:57.049150 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 00:37:57.050144 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 00:37:57.060968 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 00:37:57.061487 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 00:37:57.063552 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:57.064049 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 00:37:57.064484 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 00:37:57.155506 (ThreadPoolExecutor-1_0): SQL status: SELECT 6 in 0.09 seconds
2022-07-28 00:37:57.179388 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 00:37:57.268022 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.269542 (MainThread): On master: BEGIN
2022-07-28 00:37:57.281262 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 00:37:57.281968 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.282422 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 00:37:57.909873 (MainThread): SQL status: SELECT 1 in 0.63 seconds
2022-07-28 00:37:57.920256 (MainThread): On master: ROLLBACK
2022-07-28 00:37:57.923132 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.923580 (MainThread): On master: BEGIN
2022-07-28 00:37:57.927050 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:57.928494 (MainThread): On master: COMMIT
2022-07-28 00:37:57.933782 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.934789 (MainThread): On master: COMMIT
2022-07-28 00:37:57.938088 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 00:37:57.939684 (MainThread): 00:37:57 | Concurrency: 1 threads (target='dev')
2022-07-28 00:37:57.946805 (MainThread): 00:37:57 | 
2022-07-28 00:37:58.000752 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 00:37:58.002309 (Thread-1): 00:37:57 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 00:37:58.003327 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.003701 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 00:37:58.005027 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 00:37:58.105501 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 00:37:58.133863 (Thread-1): finished collecting timing info
2022-07-28 00:37:58.228089 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.228890 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 00:37:58.253155 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 00:37:58.269169 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.269623 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 00:37:58.270666 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:37:58.335573 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 00:37:58.337094 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.337489 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 00:37:58.339093 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:58.339933 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.340644 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 00:37:58.512950 (Thread-1): SQL status: SELECT 6 in 0.17 seconds
2022-07-28 00:37:58.528446 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.528896 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 00:37:58.530331 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:58.540443 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.541144 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 00:37:58.542391 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:58.545546 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 00:37:58.545986 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.546263 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 00:37:58.586169 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 00:37:58.593840 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.594339 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 00:37:58.862630 (Thread-1): SQL status: DROP TABLE in 0.27 seconds
2022-07-28 00:37:58.872968 (Thread-1): finished collecting timing info
2022-07-28 00:37:58.876703 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6194795f-5561-4cbc-bc08-e5233ecb8bdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb882c50>]}
2022-07-28 00:37:58.879176 (Thread-1): 00:37:58 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.87s]
2022-07-28 00:37:58.882897 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 00:37:58.885088 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 00:37:58.885722 (Thread-1): 00:37:58 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 00:37:58.887471 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.887908 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 00:37:58.888287 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 00:37:58.917763 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 00:37:58.918688 (Thread-1): finished collecting timing info
2022-07-28 00:37:58.950876 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.951360 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 00:37:58.952679 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:37:58.961467 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.961943 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 00:37:58.963257 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:37:58.969047 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 00:37:58.970340 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.970756 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 00:37:58.971916 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:58.972572 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.972869 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 00:37:59.637997 (Thread-1): SQL status: SELECT 922 in 0.66 seconds
2022-07-28 00:37:59.655610 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.656074 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 00:37:59.657770 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:59.666831 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.667299 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 00:37:59.668654 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:59.671929 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 00:37:59.672470 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.672766 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 00:37:59.732429 (Thread-1): SQL status: COMMIT in 0.06 seconds
2022-07-28 00:37:59.739400 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.739846 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 00:37:59.839979 (Thread-1): SQL status: DROP TABLE in 0.10 seconds
2022-07-28 00:37:59.857515 (Thread-1): finished collecting timing info
2022-07-28 00:37:59.861673 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6194795f-5561-4cbc-bc08-e5233ecb8bdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb843d90>]}
2022-07-28 00:37:59.862560 (Thread-1): 00:37:59 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.97s]
2022-07-28 00:37:59.863026 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 00:37:59.863475 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 00:37:59.867437 (Thread-1): 00:37:59 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 00:37:59.868604 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:37:59.869000 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 00:37:59.870631 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 00:37:59.949694 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 00:37:59.950887 (Thread-1): finished collecting timing info
2022-07-28 00:38:00.016227 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.016875 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 00:38:00.018231 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:38:00.046213 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.046835 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 00:38:00.049712 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:38:00.087138 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 00:38:00.089213 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.089674 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 00:38:00.090423 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:38:00.090854 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.091139 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 00:38:32.091043 (Thread-1): SQL status: SELECT 922 in 32.00 seconds
2022-07-28 00:38:32.113927 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.114568 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 00:38:32.116211 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:38:32.125976 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.126662 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 00:38:32.128436 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:38:32.132127 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 00:38:32.133009 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.133472 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 00:38:32.170898 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 00:38:32.193682 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.194143 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 00:38:32.217721 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 00:38:32.244294 (Thread-1): finished collecting timing info
2022-07-28 00:38:32.255718 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6194795f-5561-4cbc-bc08-e5233ecb8bdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb996790>]}
2022-07-28 00:38:32.256707 (Thread-1): 00:38:32 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 32.38s]
2022-07-28 00:38:32.261237 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 00:38:32.323457 (MainThread): Using postgres connection "master".
2022-07-28 00:38:32.323961 (MainThread): On master: BEGIN
2022-07-28 00:38:32.328506 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:38:32.329048 (MainThread): On master: COMMIT
2022-07-28 00:38:32.329362 (MainThread): Using postgres connection "master".
2022-07-28 00:38:32.329629 (MainThread): On master: COMMIT
2022-07-28 00:38:32.332698 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 00:38:32.333984 (MainThread): 00:38:32 | 
2022-07-28 00:38:32.334976 (MainThread): 00:38:32 | Finished running 3 table models in 36.03s.
2022-07-28 00:38:32.335774 (MainThread): Connection 'master' was left open.
2022-07-28 00:38:32.340511 (MainThread): On master: Close
2022-07-28 00:38:32.341269 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 00:38:32.341696 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 00:38:32.450035 (MainThread): 
2022-07-28 00:38:32.453875 (MainThread): Completed successfully
2022-07-28 00:38:32.455693 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 00:38:32.462940 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eba13e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb991490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb97f490>]}
2022-07-28 00:38:32.463732 (MainThread): Flushing usage events
2022-07-28 17:27:44.931951 (MainThread): Running with dbt=0.16.1
2022-07-28 17:27:47.442307 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 17:27:47.495315 (MainThread): Tracking: tracking
2022-07-28 17:27:44.931917 (MainThread): Running with dbt=0.16.1
2022-07-28 17:27:47.542916 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 17:27:47.544298 (MainThread): Tracking: tracking
2022-07-28 17:27:48.443914 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a22252c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a2238cc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a22249310>]}
2022-07-28 17:27:48.443949 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4beab9ebd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4bead517d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4beac26f50>]}
2022-07-28 17:27:48.856693 (MainThread): Partial parsing not enabled
2022-07-28 17:27:48.866226 (MainThread): Partial parsing not enabled
2022-07-28 17:27:49.041374 (MainThread): Parsing macros/core.sql
2022-07-28 17:27:49.041426 (MainThread): Parsing macros/core.sql
2022-07-28 17:27:49.110263 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 17:27:49.124781 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 17:27:49.250575 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 17:27:49.269097 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 17:27:49.276125 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 17:27:49.304098 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 17:27:49.653079 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 17:27:49.780328 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 17:27:50.201270 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 17:27:50.207143 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 17:27:50.399483 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 17:27:50.400796 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 17:27:50.453632 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 17:27:50.459761 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 17:27:51.117039 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 17:27:51.117059 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 17:27:51.560786 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 17:27:51.563892 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 17:27:51.832227 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 17:27:51.836226 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 17:27:52.171134 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 17:27:52.171270 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 17:27:52.769819 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 17:27:52.769859 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 17:27:53.379879 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 17:27:53.383822 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 17:27:53.990360 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 17:27:53.990392 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 17:27:54.004807 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 17:27:54.005493 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 17:27:54.024367 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 17:27:54.025125 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 17:27:54.032054 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 17:27:54.034652 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 17:27:54.055240 (MainThread): Parsing macros/etc/query.sql
2022-07-28 17:27:54.067968 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 17:27:54.079536 (MainThread): Parsing macros/etc/query.sql
2022-07-28 17:27:54.128843 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 17:27:55.688690 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 17:27:55.688713 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 17:27:55.694363 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 17:27:55.695202 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 17:27:55.943710 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 17:27:55.943709 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 17:27:55.976933 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 17:27:55.976939 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 17:27:57.010844 (MainThread): Parsing macros/relations.sql
2022-07-28 17:27:57.010857 (MainThread): Parsing macros/relations.sql
2022-07-28 17:27:57.035467 (MainThread): Parsing macros/adapters.sql
2022-07-28 17:27:57.046497 (MainThread): Parsing macros/adapters.sql
2022-07-28 17:27:57.743080 (MainThread): Parsing macros/catalog.sql
2022-07-28 17:27:57.743077 (MainThread): Parsing macros/catalog.sql
2022-07-28 17:27:57.755136 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 17:27:57.772155 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 17:27:58.492431 (MainThread): Partial parsing not enabled
2022-07-28 17:27:58.492887 (MainThread): Partial parsing not enabled
2022-07-28 17:28:00.907260 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:00.911776 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:00.930056 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:00.930642 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.029969 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:02.035852 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.037414 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:02.037992 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.337641 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:02.338093 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.347278 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:02.347776 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:06.637177 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 17:28:06.713194 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 17:28:16.690952 (MainThread): scipy not found, skipping conversion test.
2022-07-28 17:28:16.721684 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 17:28:16.722939 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 17:28:16.748861 (MainThread): 
2022-07-28 17:28:16.752907 (MainThread): Acquiring new postgres connection "master".
2022-07-28 17:28:16.753325 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:16.930209 (MainThread): scipy not found, skipping conversion test.
2022-07-28 17:28:16.966484 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 17:28:16.979544 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 17:28:17.019261 (MainThread): 
2022-07-28 17:28:17.030828 (MainThread): Acquiring new postgres connection "master".
2022-07-28 17:28:17.031324 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:19.273134 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 17:28:19.274112 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 17:28:19.278597 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 17:28:19.279192 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 17:28:20.555764 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 17:28:20.559660 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 17:28:20.916858 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 17:28:20.917369 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 17:28:21.127079 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.57 seconds
2022-07-28 17:28:21.129215 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.21 seconds
2022-07-28 17:28:22.199661 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.200004 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.200114 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 17:28:22.200429 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 17:28:22.200426 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-07-28 17:28:22.200691 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-07-28 17:28:22.206938 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.214761 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.215597 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-07-28 17:28:22.218057 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:22.218573 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.218879 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-07-28 17:28:22.223884 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-07-28 17:28:22.227914 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:22.228449 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.228752 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-07-28 17:28:22.590948 (ThreadPoolExecutor-0_0): SQL status: CREATE SCHEMA in 0.37 seconds
2022-07-28 17:28:22.596741 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-28 17:28:22.597234 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.597534 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-28 17:28:22.708515 (ThreadPoolExecutor-0_0): SQL status: COMMIT in 0.11 seconds
2022-07-28 17:28:22.709216 (ThreadPoolExecutor-0_0): Postgres error: duplicate key value violates unique constraint "pg_namespace_nspname_index"
DETAIL:  Key (nspname)=(warehouse) already exists.

2022-07-28 17:28:22.715175 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: ROLLBACK
2022-07-28 17:28:22.720493 (ThreadPoolExecutor-0_0): Error running SQL: macro create_schema
2022-07-28 17:28:22.720947 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-07-28 17:28:22.724267 (MainThread): Connection 'master' was properly closed.
2022-07-28 17:28:22.724891 (MainThread): Connection 'create_warehouse_warehouse' was left open.
2022-07-28 17:28:22.725321 (MainThread): On create_warehouse_warehouse: Close
2022-07-28 17:28:22.726299 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4bda0ec9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4be8378910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4be83641d0>]}
2022-07-28 17:28:22.727737 (MainThread): Flushing usage events
2022-07-28 17:28:22.808927 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 17:28:22.809735 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 17:28:22.816732 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 17:28:22.817209 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 17:28:22.831087 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-07-28 17:28:22.831604 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 17:28:22.831905 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 17:28:24.051290 (ThreadPoolExecutor-1_0): SQL status: SELECT 0 in 1.22 seconds
2022-07-28 17:28:24.055223 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 17:28:24.300696 (MainThread): Using postgres connection "master".
2022-07-28 17:28:24.302953 (MainThread): On master: BEGIN
2022-07-28 17:28:24.323195 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-28 17:28:24.323983 (MainThread): Using postgres connection "master".
2022-07-28 17:28:24.324307 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 17:28:25.386565 (MainThread): SQL status: SELECT 0 in 1.06 seconds
2022-07-28 17:28:25.389940 (MainThread): On master: ROLLBACK
2022-07-28 17:28:25.392067 (MainThread): Using postgres connection "master".
2022-07-28 17:28:25.392536 (MainThread): On master: BEGIN
2022-07-28 17:28:25.394702 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:25.395225 (MainThread): On master: COMMIT
2022-07-28 17:28:25.395631 (MainThread): Using postgres connection "master".
2022-07-28 17:28:25.395913 (MainThread): On master: COMMIT
2022-07-28 17:28:25.396783 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 17:28:25.397878 (MainThread): 17:28:25 | Concurrency: 1 threads (target='dev')
2022-07-28 17:28:25.398378 (MainThread): 17:28:25 | 
2022-07-28 17:28:25.752642 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 17:28:25.753661 (Thread-1): 17:28:25 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 17:28:25.758727 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:25.759151 (Thread-1): Opening a new connection, currently in state init
2022-07-28 17:28:25.771698 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 17:28:25.884176 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 17:28:26.307864 (Thread-1): finished collecting timing info
2022-07-28 17:28:26.523080 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:26.523659 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 17:28:26.646932 (Thread-1): SQL status: DROP TABLE in 0.12 seconds
2022-07-28 17:28:26.658443 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:26.665630 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 17:28:26.667898 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:26.806877 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 17:28:27.004845 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.005299 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 17:28:27.011069 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:27.011733 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.012048 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 17:28:27.474428 (Thread-1): SQL status: SELECT 6 in 0.46 seconds
2022-07-28 17:28:27.493454 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.493981 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 17:28:27.495297 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 17:28:27.498581 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 17:28:27.499033 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.499319 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 17:28:27.547076 (Thread-1): SQL status: COMMIT in 0.05 seconds
2022-07-28 17:28:27.555239 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.560101 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 17:28:27.561493 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:27.581316 (Thread-1): finished collecting timing info
2022-07-28 17:28:27.589718 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21dd752f-99dc-4c8c-8fc8-7dc44c89b526', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a11a00650>]}
2022-07-28 17:28:27.590609 (Thread-1): 17:28:27 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.83s]
2022-07-28 17:28:27.591093 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 17:28:27.596632 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 17:28:27.597470 (Thread-1): 17:28:27 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 17:28:27.598522 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.598898 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 17:28:27.599198 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 17:28:27.686302 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 17:28:27.688006 (Thread-1): finished collecting timing info
2022-07-28 17:28:27.782861 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.783402 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 17:28:27.792032 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 17:28:27.837345 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.840376 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 17:28:27.841489 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:27.853652 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 17:28:27.855008 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.855733 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 17:28:27.856510 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:27.856936 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.857223 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 17:28:28.253221 (Thread-1): SQL status: SELECT 922 in 0.40 seconds
2022-07-28 17:28:28.272179 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:28.272648 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 17:28:28.276775 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 17:28:28.287548 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 17:28:28.288031 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:28.288353 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 17:28:28.391051 (Thread-1): SQL status: COMMIT in 0.10 seconds
2022-07-28 17:28:28.408946 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:28.409846 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 17:28:28.414678 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:28.435906 (Thread-1): finished collecting timing info
2022-07-28 17:28:28.443455 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21dd752f-99dc-4c8c-8fc8-7dc44c89b526', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb8b450>]}
2022-07-28 17:28:28.446543 (Thread-1): 17:28:28 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.84s]
2022-07-28 17:28:28.447080 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 17:28:28.447646 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 17:28:28.448134 (Thread-1): 17:28:28 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 17:28:28.451184 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.451776 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 17:28:28.452142 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 17:28:28.527570 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 17:28:28.532767 (Thread-1): finished collecting timing info
2022-07-28 17:28:28.597566 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.605762 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 17:28:28.606966 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:28.629913 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.630373 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 17:28:28.636887 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 17:28:28.649023 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 17:28:28.650305 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.655220 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 17:28:28.656048 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:28.656479 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.658193 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 17:28:31.225175 (MainThread): Encountered an error:
2022-07-28 17:28:31.229086 (MainThread): Database Error
  duplicate key value violates unique constraint "pg_namespace_nspname_index"
  DETAIL:  Key (nspname)=(warehouse) already exists.
2022-07-28 17:28:33.499505 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "pg_namespace_nspname_index"
DETAIL:  Key (nspname)=(warehouse) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 371, in run
    result = self.execute_with_hooks(selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 331, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/run.py", line 199, in before_run
    self.create_schemas(adapter, selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 472, in create_schemas
    create_future.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.7/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 440, in create_schema
    adapter.create_schema(db, schema)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/impl.py", line 183, in create_schema
    self.execute_macro(CREATE_SCHEMA_MACRO_NAME, kwargs=kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 952, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 163, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 97, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 73, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error
  duplicate key value violates unique constraint "pg_namespace_nspname_index"
  DETAIL:  Key (nspname)=(warehouse) already exists.

2022-07-28 17:28:40.657022 (Thread-1): SQL status: SELECT 922 in 12.00 seconds
2022-07-28 17:28:40.689169 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:40.693498 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 17:28:40.924017 (Thread-1): SQL status: ALTER TABLE in 0.23 seconds
2022-07-28 17:28:40.927216 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 17:28:40.936371 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:40.936713 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 17:28:40.980045 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 17:28:41.017859 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:41.025977 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 17:28:41.036536 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:41.047094 (Thread-1): finished collecting timing info
2022-07-28 17:28:41.050305 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21dd752f-99dc-4c8c-8fc8-7dc44c89b526', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1faf7b10>]}
2022-07-28 17:28:41.052057 (Thread-1): 17:28:41 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 12.60s]
2022-07-28 17:28:41.052606 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 17:28:41.115226 (MainThread): Using postgres connection "master".
2022-07-28 17:28:41.115816 (MainThread): On master: BEGIN
2022-07-28 17:28:41.119830 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:41.120429 (MainThread): On master: COMMIT
2022-07-28 17:28:41.120756 (MainThread): Using postgres connection "master".
2022-07-28 17:28:41.121025 (MainThread): On master: COMMIT
2022-07-28 17:28:41.127677 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-07-28 17:28:41.128991 (MainThread): 17:28:41 | 
2022-07-28 17:28:41.129482 (MainThread): 17:28:41 | Finished running 3 table models in 24.38s.
2022-07-28 17:28:41.129834 (MainThread): Connection 'master' was left open.
2022-07-28 17:28:41.130316 (MainThread): On master: Close
2022-07-28 17:28:41.130938 (MainThread): Connection 'create_warehouse_warehouse' was left open.
2022-07-28 17:28:41.131310 (MainThread): On create_warehouse_warehouse: Close
2022-07-28 17:28:41.141748 (MainThread): Connection 'list_warehouse_warehouse' was left open.
2022-07-28 17:28:41.143807 (MainThread): On list_warehouse_warehouse: Close
2022-07-28 17:28:41.151621 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 17:28:41.152095 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 17:28:41.301464 (MainThread): 
2022-07-28 17:28:41.309245 (MainThread): Completed successfully
2022-07-28 17:28:41.309976 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 17:28:41.310729 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb29690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb29650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb34a10>]}
2022-07-28 17:28:41.321065 (MainThread): Flushing usage events
2022-07-28 20:27:33.187691 (MainThread): Running with dbt=0.16.1
2022-07-28 20:27:34.388379 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 20:27:34.472419 (MainThread): Tracking: tracking
2022-07-28 20:27:34.686581 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3c150a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3c087510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3c087f90>]}
2022-07-28 20:27:35.062666 (MainThread): Partial parsing not enabled
2022-07-28 20:27:35.128239 (MainThread): Parsing macros/core.sql
2022-07-28 20:27:35.217092 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:27:35.347026 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:27:35.384061 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:27:35.857714 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:27:36.103991 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:27:36.166944 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:27:36.187767 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:27:36.401345 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:27:36.543393 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:27:36.578000 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:27:36.623410 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:27:36.680140 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:27:37.283216 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:27:37.294101 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:27:37.327966 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:27:37.359747 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:27:37.374985 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:27:37.399863 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:27:37.416130 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:27:37.552204 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:27:37.557898 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:27:37.571585 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:27:37.579190 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:27:37.612941 (MainThread): Parsing macros/relations.sql
2022-07-28 20:27:37.635018 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:27:37.863304 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:27:37.894566 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:27:38.029168 (MainThread): Partial parsing not enabled
2022-07-28 20:27:38.203001 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:27:38.203556 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:27:38.293838 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:27:38.294286 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:27:38.335716 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:27:38.336175 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:27:38.373066 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3979f810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc397aae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc397ff750>]}
2022-07-28 20:27:38.373827 (MainThread): Flushing usage events
2022-07-28 20:27:39.718351 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 20:27:39.720822 (MainThread): Encountered an error:
2022-07-28 20:27:39.722165 (MainThread): Compilation Error
  Error reading dbt_: traffic_models/schema.yml - Runtime Error
    Syntax error near line 16
    ------------------------------
    13 |       - name: Id
    14 |         tests:
    15 |         tags: [dim_type_tests]
    16 |           - unique
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 13, column 9:
              - name: Id
                ^
    expected <block end>, but found '<block sequence start>'
      in "<unicode string>", line 16, column 11:
                  - unique
                  ^
2022-07-28 20:27:39.789953 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/yaml_helper.py", line 49, in load_yaml_text
    return yaml.safe_load(contents)
  File "/usr/local/lib/python3.7/site-packages/yaml/__init__.py", line 162, in safe_load
    return load(stream, SafeLoader)
  File "/usr/local/lib/python3.7/site-packages/yaml/__init__.py", line 114, in load
    return loader.get_single_data()
  File "/usr/local/lib/python3.7/site-packages/yaml/constructor.py", line 41, in get_single_data
    node = self.get_single_node()
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 133, in compose_mapping_node
    item_value = self.compose_node(node, item_key)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 82, in compose_node
    node = self.compose_sequence_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 111, in compose_sequence_node
    node.value.append(self.compose_node(node, index))
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 133, in compose_mapping_node
    item_value = self.compose_node(node, item_key)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 82, in compose_node
    node = self.compose_sequence_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 111, in compose_sequence_node
    node.value.append(self.compose_node(node, index))
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/usr/local/lib/python3.7/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/usr/local/lib/python3.7/site-packages/yaml/parser.py", line 439, in parse_block_mapping_key
    "expected <block end>, but found %r" % token.id, token.start_mark)
yaml.parser.ParserError: while parsing a block mapping
  in "<unicode string>", line 13, column 9:
          - name: Id
            ^
expected <block end>, but found '<block sequence start>'
  in "<unicode string>", line 16, column 11:
              - unique
              ^

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/schemas.py", line 163, in _yaml_from_file
    return load_yaml_text(source_file.contents)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 16
  ------------------------------
  13 |       - name: Id
  14 |         tests:
  15 |         tags: [dim_type_tests]
  16 |           - unique
  
  Raw Error:
  ------------------------------
  while parsing a block mapping
    in "<unicode string>", line 13, column 9:
            - name: Id
              ^
  expected <block end>, but found '<block sequence start>'
    in "<unicode string>", line 16, column 11:
                - unique
                ^

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 336, in load_all
    loader.load(internal_manifest=internal_manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 208, in load
    self.parse_project(project, macro_manifest, old_results)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 182, in parse_project
    self.parse_with_cache(path, parser, old_results)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 138, in parse_with_cache
    parser.parse_file(block)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/schemas.py", line 283, in parse_file
    dct = self._yaml_from_file(block.file)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/schemas.py", line 168, in _yaml_from_file
    .format(self.project.project_name, path, reason)
dbt.exceptions.CompilationException: Compilation Error
  Error reading dbt_: traffic_models/schema.yml - Runtime Error
    Syntax error near line 16
    ------------------------------
    13 |       - name: Id
    14 |         tests:
    15 |         tags: [dim_type_tests]
    16 |           - unique
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 13, column 9:
              - name: Id
                ^
    expected <block end>, but found '<block sequence start>'
      in "<unicode string>", line 16, column 11:
                  - unique
                  ^

2022-07-28 20:41:45.656994 (MainThread): Running with dbt=0.16.1
2022-07-28 20:41:46.009741 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 20:41:46.034064 (MainThread): Tracking: tracking
2022-07-28 20:41:46.084984 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7284cabd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7284e0c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7284ea0d0>]}
2022-07-28 20:41:46.177682 (MainThread): Partial parsing not enabled
2022-07-28 20:41:46.205485 (MainThread): Parsing macros/core.sql
2022-07-28 20:41:46.226980 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:41:46.271583 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:41:46.280818 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:41:46.421216 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:41:46.503771 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:41:46.533632 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:41:46.543178 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:41:46.665321 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:41:46.722778 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:41:46.746179 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:41:46.775790 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:41:46.809712 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:41:47.081901 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:41:47.087771 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:41:47.098131 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:41:47.115057 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:41:47.127505 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:41:47.139806 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:41:47.145714 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:41:47.210184 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:41:47.216984 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:41:47.239173 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:41:47.251911 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:41:47.265292 (MainThread): Parsing macros/relations.sql
2022-07-28 20:41:47.281291 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:41:47.384937 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:41:47.399602 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:41:47.520034 (MainThread): Partial parsing not enabled
2022-07-28 20:41:47.738578 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:47.739218 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:47.821483 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:47.822138 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:47.848863 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:47.849484 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:48.785420 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 20:41:50.043143 (MainThread): scipy not found, skipping conversion test.
2022-07-28 20:41:50.085481 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 20:41:50.086693 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 20:41:50.104466 (MainThread): 
2022-07-28 20:41:50.108434 (MainThread): Acquiring new postgres connection "master".
2022-07-28 20:41:50.109056 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:50.353689 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 20:41:50.355896 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 20:41:51.210473 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 20:41:51.211107 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 20:41:51.233347 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-28 20:41:51.349621 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 20:41:51.350070 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 20:41:51.359164 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:41:51.359893 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 20:41:51.361229 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:51.361834 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:41:51.362276 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 20:41:51.999449 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.64 seconds
2022-07-28 20:41:52.015834 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 20:41:52.081369 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.082029 (MainThread): On master: BEGIN
2022-07-28 20:41:52.093168 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 20:41:52.093828 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.094289 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 20:41:52.728452 (MainThread): SQL status: SELECT 0 in 0.63 seconds
2022-07-28 20:41:52.732507 (MainThread): On master: ROLLBACK
2022-07-28 20:41:52.733886 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.734484 (MainThread): On master: BEGIN
2022-07-28 20:41:52.735921 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:52.737319 (MainThread): On master: COMMIT
2022-07-28 20:41:52.743006 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.743495 (MainThread): On master: COMMIT
2022-07-28 20:41:52.744278 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:41:52.748574 (MainThread): 20:41:52 | Concurrency: 1 threads (target='dev')
2022-07-28 20:41:52.749293 (MainThread): 20:41:52 | 
2022-07-28 20:41:52.792873 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 20:41:52.793577 (Thread-1): 20:41:52 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 20:41:52.795308 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:52.795988 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 20:41:52.796352 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 20:41:52.863557 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 20:41:52.864596 (Thread-1): finished collecting timing info
2022-07-28 20:41:52.950738 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:52.951452 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 20:41:52.953008 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:52.961806 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:52.962467 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 20:41:52.963648 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:53.044248 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 20:41:53.045670 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.046387 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 20:41:53.048561 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:53.049236 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.049671 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 20:41:53.737208 (Thread-1): SQL status: SELECT 6 in 0.69 seconds
2022-07-28 20:41:53.756382 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.756900 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 20:41:53.804841 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2022-07-28 20:41:53.814403 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.814841 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 20:41:53.816356 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:41:53.823928 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 20:41:53.824802 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.825497 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 20:41:53.852750 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 20:41:53.859812 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.860326 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 20:41:53.887015 (Thread-1): SQL status: DROP TABLE in 0.03 seconds
2022-07-28 20:41:53.903996 (Thread-1): finished collecting timing info
2022-07-28 20:41:53.906606 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ec9ac10-d481-4a25-a666-45e4018bb38f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725c88390>]}
2022-07-28 20:41:53.907982 (Thread-1): 20:41:53 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.11s]
2022-07-28 20:41:53.908648 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 20:41:53.910630 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 20:41:53.911517 (Thread-1): 20:41:53 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 20:41:53.913195 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:53.913787 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 20:41:53.914245 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 20:41:53.941946 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 20:41:53.943165 (Thread-1): finished collecting timing info
2022-07-28 20:41:53.984465 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:53.984936 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 20:41:53.986057 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:53.995114 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:53.995854 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 20:41:53.997045 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:54.002967 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 20:41:54.004452 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.005099 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 20:41:54.006359 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:54.006985 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.007484 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 20:41:54.229349 (Thread-1): SQL status: SELECT 922 in 0.22 seconds
2022-07-28 20:41:54.245678 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.246354 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 20:41:54.247926 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:41:54.257624 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.258311 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 20:41:54.260228 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:41:54.263728 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 20:41:54.264357 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.264856 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 20:41:54.296627 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 20:41:54.303663 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.304545 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 20:41:54.341959 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 20:41:54.351688 (Thread-1): finished collecting timing info
2022-07-28 20:41:54.354656 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ec9ac10-d481-4a25-a666-45e4018bb38f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725bc5d90>]}
2022-07-28 20:41:54.355992 (Thread-1): 20:41:54 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.44s]
2022-07-28 20:41:54.357152 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 20:41:54.358815 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 20:41:54.360351 (Thread-1): 20:41:54 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 20:41:54.362542 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.363168 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 20:41:54.363734 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 20:41:54.399043 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 20:41:54.400365 (Thread-1): finished collecting timing info
2022-07-28 20:41:54.428853 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.429552 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 20:41:54.431230 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:54.440288 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.440959 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 20:41:54.442141 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:54.448458 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 20:41:54.449986 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.450613 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 20:41:54.451851 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:54.452778 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.453286 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 20:42:01.794009 (Thread-1): SQL status: SELECT 922 in 7.34 seconds
2022-07-28 20:42:01.820403 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.823880 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 20:42:01.826146 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:42:01.850453 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.850915 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 20:42:01.859652 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 20:42:01.862735 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 20:42:01.863171 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.863517 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 20:42:01.883048 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 20:42:01.889914 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.890374 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 20:42:01.922128 (Thread-1): SQL status: DROP TABLE in 0.03 seconds
2022-07-28 20:42:01.936644 (Thread-1): finished collecting timing info
2022-07-28 20:42:01.945351 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ec9ac10-d481-4a25-a666-45e4018bb38f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725c88390>]}
2022-07-28 20:42:01.956059 (Thread-1): 20:42:01 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 7.58s]
2022-07-28 20:42:01.957736 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 20:42:02.051267 (MainThread): Using postgres connection "master".
2022-07-28 20:42:02.052254 (MainThread): On master: BEGIN
2022-07-28 20:42:02.059742 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:42:02.060738 (MainThread): On master: COMMIT
2022-07-28 20:42:02.061208 (MainThread): Using postgres connection "master".
2022-07-28 20:42:02.062376 (MainThread): On master: COMMIT
2022-07-28 20:42:02.067521 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:42:02.068931 (MainThread): 20:42:02 | 
2022-07-28 20:42:02.070353 (MainThread): 20:42:02 | Finished running 3 table models in 11.96s.
2022-07-28 20:42:02.071566 (MainThread): Connection 'master' was left open.
2022-07-28 20:42:02.072128 (MainThread): On master: Close
2022-07-28 20:42:02.072800 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 20:42:02.076477 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 20:42:02.135852 (MainThread): 
2022-07-28 20:42:02.137046 (MainThread): Completed successfully
2022-07-28 20:42:02.138076 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 20:42:02.139490 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd717cc7b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725d16a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725dd2710>]}
2022-07-28 20:42:02.140389 (MainThread): Flushing usage events
2022-07-28 20:42:21.257344 (MainThread): Running with dbt=0.16.1
2022-07-28 20:42:21.562674 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 20:42:21.564201 (MainThread): Tracking: tracking
2022-07-28 20:42:21.584065 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3b04b6550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3b04aff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3b05a9fd0>]}
2022-07-28 20:42:21.707763 (MainThread): Partial parsing not enabled
2022-07-28 20:42:21.713974 (MainThread): Parsing macros/core.sql
2022-07-28 20:42:21.736129 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:42:21.826841 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:42:21.846368 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:42:22.177223 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:42:22.397539 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:42:22.504308 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:42:22.522352 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:42:22.668052 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:42:22.776733 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:42:22.824127 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:42:22.868399 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:42:22.916921 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:42:23.233480 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:42:23.240880 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:42:23.250382 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:42:23.264164 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:42:23.268901 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:42:23.281069 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:42:23.286181 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:42:23.348857 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:42:23.353548 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:42:23.363897 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:42:23.371218 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:42:23.395755 (MainThread): Parsing macros/relations.sql
2022-07-28 20:42:23.421559 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:42:23.545676 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:42:23.562697 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:42:23.734272 (MainThread): Partial parsing not enabled
2022-07-28 20:42:23.956861 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:42:23.957331 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:42:24.081159 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:42:24.081635 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:42:24.137312 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:24.137797 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:42:25.351112 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 20:42:26.787772 (MainThread): scipy not found, skipping conversion test.
2022-07-28 20:42:26.808055 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 20:42:26.809141 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 20:42:26.828670 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-07-28 20:42:26.829457 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb39fc6cb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb39fc74dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb39fc74d50>]}
2022-07-28 20:42:26.830145 (MainThread): Flushing usage events
2022-07-28 20:42:27.769757 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 20:54:30.265180 (MainThread): Running with dbt=0.16.1
2022-07-28 20:54:30.635886 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 20:54:30.649124 (MainThread): Tracking: tracking
2022-07-28 20:54:30.667911 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e4bc63bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e4bc70f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e4bc70a10>]}
2022-07-28 20:54:30.776906 (MainThread): Partial parsing not enabled
2022-07-28 20:54:30.799910 (MainThread): Parsing macros/core.sql
2022-07-28 20:54:30.821320 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:54:30.857304 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:54:30.865937 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:54:31.022216 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:54:31.113941 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:54:31.196880 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:54:31.212002 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:54:31.378401 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:54:31.436662 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:54:31.467285 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:54:31.495926 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:54:31.526183 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:54:31.708497 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:54:31.713457 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:54:31.722648 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:54:31.732798 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:54:31.737321 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:54:31.745090 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:54:31.750213 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:54:31.791953 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:54:31.796506 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:54:31.805776 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:54:31.810954 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:54:31.840529 (MainThread): Parsing macros/relations.sql
2022-07-28 20:54:31.849956 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:54:31.922408 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:54:31.933017 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:54:32.031994 (MainThread): Partial parsing not enabled
2022-07-28 20:54:32.169145 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:32.169574 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:32.243595 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:54:32.244041 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:32.281924 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:54:32.282672 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:32.955152 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 20:54:34.081571 (MainThread): scipy not found, skipping conversion test.
2022-07-28 20:54:34.090945 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 20:54:34.092158 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 20:54:34.101144 (MainThread): 
2022-07-28 20:54:34.102676 (MainThread): Acquiring new postgres connection "master".
2022-07-28 20:54:34.103283 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:34.166035 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 20:54:34.166555 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 20:54:34.829832 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 20:54:34.831648 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 20:54:34.853990 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-28 20:54:35.067091 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 20:54:35.068397 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 20:54:35.078032 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:54:35.078683 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 20:54:35.081756 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:35.084526 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:54:35.085662 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 20:54:35.098456 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 20:54:35.125080 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 20:54:35.338895 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.348993 (MainThread): On master: BEGIN
2022-07-28 20:54:35.385896 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-07-28 20:54:35.386381 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.386671 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 20:54:35.415369 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-07-28 20:54:35.430402 (MainThread): On master: ROLLBACK
2022-07-28 20:54:35.448186 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.448691 (MainThread): On master: BEGIN
2022-07-28 20:54:35.451615 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:35.452163 (MainThread): On master: COMMIT
2022-07-28 20:54:35.463823 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.464243 (MainThread): On master: COMMIT
2022-07-28 20:54:35.467646 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:54:35.468818 (MainThread): 20:54:35 | Concurrency: 1 threads (target='dev')
2022-07-28 20:54:35.474003 (MainThread): 20:54:35 | 
2022-07-28 20:54:35.590344 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 20:54:35.591097 (Thread-1): 20:54:35 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 20:54:35.592629 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:35.593044 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 20:54:35.593384 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 20:54:35.757621 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 20:54:35.758856 (Thread-1): finished collecting timing info
2022-07-28 20:54:35.932540 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:35.933412 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 20:54:35.934921 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:54:35.947391 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:35.948027 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 20:54:35.949170 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:54:36.038926 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 20:54:36.040341 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:36.041059 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 20:54:36.043192 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:36.044669 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:36.045320 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 20:54:36.073574 (Thread-1): Postgres error: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^

2022-07-28 20:54:36.074249 (Thread-1): On model.dbt_.dim_types: ROLLBACK
2022-07-28 20:54:36.075833 (Thread-1): finished collecting timing info
2022-07-28 20:54:36.077860 (Thread-1): Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-28 20:54:36.684079 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9327968d-4386-49c3-a1b3-75026fde49d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e3b08a3d0>]}
2022-07-28 20:54:36.685090 (Thread-1): 20:54:36 | 1 of 3 ERROR creating table model warehouse.dim_types................ [ERROR in 1.09s]
2022-07-28 20:54:36.686308 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 20:54:36.688056 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 20:54:36.688808 (Thread-1): 20:54:36 | 2 of 3 SKIP relation warehouse.fct_summary........................... [SKIP]
2022-07-28 20:54:36.689838 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 20:54:36.690528 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 20:54:36.691313 (Thread-1): 20:54:36 | 3 of 3 SKIP relation warehouse.fct_trajectory........................ [SKIP]
2022-07-28 20:54:36.692473 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 20:54:36.705310 (MainThread): Using postgres connection "master".
2022-07-28 20:54:36.705782 (MainThread): On master: BEGIN
2022-07-28 20:54:36.706535 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:36.706983 (MainThread): On master: COMMIT
2022-07-28 20:54:36.707276 (MainThread): Using postgres connection "master".
2022-07-28 20:54:36.707661 (MainThread): On master: COMMIT
2022-07-28 20:54:36.708527 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:54:36.709751 (MainThread): 20:54:36 | 
2022-07-28 20:54:36.710244 (MainThread): 20:54:36 | Finished running 3 table models in 2.61s.
2022-07-28 20:54:36.710607 (MainThread): Connection 'master' was left open.
2022-07-28 20:54:36.710946 (MainThread): On master: Close
2022-07-28 20:54:36.713226 (MainThread): Connection 'model.dbt_.dim_types' was left open.
2022-07-28 20:54:36.713873 (MainThread): On model.dbt_.dim_types: Close
2022-07-28 20:54:36.786952 (MainThread): 
2022-07-28 20:54:36.788771 (MainThread): Completed with 1 error and 0 warnings:
2022-07-28 20:54:36.789830 (MainThread): 
2022-07-28 20:54:36.790828 (MainThread): Database Error in model dim_types (models/traffic_models/dim_types.sql)
2022-07-28 20:54:36.791842 (MainThread):   relation "traffic_source.source" does not exist
2022-07-28 20:54:36.792752 (MainThread):   LINE 8:     select * from "warehouse"."traffic_source"."source"
2022-07-28 20:54:36.793627 (MainThread):                             ^
2022-07-28 20:54:36.794507 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-28 20:54:36.795475 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2022-07-28 20:54:36.797629 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e493f3310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e493ceed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e493abb10>]}
2022-07-28 20:54:36.799197 (MainThread): Flushing usage events
2022-07-28 21:02:06.366377 (MainThread): Running with dbt=0.16.1
2022-07-28 21:02:06.716212 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 21:02:06.771197 (MainThread): Tracking: tracking
2022-07-28 21:02:06.818772 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337be1850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337bd82d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337b14610>]}
2022-07-28 21:02:06.930306 (MainThread): Partial parsing not enabled
2022-07-28 21:02:06.959562 (MainThread): Parsing macros/core.sql
2022-07-28 21:02:07.005376 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:02:07.041421 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:02:07.050458 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:02:07.206286 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:02:07.286428 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:02:07.316773 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:02:07.326665 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:02:07.420638 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:02:07.476588 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:02:07.500620 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:02:07.538434 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:02:07.569996 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:02:07.762412 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:02:07.768244 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:02:07.778830 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:02:07.789913 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:02:07.795455 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:02:07.803898 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:02:07.819841 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:02:07.927710 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:02:07.934286 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:02:07.944495 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:02:07.950237 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:02:07.957784 (MainThread): Parsing macros/relations.sql
2022-07-28 21:02:07.965414 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:02:08.035397 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:02:08.046200 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:02:08.167525 (MainThread): Partial parsing not enabled
2022-07-28 21:02:08.343845 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:08.344436 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:08.452752 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:08.453344 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:08.479670 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:08.480264 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:09.165987 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:02:10.309421 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:02:10.324366 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:02:10.325714 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:02:10.335496 (MainThread): 
2022-07-28 21:02:10.337247 (MainThread): Acquiring new postgres connection "master".
2022-07-28 21:02:10.337850 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:10.411907 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 21:02:10.413113 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:02:10.910160 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 21:02:10.910803 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 21:02:10.960756 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.05 seconds
2022-07-28 21:02:11.042444 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 21:02:11.042973 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 21:02:11.054700 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:02:11.055144 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 21:02:11.057776 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.060785 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:02:11.061868 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 21:02:11.070087 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 21:02:11.090162 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 21:02:11.152750 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.153415 (MainThread): On master: BEGIN
2022-07-28 21:02:11.166372 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 21:02:11.166849 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.167134 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 21:02:11.191698 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-28 21:02:11.195811 (MainThread): On master: ROLLBACK
2022-07-28 21:02:11.197186 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.197680 (MainThread): On master: BEGIN
2022-07-28 21:02:11.199184 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.199709 (MainThread): On master: COMMIT
2022-07-28 21:02:11.200018 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.200289 (MainThread): On master: COMMIT
2022-07-28 21:02:11.200995 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:02:11.202012 (MainThread): 21:02:11 | Concurrency: 1 threads (target='dev')
2022-07-28 21:02:11.202502 (MainThread): 21:02:11 | 
2022-07-28 21:02:11.241728 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:02:11.242439 (Thread-1): 21:02:11 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 21:02:11.243428 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.243812 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 21:02:11.244142 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:02:11.319269 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:02:11.320359 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.399187 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.399810 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 21:02:11.401737 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.410106 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.410563 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:02:11.411680 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.477382 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 21:02:11.478619 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.479148 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 21:02:11.481864 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.482352 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.482639 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 21:02:11.572626 (Thread-1): SQL status: SELECT 6 in 0.09 seconds
2022-07-28 21:02:11.587136 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.587699 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 21:02:11.590840 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:11.600017 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.600465 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 21:02:11.601850 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:11.605117 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:02:11.605561 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.605840 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:02:11.636023 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 21:02:11.642766 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.643228 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:02:11.658345 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 21:02:11.667477 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.670049 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a311e44b-b0a4-4574-88a8-3fb7c9840a22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63351f5c50>]}
2022-07-28 21:02:11.670919 (Thread-1): 21:02:11 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.43s]
2022-07-28 21:02:11.671451 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:02:11.673066 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:02:11.673801 (Thread-1): 21:02:11 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 21:02:11.675256 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.675756 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 21:02:11.676064 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:02:11.711690 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:02:11.713043 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.744658 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.745144 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 21:02:11.746209 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.754627 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.755275 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:02:11.756361 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.761657 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 21:02:11.762885 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.763478 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 21:02:11.764251 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.764676 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.764960 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:02:11.846148 (Thread-1): SQL status: SELECT 922 in 0.08 seconds
2022-07-28 21:02:11.866355 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.866821 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 21:02:11.879444 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 21:02:11.894249 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.894726 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 21:02:11.899640 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:11.902760 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:02:11.903214 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.903576 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:02:11.924564 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 21:02:11.931499 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.935670 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:02:11.948015 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 21:02:11.959563 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.962457 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a311e44b-b0a4-4574-88a8-3fb7c9840a22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337ab6950>]}
2022-07-28 21:02:11.963606 (Thread-1): 21:02:11 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.29s]
2022-07-28 21:02:11.964721 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:02:11.965447 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:02:11.967045 (Thread-1): 21:02:11 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 21:02:11.968969 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:11.969570 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 21:02:11.970479 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:02:11.999188 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:02:12.000474 (Thread-1): finished collecting timing info
2022-07-28 21:02:12.026444 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.027153 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 21:02:12.029027 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:12.037979 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.038654 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:02:12.040563 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:12.046782 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:02:12.048300 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.049223 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 21:02:12.050717 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:12.051544 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.052078 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:02:23.537423 (Thread-1): SQL status: SELECT 922 in 11.48 seconds
2022-07-28 21:02:23.566920 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.567657 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 21:02:23.569175 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:23.580824 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.581477 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 21:02:23.582949 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:23.586429 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:02:23.587107 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.587609 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:02:23.635495 (Thread-1): SQL status: COMMIT in 0.05 seconds
2022-07-28 21:02:23.646982 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.647721 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:02:23.819708 (Thread-1): SQL status: DROP TABLE in 0.17 seconds
2022-07-28 21:02:23.843671 (Thread-1): finished collecting timing info
2022-07-28 21:02:23.851957 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a311e44b-b0a4-4574-88a8-3fb7c9840a22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335329ad0>]}
2022-07-28 21:02:23.852829 (Thread-1): 21:02:23 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 11.88s]
2022-07-28 21:02:23.853927 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:02:23.941744 (MainThread): Using postgres connection "master".
2022-07-28 21:02:23.942217 (MainThread): On master: BEGIN
2022-07-28 21:02:23.943015 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:23.943511 (MainThread): On master: COMMIT
2022-07-28 21:02:23.943808 (MainThread): Using postgres connection "master".
2022-07-28 21:02:23.944070 (MainThread): On master: COMMIT
2022-07-28 21:02:23.945004 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:02:23.946182 (MainThread): 21:02:23 | 
2022-07-28 21:02:23.948730 (MainThread): 21:02:23 | Finished running 3 table models in 13.61s.
2022-07-28 21:02:23.953844 (MainThread): Connection 'master' was left open.
2022-07-28 21:02:23.955462 (MainThread): On master: Close
2022-07-28 21:02:23.965882 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 21:02:23.966832 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 21:02:24.047708 (MainThread): 
2022-07-28 21:02:24.048869 (MainThread): Completed successfully
2022-07-28 21:02:24.049898 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 21:02:24.051543 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335398b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633529c110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335388190>]}
2022-07-28 21:02:24.052604 (MainThread): Flushing usage events
2022-07-28 21:02:40.055489 (MainThread): Running with dbt=0.16.1
2022-07-28 21:02:40.327589 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 21:02:40.328805 (MainThread): Tracking: tracking
2022-07-28 21:02:40.355054 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61a214e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61cfabb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61a20d110>]}
2022-07-28 21:02:40.440838 (MainThread): Partial parsing not enabled
2022-07-28 21:02:40.446779 (MainThread): Parsing macros/core.sql
2022-07-28 21:02:40.468057 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:02:40.504969 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:02:40.513345 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:02:40.646557 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:02:40.748239 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:02:40.776804 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:02:40.785719 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:02:40.883233 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:02:40.957293 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:02:40.979957 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:02:41.008275 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:02:41.074809 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:02:41.267842 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:02:41.272890 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:02:41.282232 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:02:41.293042 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:02:41.297603 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:02:41.305660 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:02:41.310920 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:02:41.351657 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:02:41.356083 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:02:41.365472 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:02:41.370576 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:02:41.376848 (MainThread): Parsing macros/relations.sql
2022-07-28 21:02:41.386335 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:02:41.455581 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:02:41.466119 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:02:41.572649 (MainThread): Partial parsing not enabled
2022-07-28 21:02:41.734103 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:41.734733 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:41.814608 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:41.815255 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:41.842760 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:41.843410 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:42.521387 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:02:43.447699 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:02:43.457278 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:02:43.458550 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:02:43.467311 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-07-28 21:02:43.468571 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6097ae410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6097ae510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6097b77d0>]}
2022-07-28 21:02:43.469443 (MainThread): Flushing usage events
2022-07-28 21:02:44.368954 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:08:32.454563 (MainThread): Running with dbt=0.16.1
2022-07-28 21:08:33.562640 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 21:08:33.617636 (MainThread): Tracking: tracking
2022-07-28 21:08:33.803057 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aac563990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aaac2c250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aaac33390>]}
2022-07-28 21:08:34.062511 (MainThread): Partial parsing not enabled
2022-07-28 21:08:34.144508 (MainThread): Parsing macros/core.sql
2022-07-28 21:08:34.247916 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:08:34.326751 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:08:34.352600 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:08:34.828219 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:08:35.140105 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:08:35.173819 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:08:35.183695 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:08:35.367886 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:08:35.430863 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:08:35.454523 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:08:35.483564 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:08:35.513670 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:08:35.851841 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:08:35.857323 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:08:35.867254 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:08:35.878164 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:08:35.882984 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:08:35.891420 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:08:35.897109 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:08:35.968302 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:08:35.975898 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:08:35.985696 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:08:35.991504 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:08:36.021834 (MainThread): Parsing macros/relations.sql
2022-07-28 21:08:36.032904 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:08:36.116839 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:08:36.127660 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:08:36.256712 (MainThread): Partial parsing not enabled
2022-07-28 21:08:36.777634 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:36.778066 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:36.993125 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:36.993745 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:37.083531 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:37.083986 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:37.983624 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:08:39.110893 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:08:39.124410 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:08:39.125610 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:08:39.135434 (MainThread): 
2022-07-28 21:08:39.136586 (MainThread): Acquiring new postgres connection "master".
2022-07-28 21:08:39.136982 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:39.246201 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 21:08:39.246896 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:08:39.744071 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 21:08:39.744728 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 21:08:39.761213 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-28 21:08:39.845981 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 21:08:39.846636 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 21:08:39.852543 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:08:39.852996 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 21:08:39.854431 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:39.854904 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:08:39.855190 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 21:08:39.863527 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 21:08:39.879583 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 21:08:39.945034 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.945485 (MainThread): On master: BEGIN
2022-07-28 21:08:39.957507 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 21:08:39.958040 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.958355 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 21:08:39.978181 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-28 21:08:39.982949 (MainThread): On master: ROLLBACK
2022-07-28 21:08:39.984088 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.984541 (MainThread): On master: BEGIN
2022-07-28 21:08:39.985991 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:39.986523 (MainThread): On master: COMMIT
2022-07-28 21:08:39.986834 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.987103 (MainThread): On master: COMMIT
2022-07-28 21:08:39.987859 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:08:39.989168 (MainThread): 21:08:39 | Concurrency: 1 threads (target='dev')
2022-07-28 21:08:39.989695 (MainThread): 21:08:39 | 
2022-07-28 21:08:40.031700 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:08:40.032384 (Thread-1): 21:08:40 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 21:08:40.033382 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.033744 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 21:08:40.034060 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:08:40.108003 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:08:40.117092 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.216823 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.217319 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 21:08:40.218693 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.228213 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.228660 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:08:40.229734 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.296737 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 21:08:40.297917 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.298377 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 21:08:40.299439 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:40.299897 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.300175 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 21:08:40.379417 (Thread-1): SQL status: SELECT 6 in 0.08 seconds
2022-07-28 21:08:40.393824 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.394264 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 21:08:40.395690 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.405123 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.405569 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 21:08:40.406829 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.410083 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:08:40.410527 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.410805 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:08:40.429692 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 21:08:40.437071 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.437553 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:08:40.452523 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 21:08:40.463111 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.471164 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da4e3e9d-5a3a-43e4-80d5-d3d4c8808a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa8277910>]}
2022-07-28 21:08:40.472145 (Thread-1): 21:08:40 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.43s]
2022-07-28 21:08:40.472626 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:08:40.473729 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:08:40.474280 (Thread-1): 21:08:40 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 21:08:40.475628 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.476020 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 21:08:40.476315 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:08:40.515530 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:08:40.516573 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.543221 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.543761 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 21:08:40.545077 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.567098 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.568868 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:08:40.571734 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.578228 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 21:08:40.579946 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.580385 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 21:08:40.581121 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:40.581544 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.582534 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:08:40.662469 (Thread-1): SQL status: SELECT 922 in 0.08 seconds
2022-07-28 21:08:40.677600 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.678049 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 21:08:40.679502 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.688460 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.688908 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 21:08:40.690235 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.693859 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:08:40.694308 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.694586 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:08:40.708743 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-28 21:08:40.715871 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.716318 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:08:40.732239 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 21:08:40.742323 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.744766 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da4e3e9d-5a3a-43e4-80d5-d3d4c8808a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa829cfd0>]}
2022-07-28 21:08:40.745606 (Thread-1): 21:08:40 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.27s]
2022-07-28 21:08:40.746060 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:08:40.746629 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:08:40.747410 (Thread-1): 21:08:40 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 21:08:40.748691 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.749069 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 21:08:40.749365 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:08:40.784354 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:08:40.785410 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.812471 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.812946 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 21:08:40.814037 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.831247 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.831764 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:08:40.832820 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.838304 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:08:40.839845 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.840280 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 21:08:40.841031 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:40.841448 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.841726 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:08:41.817798 (Thread-1): SQL status: SELECT 922 in 0.98 seconds
2022-07-28 21:08:41.833710 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.834160 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 21:08:41.835593 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:41.844951 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.845411 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 21:08:41.846728 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:41.850150 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:08:41.850597 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.850883 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:08:41.910796 (Thread-1): SQL status: COMMIT in 0.06 seconds
2022-07-28 21:08:41.917620 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.918114 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:08:41.955096 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 21:08:41.964534 (Thread-1): finished collecting timing info
2022-07-28 21:08:41.967916 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da4e3e9d-5a3a-43e4-80d5-d3d4c8808a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa83f9e50>]}
2022-07-28 21:08:41.968784 (Thread-1): 21:08:41 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 1.22s]
2022-07-28 21:08:41.969242 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:08:42.069905 (MainThread): Using postgres connection "master".
2022-07-28 21:08:42.071076 (MainThread): On master: BEGIN
2022-07-28 21:08:42.073520 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:42.074730 (MainThread): On master: COMMIT
2022-07-28 21:08:42.076357 (MainThread): Using postgres connection "master".
2022-07-28 21:08:42.076685 (MainThread): On master: COMMIT
2022-07-28 21:08:42.077739 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:08:42.078992 (MainThread): 21:08:42 | 
2022-07-28 21:08:42.079594 (MainThread): 21:08:42 | Finished running 3 table models in 2.94s.
2022-07-28 21:08:42.079987 (MainThread): Connection 'master' was left open.
2022-07-28 21:08:42.080281 (MainThread): On master: Close
2022-07-28 21:08:42.086552 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 21:08:42.087038 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 21:08:42.149643 (MainThread): 
2022-07-28 21:08:42.151950 (MainThread): Completed successfully
2022-07-28 21:08:42.152482 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 21:08:42.153085 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a9a04b350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa83b7790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa831a890>]}
2022-07-28 21:08:42.160856 (MainThread): Flushing usage events
2022-07-28 21:08:58.638216 (MainThread): Running with dbt=0.16.1
2022-07-28 21:08:58.902426 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 21:08:58.903687 (MainThread): Tracking: tracking
2022-07-28 21:08:58.927798 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558ef8d690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558f0d4410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558efc1290>]}
2022-07-28 21:08:59.018592 (MainThread): Partial parsing not enabled
2022-07-28 21:08:59.024735 (MainThread): Parsing macros/core.sql
2022-07-28 21:08:59.045579 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:08:59.080100 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:08:59.088161 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:08:59.219569 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:08:59.302332 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:08:59.330551 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:08:59.339577 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:08:59.443109 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:08:59.522748 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:08:59.572887 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:08:59.605065 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:08:59.634598 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:08:59.814984 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:08:59.820077 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:08:59.829304 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:08:59.839481 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:08:59.843972 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:08:59.851944 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:08:59.857080 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:08:59.896976 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:08:59.901850 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:08:59.916750 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:08:59.922427 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:08:59.929108 (MainThread): Parsing macros/relations.sql
2022-07-28 21:08:59.936044 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:09:00.019814 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:09:00.030796 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:09:00.122311 (MainThread): Partial parsing not enabled
2022-07-28 21:09:00.261992 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:09:00.262438 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:00.335481 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:09:00.335926 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:00.366593 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:09:00.367019 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:01.009566 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:09:02.291637 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:09:02.314423 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:09:02.323232 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:09:02.345732 (MainThread): 
2022-07-28 21:09:02.346416 (MainThread): 21:09:02 | Concurrency: 1 threads (target='dev')
2022-07-28 21:09:02.346813 (MainThread): 21:09:02 | 
2022-07-28 21:09:02.396025 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:09:02.397303 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:09:02.403609 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:09:02.404260 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:09:02.540942 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:09:02.542021 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.543423 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.545047 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:09:02.546548 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:09:02.548166 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:09:02.548822 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:09:02.549284 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:09:02.698337 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:09:02.699433 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.700442 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.701968 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:09:02.702531 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:09:02.708990 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:09:02.709432 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:09:02.709736 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:09:02.744799 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:09:02.745995 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.747223 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.749032 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:09:02.781824 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:09:02.782335 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:09:02.818180 (MainThread): 21:09:02 | Done.
2022-07-28 21:09:03.009471 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 21:09:03.010172 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:03.011471 (MainThread): 21:09:03 | Building catalog
2022-07-28 21:09:03.250533 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 21:09:03.252844 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:09:04.042102 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.042576 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:09:04.068189 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.03 seconds
2022-07-28 21:09:04.068734 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.069033 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:09:04.495177 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.43 seconds
2022-07-28 21:09:04.610562 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:09:04.649804 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.650271 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:09:04.654572 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:09:04.655066 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.655447 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:09:04.658686 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-28 21:09:04.665607 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:09:05.166273 (MainThread): 21:09:05 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 21:09:05.169699 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558c70cdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55814538d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558c70c7d0>]}
2022-07-28 21:09:05.170443 (MainThread): Flushing usage events
2022-07-28 21:09:06.055242 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 21:09:06.057769 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 21:09:06.058258 (MainThread): On warehouse.information_schema: Close
2022-07-28 21:09:14.126461 (MainThread): Running with dbt=0.16.1
2022-07-28 21:09:14.915677 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=8080, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 21:09:14.916906 (MainThread): Tracking: tracking
2022-07-28 21:09:14.957003 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd549c5c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd549c0cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd549e5510>]}
2022-07-28 21:09:14.977570 (MainThread): Serving docs at 0.0.0.0:8080
2022-07-28 21:09:14.978605 (MainThread): To access from your browser, navigate to:  http://localhost:8080
2022-07-28 21:09:14.979225 (MainThread): Press Ctrl+C to exit.


2022-07-28 21:09:14.983837 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd54aa80d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd54ab8710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd54ab8e50>]}
2022-07-28 21:09:14.985121 (MainThread): Flushing usage events
2022-07-28 21:09:15.912719 (MainThread): Encountered an error:
2022-07-28 21:09:15.914453 (MainThread): [Errno 98] Address already in use
2022-07-28 21:09:15.959532 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/serve.py", line 31, in run
    SimpleHTTPRequestHandler  # type: ignore
  File "/usr/local/lib/python3.7/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/usr/local/lib/python3.7/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use

2022-07-28 21:25:19.703695 (MainThread): Running with dbt=0.16.1
2022-07-28 21:25:20.033141 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 21:25:20.097706 (MainThread): Tracking: tracking
2022-07-28 21:25:20.157754 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f786974a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7869804250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f78697a3e90>]}
2022-07-28 21:25:20.264712 (MainThread): Partial parsing not enabled
2022-07-28 21:25:20.320058 (MainThread): Parsing macros/core.sql
2022-07-28 21:25:20.341931 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:25:20.378518 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:25:20.387712 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:25:20.519641 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:25:20.599742 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:25:20.630074 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:25:20.639693 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:25:20.736683 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:25:20.827797 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:25:20.851892 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:25:20.880736 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:25:20.968318 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:25:21.528681 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:25:21.543410 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:25:21.574256 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:25:21.601904 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:25:21.622223 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:25:21.762952 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:25:21.795493 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:25:21.917429 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:25:21.932702 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:25:21.964052 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:25:21.978343 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:25:22.076416 (MainThread): Parsing macros/relations.sql
2022-07-28 21:25:22.096148 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:25:22.322848 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:25:22.362939 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:25:22.704615 (MainThread): Partial parsing not enabled
2022-07-28 21:25:23.311061 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:23.312878 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:23.431488 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:23.432496 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:23.481906 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:23.482940 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:25.115741 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:25:27.947018 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:25:27.999674 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:25:28.003187 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:25:28.038510 (MainThread): 
2022-07-28 21:25:28.040508 (MainThread): Acquiring new postgres connection "master".
2022-07-28 21:25:28.041157 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:28.207719 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 21:25:28.213440 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:25:29.091647 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 21:25:29.092299 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 21:25:29.132231 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.04 seconds
2022-07-28 21:25:29.227619 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 21:25:29.228047 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 21:25:29.236494 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:25:29.237185 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 21:25:29.238368 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:29.238836 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:25:29.239132 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 21:25:29.247438 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 21:25:29.263056 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 21:25:29.326529 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.326979 (MainThread): On master: BEGIN
2022-07-28 21:25:29.342925 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 21:25:29.343973 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.345141 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 21:25:29.367548 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-28 21:25:29.372201 (MainThread): On master: ROLLBACK
2022-07-28 21:25:29.373144 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.373543 (MainThread): On master: BEGIN
2022-07-28 21:25:29.374836 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:29.375321 (MainThread): On master: COMMIT
2022-07-28 21:25:29.375944 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.376247 (MainThread): On master: COMMIT
2022-07-28 21:25:29.376973 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:25:29.378209 (MainThread): 21:25:29 | Concurrency: 1 threads (target='dev')
2022-07-28 21:25:29.378720 (MainThread): 21:25:29 | 
2022-07-28 21:25:29.426580 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:25:29.427280 (Thread-1): 21:25:29 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 21:25:29.428341 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.428750 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 21:25:29.429080 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:25:29.497192 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:25:29.517117 (Thread-1): finished collecting timing info
2022-07-28 21:25:29.601028 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.601516 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 21:25:29.602863 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:29.610977 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.611509 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:25:29.612510 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:29.678829 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 21:25:29.731985 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.732497 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 21:25:29.733490 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:29.734043 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.734335 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 21:25:29.904909 (Thread-1): SQL status: SELECT 6 in 0.17 seconds
2022-07-28 21:25:29.920690 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.921341 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 21:25:29.923224 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:29.934069 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.934753 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 21:25:29.937199 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:29.940832 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:25:29.941458 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.942177 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:25:30.014988 (Thread-1): SQL status: COMMIT in 0.07 seconds
2022-07-28 21:25:30.027577 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:30.028184 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:25:30.070910 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 21:25:30.086978 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.089474 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3b5d0f-1f7d-491d-99e9-ce1526535d2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f786978c250>]}
2022-07-28 21:25:30.090391 (Thread-1): 21:25:30 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.66s]
2022-07-28 21:25:30.091440 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:25:30.092956 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:25:30.093538 (Thread-1): 21:25:30 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 21:25:30.094961 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.095413 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 21:25:30.095742 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:25:30.138622 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:25:30.140012 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.166547 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.167027 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 21:25:30.168703 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.177468 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.177927 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:25:30.179716 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.185696 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 21:25:30.186971 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.187545 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 21:25:30.188886 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:30.189411 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.189702 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:25:30.310984 (Thread-1): SQL status: SELECT 922 in 0.12 seconds
2022-07-28 21:25:30.334207 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.334673 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 21:25:30.337983 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:30.355763 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.356212 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 21:25:30.358630 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:30.365734 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:25:30.366326 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.368642 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:25:30.391874 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 21:25:30.399105 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.399806 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:25:30.415605 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 21:25:30.424655 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.427587 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3b5d0f-1f7d-491d-99e9-ce1526535d2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866fa1390>]}
2022-07-28 21:25:30.428614 (Thread-1): 21:25:30 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.33s]
2022-07-28 21:25:30.429560 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:25:30.430285 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:25:30.430953 (Thread-1): 21:25:30 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 21:25:30.433054 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.433644 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 21:25:30.434098 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:25:30.469035 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:25:30.470302 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.500598 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.501318 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 21:25:30.503018 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.517361 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.518019 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:25:30.519221 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.524658 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:25:30.526439 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.527059 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 21:25:30.527978 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:30.528614 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.529066 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:25:31.842555 (Thread-1): SQL status: SELECT 922 in 1.31 seconds
2022-07-28 21:25:31.864682 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:31.865704 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 21:25:31.868330 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:31.880465 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:31.881233 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 21:25:31.882987 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:31.886976 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:25:31.888230 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:31.888818 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:25:32.047934 (Thread-1): SQL status: COMMIT in 0.16 seconds
2022-07-28 21:25:32.081201 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:32.091861 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:25:32.253226 (Thread-1): SQL status: DROP TABLE in 0.16 seconds
2022-07-28 21:25:32.267962 (Thread-1): finished collecting timing info
2022-07-28 21:25:32.270361 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3b5d0f-1f7d-491d-99e9-ce1526535d2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866fa1390>]}
2022-07-28 21:25:32.271190 (Thread-1): 21:25:32 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 1.84s]
2022-07-28 21:25:32.275638 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:25:32.296067 (MainThread): Using postgres connection "master".
2022-07-28 21:25:32.296713 (MainThread): On master: BEGIN
2022-07-28 21:25:32.297651 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:32.298291 (MainThread): On master: COMMIT
2022-07-28 21:25:32.298742 (MainThread): Using postgres connection "master".
2022-07-28 21:25:32.299147 (MainThread): On master: COMMIT
2022-07-28 21:25:32.300451 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:25:32.301839 (MainThread): 21:25:32 | 
2022-07-28 21:25:32.302970 (MainThread): 21:25:32 | Finished running 3 table models in 4.26s.
2022-07-28 21:25:32.304036 (MainThread): Connection 'master' was left open.
2022-07-28 21:25:32.304568 (MainThread): On master: Close
2022-07-28 21:25:32.310580 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 21:25:32.311258 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 21:25:32.370685 (MainThread): 
2022-07-28 21:25:32.372164 (MainThread): Completed successfully
2022-07-28 21:25:32.373381 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 21:25:32.374831 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866eef750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866fddb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7869804f50>]}
2022-07-28 21:25:32.380243 (MainThread): Flushing usage events
2022-07-28 21:25:49.135999 (MainThread): Running with dbt=0.16.1
2022-07-28 21:25:49.454608 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 21:25:49.456087 (MainThread): Tracking: tracking
2022-07-28 21:25:49.475656 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f909f590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93fa314c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f9045d10>]}
2022-07-28 21:25:49.564025 (MainThread): Partial parsing not enabled
2022-07-28 21:25:49.570571 (MainThread): Parsing macros/core.sql
2022-07-28 21:25:49.592070 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:25:49.627323 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:25:49.635940 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:25:49.764637 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:25:49.843860 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:25:49.873279 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:25:49.882434 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:25:49.978255 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:25:50.034598 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:25:50.057629 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:25:50.086011 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:25:50.116194 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:25:50.350677 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:25:50.355737 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:25:50.365384 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:25:50.376058 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:25:50.380877 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:25:50.388738 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:25:50.394210 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:25:50.437366 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:25:50.450277 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:25:50.474963 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:25:50.480598 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:25:50.487015 (MainThread): Parsing macros/relations.sql
2022-07-28 21:25:50.494733 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:25:50.575523 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:25:50.586539 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:25:50.685498 (MainThread): Partial parsing not enabled
2022-07-28 21:25:50.827249 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:50.827780 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:50.909269 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:50.909718 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:50.939607 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:50.940060 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:51.655915 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:25:52.542091 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:25:52.551922 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:25:52.553239 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:25:52.562948 (MainThread): 
2022-07-28 21:25:52.564370 (MainThread): 21:25:52 | Concurrency: 1 threads (target='dev')
2022-07-28 21:25:52.565461 (MainThread): 21:25:52 | 
2022-07-28 21:25:52.584428 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:25:52.585925 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:52.586570 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:25:52.587067 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:25:52.656958 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:25:52.658280 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.659672 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.661493 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:25:52.662863 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:25:52.664183 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:52.664804 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:25:52.665280 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:25:52.805505 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:25:52.806766 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.808184 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.809952 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:25:52.810739 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:25:52.812178 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:52.812813 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:25:52.813291 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:25:52.845464 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:25:52.846708 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.848030 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.849769 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:25:52.888175 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:25:52.888841 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:25:52.920313 (MainThread): 21:25:52 | Done.
2022-07-28 21:25:52.989275 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 21:25:52.989732 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:52.990030 (MainThread): 21:25:52 | Building catalog
2022-07-28 21:25:53.136343 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.136805 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:25:53.470129 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.470569 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:25:53.490583 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.02 seconds
2022-07-28 21:25:53.491667 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.492204 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:25:53.683156 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.19 seconds
2022-07-28 21:25:53.702559 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:25:53.719212 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.719894 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:25:53.721685 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:53.722695 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.723181 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:25:53.726692 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-28 21:25:53.732097 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:25:53.954993 (MainThread): 21:25:53 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 21:25:53.955875 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e83b0490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f672ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f6863510>]}
2022-07-28 21:25:53.956878 (MainThread): Flushing usage events
2022-07-28 21:25:54.896497 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 21:25:54.898348 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 21:25:54.898890 (MainThread): On warehouse.information_schema: Close
2022-07-28 21:25:58.111235 (MainThread): Running with dbt=0.16.1
2022-07-28 21:25:58.475181 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 21:25:58.484392 (MainThread): Tracking: tracking
2022-07-28 21:25:58.519452 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd63d9990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd648cb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd6491e90>]}
2022-07-28 21:25:58.545972 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-28 21:25:58.552690 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-28 21:25:58.558434 (MainThread): Press Ctrl+C to exit.


2022-07-28 22:15:21.026669 (MainThread): Running with dbt=0.16.1
2022-07-28 22:15:21.366822 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 22:15:21.414560 (MainThread): Tracking: tracking
2022-07-28 22:15:21.490661 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b48b35c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b489fcc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b489fcbd0>]}
2022-07-28 22:15:21.633658 (MainThread): Partial parsing not enabled
2022-07-28 22:15:21.665214 (MainThread): Parsing macros/core.sql
2022-07-28 22:15:21.687439 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:15:21.725629 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:15:21.735061 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:15:21.874273 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:15:21.962565 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:15:21.992136 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:15:22.001693 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:15:22.144832 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:15:22.204394 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:15:22.269729 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:15:22.314349 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:15:22.364116 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:15:22.555827 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:15:22.565415 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:15:22.582569 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:15:22.593773 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:15:22.599012 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:15:22.607762 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:15:22.613706 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:15:22.656602 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:15:22.661570 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:15:22.671790 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:15:22.679564 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:15:22.719756 (MainThread): Parsing macros/relations.sql
2022-07-28 22:15:22.731044 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:15:22.923195 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:15:22.956237 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:15:23.160773 (MainThread): Partial parsing not enabled
2022-07-28 22:15:23.360710 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:23.361144 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:23.474958 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:23.487646 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:23.588174 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:23.588611 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:23.977705 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:15:23.978159 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:25.101929 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:15:26.605605 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:15:26.656468 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:15:26.658212 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:15:26.682310 (MainThread): 
2022-07-28 22:15:26.696731 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:15:26.697162 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:27.266338 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 22:15:27.267229 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:15:27.854478 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 22:15:27.855559 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 22:15:27.888373 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-07-28 22:15:28.088275 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:15:28.089563 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 22:15:28.100139 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:15:28.101081 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:15:28.102556 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:28.103271 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:15:28.104094 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:15:28.114914 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 22:15:28.137134 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:15:28.270769 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.271450 (MainThread): On master: BEGIN
2022-07-28 22:15:28.284598 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:15:28.285244 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.285676 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:15:28.327478 (MainThread): SQL status: SELECT 0 in 0.04 seconds
2022-07-28 22:15:28.330630 (MainThread): On master: ROLLBACK
2022-07-28 22:15:28.336647 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.337268 (MainThread): On master: BEGIN
2022-07-28 22:15:28.338642 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:28.339682 (MainThread): On master: COMMIT
2022-07-28 22:15:28.340222 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.340647 (MainThread): On master: COMMIT
2022-07-28 22:15:28.342752 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:15:28.344082 (MainThread): 22:15:28 | Concurrency: 1 threads (target='dev')
2022-07-28 22:15:28.347113 (MainThread): 22:15:28 | 
2022-07-28 22:15:28.434035 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:15:28.434687 (Thread-1): 22:15:28 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 22:15:28.435783 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:28.436157 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:15:28.436532 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:15:28.560765 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:15:28.575554 (Thread-1): finished collecting timing info
2022-07-28 22:15:28.817587 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:28.818236 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 22:15:28.820609 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:28.838381 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:28.843871 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:15:28.845037 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:28.997082 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 22:15:29.008066 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.008534 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 22:15:29.009541 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:29.009984 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.010254 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 22:15:29.125537 (Thread-1): SQL status: SELECT 6 in 0.11 seconds
2022-07-28 22:15:29.163782 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.166014 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 22:15:29.167640 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.180078 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.181071 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 22:15:29.182445 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.185552 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:15:29.185981 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.186247 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:15:29.210409 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 22:15:29.218386 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.218858 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:15:29.232852 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 22:15:29.248094 (Thread-1): finished collecting timing info
2022-07-28 22:15:29.253970 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '273c9572-1c03-4263-99f3-51eeac0449de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b462d9150>]}
2022-07-28 22:15:29.254869 (Thread-1): 22:15:29 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.82s]
2022-07-28 22:15:29.258124 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:15:29.259425 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:15:29.260009 (Thread-1): 22:15:29 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 22:15:29.261458 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.261855 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 22:15:29.262154 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:15:29.302977 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:15:29.304422 (Thread-1): finished collecting timing info
2022-07-28 22:15:29.338371 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.338860 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 22:15:29.340011 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:29.349388 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.349848 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:15:29.350876 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:29.356937 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 22:15:29.358228 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.358648 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 22:15:29.359430 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:29.359869 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.360143 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:15:29.607297 (Thread-1): SQL status: SELECT 922 in 0.25 seconds
2022-07-28 22:15:29.629477 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.630091 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 22:15:29.631549 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.644696 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.645156 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 22:15:29.646533 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.659861 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:15:29.660326 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.660609 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:15:29.699096 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 22:15:29.706641 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.707135 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:15:29.811249 (Thread-1): SQL status: DROP TABLE in 0.10 seconds
2022-07-28 22:15:29.821890 (Thread-1): finished collecting timing info
2022-07-28 22:15:29.832714 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '273c9572-1c03-4263-99f3-51eeac0449de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b46296350>]}
2022-07-28 22:15:29.836413 (Thread-1): 22:15:29 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.56s]
2022-07-28 22:15:29.837382 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:15:29.837964 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:15:29.838674 (Thread-1): 22:15:29 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 22:15:29.844398 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:29.844912 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 22:15:29.845358 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:15:29.928017 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:15:29.929107 (Thread-1): finished collecting timing info
2022-07-28 22:15:30.021514 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.022071 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 22:15:30.027588 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:30.045539 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.051666 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:15:30.054133 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:30.063547 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:15:30.068185 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.068610 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 22:15:30.071671 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:30.072201 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.072496 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:15:47.959397 (Thread-1): SQL status: SELECT 922 in 17.89 seconds
2022-07-28 22:15:48.030098 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.030544 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 22:15:48.032039 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:48.042749 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.044932 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 22:15:48.046471 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:48.050284 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:15:48.050730 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.051013 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:15:48.708028 (Thread-1): SQL status: COMMIT in 0.66 seconds
2022-07-28 22:15:48.717063 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.717506 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:15:49.083632 (Thread-1): SQL status: DROP TABLE in 0.37 seconds
2022-07-28 22:15:49.092949 (Thread-1): finished collecting timing info
2022-07-28 22:15:49.265531 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '273c9572-1c03-4263-99f3-51eeac0449de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b462ff550>]}
2022-07-28 22:15:49.266393 (Thread-1): 22:15:49 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 19.42s]
2022-07-28 22:15:49.266856 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:15:49.366096 (MainThread): Using postgres connection "master".
2022-07-28 22:15:49.366561 (MainThread): On master: BEGIN
2022-07-28 22:15:49.367631 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:49.368087 (MainThread): On master: COMMIT
2022-07-28 22:15:49.368369 (MainThread): Using postgres connection "master".
2022-07-28 22:15:49.368630 (MainThread): On master: COMMIT
2022-07-28 22:15:49.369562 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:15:49.370787 (MainThread): 22:15:49 | 
2022-07-28 22:15:49.371276 (MainThread): 22:15:49 | Finished running 3 table models in 22.67s.
2022-07-28 22:15:49.372731 (MainThread): Connection 'master' was left open.
2022-07-28 22:15:49.373049 (MainThread): On master: Close
2022-07-28 22:15:49.379082 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 22:15:49.379789 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 22:15:49.428725 (MainThread): 
2022-07-28 22:15:49.430526 (MainThread): Completed successfully
2022-07-28 22:15:49.431469 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 22:15:49.432707 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b461e27d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b461fdfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b48a3cd50>]}
2022-07-28 22:15:49.433413 (MainThread): Flushing usage events
2022-07-28 22:16:58.071110 (MainThread): Running with dbt=0.16.1
2022-07-28 22:16:58.656935 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 22:16:58.687105 (MainThread): Tracking: tracking
2022-07-28 22:16:58.718001 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc016e9f650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc016fd46d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc016fdad50>]}
2022-07-28 22:16:58.887884 (MainThread): Partial parsing not enabled
2022-07-28 22:16:58.999953 (MainThread): Parsing macros/core.sql
2022-07-28 22:16:59.059886 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:16:59.182856 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:16:59.239160 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:16:59.589707 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:16:59.787925 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:16:59.819423 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:16:59.828800 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:16:59.928146 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:16:59.987533 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:17:00.012778 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:17:00.042210 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:17:00.073145 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:17:00.278230 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:17:00.284271 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:17:00.294937 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:17:00.330588 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:17:00.350400 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:17:00.383935 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:17:00.395246 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:17:00.516101 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:17:00.533578 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:17:00.559891 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:17:00.574516 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:17:00.616419 (MainThread): Parsing macros/relations.sql
2022-07-28 22:17:00.631908 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:17:00.773883 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:17:00.787590 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:17:00.921877 (MainThread): Partial parsing not enabled
2022-07-28 22:17:01.275973 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:17:01.276618 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:01.471482 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:17:01.471940 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:01.545002 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:17:01.545627 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:02.094844 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:17:02.102194 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:03.538715 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:17:05.603023 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:17:05.637931 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:17:05.643637 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:17:05.677293 (MainThread): 
2022-07-28 22:17:05.678467 (MainThread): 22:17:05 | Concurrency: 1 threads (target='dev')
2022-07-28 22:17:05.684069 (MainThread): 22:17:05 | 
2022-07-28 22:17:05.798082 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:17:05.799457 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:17:05.799900 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:05.800220 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:17:06.208004 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:17:06.209628 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.214523 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.216160 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:17:06.220634 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:17:06.225233 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:17:06.226029 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:06.226595 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:17:06.281285 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:17:06.283146 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.285594 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.288800 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:17:06.289366 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:17:06.300454 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:17:06.300930 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:06.301242 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:17:06.387321 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:17:06.394260 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.395488 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.397001 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:17:06.397560 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:17:06.399050 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:17:06.401010 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:06.401498 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:17:06.539915 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:17:06.563142 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.564567 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.569158 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:17:06.640026 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:17:06.640495 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:17:06.738109 (MainThread): 22:17:06 | Done.
2022-07-28 22:17:06.789431 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 22:17:06.792221 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:06.792658 (MainThread): 22:17:06 | Building catalog
2022-07-28 22:17:07.039548 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.040602 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:17:07.644416 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.645017 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:17:07.705186 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.06 seconds
2022-07-28 22:17:07.705770 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.706143 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:17:07.805527 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.10 seconds
2022-07-28 22:17:07.851252 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:17:07.875574 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.876066 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:17:07.878599 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:17:07.879275 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.879654 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:17:07.882961 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-28 22:17:07.890470 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:17:08.163485 (MainThread): 22:17:08 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 22:17:08.164354 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc01459df90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc01459d690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc01459d7d0>]}
2022-07-28 22:17:08.165152 (MainThread): Flushing usage events
2022-07-28 22:17:09.237337 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 22:17:09.239077 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 22:17:09.239458 (MainThread): On warehouse.information_schema: Close
2022-07-28 22:17:13.104226 (MainThread): Running with dbt=0.16.1
2022-07-28 22:17:13.614043 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 22:17:13.618711 (MainThread): Tracking: tracking
2022-07-28 22:17:13.671297 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f731d7d58d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f731d82dbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f731d6b6390>]}
2022-07-28 22:17:13.696274 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-28 22:17:13.700651 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-28 22:17:13.702273 (MainThread): Press Ctrl+C to exit.


2022-07-28 22:21:43.372757 (MainThread): Running with dbt=0.16.1
2022-07-28 22:21:44.685939 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 22:21:44.898380 (MainThread): Tracking: tracking
2022-07-28 22:21:46.039943 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374f2ebd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374f2eb050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374f307810>]}
2022-07-28 22:21:46.216624 (MainThread): Partial parsing not enabled
2022-07-28 22:21:46.327744 (MainThread): Parsing macros/core.sql
2022-07-28 22:21:46.358497 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:21:46.403298 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:21:46.414912 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:21:46.615015 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:21:46.706229 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:21:46.802476 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:21:46.814615 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:21:47.002594 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:21:47.138371 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:21:47.204724 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:21:47.277072 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:21:47.385625 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:21:48.000576 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:21:48.011866 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:21:48.033683 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:21:48.071775 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:21:48.081312 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:21:48.107841 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:21:48.121836 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:21:48.234290 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:21:48.243588 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:21:48.271371 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:21:48.287704 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:21:48.381468 (MainThread): Parsing macros/relations.sql
2022-07-28 22:21:48.414684 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:21:48.575786 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:21:48.622327 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:21:48.875583 (MainThread): Partial parsing not enabled
2022-07-28 22:21:49.187688 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:21:49.188177 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:49.292413 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:21:49.292863 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:49.344079 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:21:49.344562 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:49.544530 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:49.544966 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:50.590779 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:21:52.678516 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:21:52.692153 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:21:52.693245 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:21:52.705514 (MainThread): 
2022-07-28 22:21:52.708177 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:21:52.708636 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:53.092702 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:21:53.093597 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 22:21:53.933944 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:21:53.934422 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:21:53.999030 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.06 seconds
2022-07-28 22:21:54.000380 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:21:54.002144 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:21:54.018015 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.02 seconds
2022-07-28 22:21:54.138963 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:21:54.388524 (MainThread): Using postgres connection "master".
2022-07-28 22:21:54.388980 (MainThread): On master: BEGIN
2022-07-28 22:21:54.408573 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-28 22:21:54.409069 (MainThread): Using postgres connection "master".
2022-07-28 22:21:54.409430 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:21:54.452240 (MainThread): SQL status: SELECT 0 in 0.04 seconds
2022-07-28 22:21:54.455237 (MainThread): On master: ROLLBACK
2022-07-28 22:21:54.456843 (MainThread): 22:21:54 | Concurrency: 1 threads (target='dev')
2022-07-28 22:21:54.457382 (MainThread): 22:21:54 | 
2022-07-28 22:21:54.517308 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:21:54.517993 (Thread-1): 22:21:54 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-28 22:21:54.523864 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:54.532549 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:21:54.535700 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:21:54.736989 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:21:54.782911 (Thread-1): finished collecting timing info
2022-07-28 22:21:54.784150 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:54.784683 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-28 22:21:54.786209 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:21:54.786722 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:54.787025 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-28 22:21:54.901268 (Thread-1): SQL status: SELECT 1 in 0.11 seconds
2022-07-28 22:21:54.902275 (Thread-1): finished collecting timing info
2022-07-28 22:21:54.903330 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-28 22:21:54.905098 (Thread-1): 22:21:54 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.38s]
2022-07-28 22:21:54.905627 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:21:54.926221 (MainThread): 22:21:54 | 
2022-07-28 22:21:54.928160 (MainThread): 22:21:54 | Finished running 1 test in 2.22s.
2022-07-28 22:21:54.928648 (MainThread): Connection 'master' was left open.
2022-07-28 22:21:54.928945 (MainThread): On master: Close
2022-07-28 22:21:54.936271 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-28 22:21:54.937305 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-28 22:21:54.964828 (MainThread): 
2022-07-28 22:21:54.965396 (MainThread): Completed successfully
2022-07-28 22:21:54.965802 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-28 22:21:54.966423 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374c9f3d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374ca08550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f373e7c3350>]}
2022-07-28 22:21:54.967153 (MainThread): Flushing usage events
2022-07-28 22:29:18.061486 (MainThread): Running with dbt=0.16.1
2022-07-28 22:29:18.683074 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 22:29:18.738599 (MainThread): Tracking: tracking
2022-07-28 22:29:18.786431 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce99a9e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5cea4b4a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce9ae56d0>]}
2022-07-28 22:29:18.918216 (MainThread): Partial parsing not enabled
2022-07-28 22:29:18.955853 (MainThread): Parsing macros/core.sql
2022-07-28 22:29:18.984961 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:29:19.031508 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:29:19.040180 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:29:19.180459 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:29:19.267484 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:29:19.298818 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:29:19.308836 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:29:19.412542 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:29:19.486390 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:29:19.516669 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:29:19.549525 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:29:19.581532 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:29:19.781941 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:29:19.788399 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:29:19.800828 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:29:19.847945 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:29:19.860196 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:29:19.878597 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:29:19.888802 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:29:19.944125 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:29:19.952843 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:29:19.990876 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:29:20.007844 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:29:20.053784 (MainThread): Parsing macros/relations.sql
2022-07-28 22:29:20.064722 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:29:20.206688 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:29:20.233778 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:29:20.357965 (MainThread): Partial parsing not enabled
2022-07-28 22:29:20.583932 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:20.584390 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:20.696731 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:20.697175 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:20.760769 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:20.761214 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:20.936519 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:29:20.936948 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:21.773088 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:29:24.696176 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:29:24.744233 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:29:24.745382 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:29:24.836865 (MainThread): 
2022-07-28 22:29:24.839067 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:29:24.848536 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:25.781197 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 22:29:25.782386 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:29:26.299945 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 22:29:26.300421 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 22:29:26.448331 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.15 seconds
2022-07-28 22:29:26.680747 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:29:26.681234 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 22:29:26.691412 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:29:26.692140 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:29:26.704702 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:29:26.705223 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:29:26.705506 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:29:26.845801 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.14 seconds
2022-07-28 22:29:26.871606 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:29:27.020777 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.021235 (MainThread): On master: BEGIN
2022-07-28 22:29:27.032527 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:29:27.032991 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.033268 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:29:27.131932 (MainThread): SQL status: SELECT 0 in 0.10 seconds
2022-07-28 22:29:27.134959 (MainThread): On master: ROLLBACK
2022-07-28 22:29:27.136057 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.136509 (MainThread): On master: BEGIN
2022-07-28 22:29:27.137826 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:27.138733 (MainThread): On master: COMMIT
2022-07-28 22:29:27.139781 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.140114 (MainThread): On master: COMMIT
2022-07-28 22:29:27.143251 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:29:27.144471 (MainThread): 22:29:27 | Concurrency: 1 threads (target='dev')
2022-07-28 22:29:27.144970 (MainThread): 22:29:27 | 
2022-07-28 22:29:27.225540 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:29:27.226387 (Thread-1): 22:29:27 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 22:29:27.228861 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.229668 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:29:27.230208 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:29:27.445724 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:29:27.604320 (Thread-1): finished collecting timing info
2022-07-28 22:29:27.720977 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.724544 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 22:29:27.795632 (Thread-1): SQL status: DROP TABLE in 0.07 seconds
2022-07-28 22:29:27.804688 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.805153 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:29:27.806393 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:27.916070 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 22:29:27.950085 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.950701 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 22:29:27.953194 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:27.953826 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.954241 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 22:29:28.221450 (Thread-1): SQL status: SELECT 6 in 0.27 seconds
2022-07-28 22:29:28.237766 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.238230 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 22:29:28.239837 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.261853 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.262514 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 22:29:28.264852 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.272056 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:29:28.273158 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.273785 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:29:28.302960 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 22:29:28.314096 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.314580 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:29:28.357925 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 22:29:28.372554 (Thread-1): finished collecting timing info
2022-07-28 22:29:28.376334 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d326e90-8b69-4b3f-b3a4-c88996af35f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5cd90ed790>]}
2022-07-28 22:29:28.377431 (Thread-1): 22:29:28 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.15s]
2022-07-28 22:29:28.378103 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:29:28.388598 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:29:28.389489 (Thread-1): 22:29:28 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 22:29:28.390673 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.397025 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 22:29:28.397516 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:29:28.503175 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:29:28.507240 (Thread-1): finished collecting timing info
2022-07-28 22:29:28.569801 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.571643 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 22:29:28.573039 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:28.594139 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.595308 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:29:28.598189 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:28.614907 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 22:29:28.616409 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.616990 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 22:29:28.617892 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:28.618645 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.619142 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:29:28.723791 (Thread-1): SQL status: SELECT 922 in 0.10 seconds
2022-07-28 22:29:28.752901 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.753561 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 22:29:28.763190 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.781191 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.781854 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 22:29:28.783309 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.786861 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:29:28.788456 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.793685 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:29:28.803559 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-28 22:29:28.810905 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.813605 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:29:28.826382 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 22:29:28.840286 (Thread-1): finished collecting timing info
2022-07-28 22:29:28.845227 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d326e90-8b69-4b3f-b3a4-c88996af35f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce71aa150>]}
2022-07-28 22:29:28.848312 (Thread-1): 22:29:28 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.45s]
2022-07-28 22:29:28.850536 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:29:28.855967 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:29:28.857021 (Thread-1): 22:29:28 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 22:29:28.858363 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:28.859754 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 22:29:28.860388 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:29:28.966475 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:29:28.974763 (Thread-1): finished collecting timing info
2022-07-28 22:29:29.063221 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.063787 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 22:29:29.065260 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:29.074981 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.075573 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:29:29.077344 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:29.083596 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:29:29.086184 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.086643 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 22:29:29.087479 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:29.087897 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.088177 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:29:33.950235 (Thread-1): SQL status: SELECT 922 in 4.86 seconds
2022-07-28 22:29:33.978290 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:33.978753 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 22:29:33.984891 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 22:29:34.003098 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:34.003628 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 22:29:34.009693 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 22:29:34.012946 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:29:34.013426 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:34.013708 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:29:34.027656 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-28 22:29:34.034364 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:34.034834 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:29:34.072631 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 22:29:34.084011 (Thread-1): finished collecting timing info
2022-07-28 22:29:34.092931 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d326e90-8b69-4b3f-b3a4-c88996af35f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5cd90edad0>]}
2022-07-28 22:29:34.093830 (Thread-1): 22:29:34 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 5.23s]
2022-07-28 22:29:34.095800 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:29:34.153743 (MainThread): Using postgres connection "master".
2022-07-28 22:29:34.154422 (MainThread): On master: BEGIN
2022-07-28 22:29:34.155440 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:34.156103 (MainThread): On master: COMMIT
2022-07-28 22:29:34.156560 (MainThread): Using postgres connection "master".
2022-07-28 22:29:34.156968 (MainThread): On master: COMMIT
2022-07-28 22:29:34.157939 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:29:34.159283 (MainThread): 22:29:34 | 
2022-07-28 22:29:34.160337 (MainThread): 22:29:34 | Finished running 3 table models in 9.32s.
2022-07-28 22:29:34.164752 (MainThread): Connection 'master' was left open.
2022-07-28 22:29:34.165297 (MainThread): On master: Close
2022-07-28 22:29:34.166113 (MainThread): Connection 'list_warehouse' was left open.
2022-07-28 22:29:34.166628 (MainThread): On list_warehouse: Close
2022-07-28 22:29:34.169602 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 22:29:34.170126 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 22:29:34.245845 (MainThread): 
2022-07-28 22:29:34.246958 (MainThread): Completed successfully
2022-07-28 22:29:34.248262 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 22:29:34.251712 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce7197b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce71aae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce7282190>]}
2022-07-28 22:29:34.252613 (MainThread): Flushing usage events
2022-07-28 22:30:13.711947 (MainThread): Running with dbt=0.16.1
2022-07-28 22:30:14.301514 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 22:30:14.361969 (MainThread): Tracking: tracking
2022-07-28 22:30:14.541694 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b7967990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b7aa98d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b7b18ed0>]}
2022-07-28 22:30:14.683542 (MainThread): Partial parsing not enabled
2022-07-28 22:30:14.720720 (MainThread): Parsing macros/core.sql
2022-07-28 22:30:14.743429 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:30:14.789022 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:30:14.798541 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:30:14.951679 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:30:15.095097 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:30:15.147135 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:30:15.165237 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:30:15.313457 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:30:15.392748 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:30:15.418041 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:30:15.450271 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:30:15.493232 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:30:15.693974 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:30:15.699807 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:30:15.709878 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:30:15.721688 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:30:15.727757 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:30:15.736846 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:30:15.742548 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:30:15.786837 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:30:15.791971 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:30:15.807487 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:30:15.813525 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:30:15.849264 (MainThread): Parsing macros/relations.sql
2022-07-28 22:30:15.857391 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:30:15.939500 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:30:15.952038 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:30:16.068464 (MainThread): Partial parsing not enabled
2022-07-28 22:30:16.314917 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:30:16.315406 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:16.407781 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:30:16.408240 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:16.458397 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:30:16.458862 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:16.674625 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:16.675075 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:17.581068 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:30:19.127175 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:30:19.140838 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:30:19.142116 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:30:19.181047 (MainThread): 
2022-07-28 22:30:19.182438 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:30:19.183056 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:19.416529 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:30:19.419553 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 22:30:19.839997 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:30:19.840614 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:30:19.894248 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.05 seconds
2022-07-28 22:30:19.896506 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:30:19.897355 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:30:19.909611 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 22:30:19.981355 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:30:20.092949 (MainThread): Using postgres connection "master".
2022-07-28 22:30:20.093398 (MainThread): On master: BEGIN
2022-07-28 22:30:20.113670 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-28 22:30:20.114716 (MainThread): Using postgres connection "master".
2022-07-28 22:30:20.115553 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:30:20.143156 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-07-28 22:30:20.147328 (MainThread): On master: ROLLBACK
2022-07-28 22:30:20.150331 (MainThread): 22:30:20 | Concurrency: 1 threads (target='dev')
2022-07-28 22:30:20.151457 (MainThread): 22:30:20 | 
2022-07-28 22:30:20.194776 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:20.196444 (Thread-1): 22:30:20 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-28 22:30:20.197627 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:20.200908 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:30:20.203842 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:30:20.305612 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:30:20.332502 (Thread-1): finished collecting timing info
2022-07-28 22:30:20.333947 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:20.334477 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-28 22:30:20.336224 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:30:20.336882 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:20.337327 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-28 22:30:20.347679 (Thread-1): SQL status: SELECT 1 in 0.01 seconds
2022-07-28 22:30:20.348728 (Thread-1): finished collecting timing info
2022-07-28 22:30:20.349774 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-28 22:30:20.352687 (Thread-1): 22:30:20 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.16s]
2022-07-28 22:30:20.355235 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:20.401307 (MainThread): 22:30:20 | 
2022-07-28 22:30:20.407645 (MainThread): 22:30:20 | Finished running 1 test in 1.22s.
2022-07-28 22:30:20.408735 (MainThread): Connection 'master' was left open.
2022-07-28 22:30:20.409178 (MainThread): On master: Close
2022-07-28 22:30:20.416512 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-28 22:30:20.417179 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-28 22:30:20.434902 (MainThread): 
2022-07-28 22:30:20.436122 (MainThread): Completed successfully
2022-07-28 22:30:20.437270 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-28 22:30:20.438675 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b50370d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b5074bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b50b38d0>]}
2022-07-28 22:30:20.440204 (MainThread): Flushing usage events
2022-07-28 22:30:41.815562 (MainThread): Running with dbt=0.16.1
2022-07-28 22:30:42.166404 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 22:30:42.170680 (MainThread): Tracking: tracking
2022-07-28 22:30:42.212765 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b3635b390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b36350510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b36350d10>]}
2022-07-28 22:30:42.367325 (MainThread): Partial parsing not enabled
2022-07-28 22:30:42.380196 (MainThread): Parsing macros/core.sql
2022-07-28 22:30:42.414043 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:30:42.466587 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:30:42.475032 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:30:42.692294 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:30:42.785472 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:30:42.816282 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:30:42.833694 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:30:43.016680 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:30:43.149916 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:30:43.198862 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:30:43.334250 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:30:43.522583 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:30:44.037583 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:30:44.052217 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:30:44.074804 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:30:44.102036 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:30:44.107566 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:30:44.130478 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:30:44.137913 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:30:44.237889 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:30:44.247591 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:30:44.270671 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:30:44.278428 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:30:44.302350 (MainThread): Parsing macros/relations.sql
2022-07-28 22:30:44.317335 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:30:44.470608 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:30:44.489276 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:30:44.793843 (MainThread): Partial parsing not enabled
2022-07-28 22:30:45.420471 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:30:45.420932 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:45.788613 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:30:45.789054 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:45.919039 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:30:45.925129 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:46.497662 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:46.498130 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:47.824200 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:30:52.368183 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:30:52.407861 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:30:52.408939 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:30:52.432713 (MainThread): 
2022-07-28 22:30:52.434448 (MainThread): 22:30:52 | Concurrency: 1 threads (target='dev')
2022-07-28 22:30:52.436243 (MainThread): 22:30:52 | 
2022-07-28 22:30:52.534846 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:30:52.538147 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:30:52.538631 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:52.538958 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:30:53.066345 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:30:53.084747 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.085973 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.087703 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:30:53.089429 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:30:53.090478 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:30:53.090869 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:53.091171 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:30:53.249811 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:30:53.263791 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.264887 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.266597 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:30:53.271675 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:30:53.277180 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:30:53.277606 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:53.277911 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:30:53.495930 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:30:53.496976 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.501962 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.508821 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:30:53.509592 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:53.511856 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:53.512387 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:53.512756 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:30:53.624787 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:30:53.626443 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.627719 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.629600 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:53.728664 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:30:53.729133 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:30:53.880635 (MainThread): 22:30:53 | Done.
2022-07-28 22:30:54.321898 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 22:30:54.325291 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:54.325747 (MainThread): 22:30:54 | Building catalog
2022-07-28 22:30:54.644321 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 22:30:54.645219 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:30:55.613070 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:55.613541 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:30:55.667877 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.05 seconds
2022-07-28 22:30:55.669874 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:55.670380 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:30:55.846672 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.18 seconds
2022-07-28 22:30:56.040829 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:30:56.099482 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:56.106261 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:30:56.121597 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:30:56.122103 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:56.122500 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:30:56.129011 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.01 seconds
2022-07-28 22:30:56.153946 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:30:56.703097 (MainThread): 22:30:56 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 22:30:56.708733 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b33a57990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b29de62d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b363da850>]}
2022-07-28 22:30:56.709473 (MainThread): Flushing usage events
2022-07-28 22:30:58.076562 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 22:30:58.078229 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 22:30:58.078609 (MainThread): On warehouse.information_schema: Close
2022-07-28 22:31:06.158824 (MainThread): Running with dbt=0.16.1
2022-07-28 22:31:06.864648 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 22:31:06.865852 (MainThread): Tracking: tracking
2022-07-28 22:31:07.058942 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a02aeef10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a02bd6910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a02c82cd0>]}
2022-07-28 22:31:07.071948 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-28 22:31:07.073150 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-28 22:31:07.074140 (MainThread): Press Ctrl+C to exit.


2022-07-29 00:02:15.217180 (MainThread): Running with dbt=0.16.1
2022-07-29 00:02:15.596579 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-29 00:02:15.633028 (MainThread): Tracking: tracking
2022-07-29 00:02:15.688429 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f94bc650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f959b7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f95f7ed0>]}
2022-07-29 00:02:15.784425 (MainThread): Partial parsing not enabled
2022-07-29 00:02:15.849083 (MainThread): Parsing macros/core.sql
2022-07-29 00:02:15.875609 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-29 00:02:15.915623 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-29 00:02:15.926915 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-29 00:02:16.107872 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-29 00:02:16.221804 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-29 00:02:16.255159 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-29 00:02:16.268143 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-29 00:02:16.396162 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-29 00:02:16.460894 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-29 00:02:16.485534 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-29 00:02:16.516682 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-29 00:02:16.548798 (MainThread): Parsing macros/adapters/common.sql
2022-07-29 00:02:16.740867 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-29 00:02:16.746686 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-29 00:02:16.757392 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-29 00:02:16.768437 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-29 00:02:16.774260 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-29 00:02:16.782981 (MainThread): Parsing macros/etc/query.sql
2022-07-29 00:02:16.789423 (MainThread): Parsing macros/etc/datetime.sql
2022-07-29 00:02:16.833688 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-29 00:02:16.839224 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-29 00:02:16.849476 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-29 00:02:16.858771 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-29 00:02:16.899050 (MainThread): Parsing macros/relations.sql
2022-07-29 00:02:16.907785 (MainThread): Parsing macros/adapters.sql
2022-07-29 00:02:16.981953 (MainThread): Parsing macros/catalog.sql
2022-07-29 00:02:16.993516 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-29 00:02:17.107745 (MainThread): Partial parsing not enabled
2022-07-29 00:02:17.290837 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:17.291275 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:17.369431 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:17.369847 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:17.416706 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:17.417121 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:17.587786 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:17.588431 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:18.347155 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-29 00:02:19.360617 (MainThread): scipy not found, skipping conversion test.
2022-07-29 00:02:19.374856 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-29 00:02:19.376329 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-29 00:02:19.389075 (MainThread): 
2022-07-29 00:02:19.390465 (MainThread): Acquiring new postgres connection "master".
2022-07-29 00:02:19.391093 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:19.568925 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-29 00:02:19.569808 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-29 00:02:20.013167 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-29 00:02:20.013624 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-29 00:02:20.045922 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-07-29 00:02:20.186725 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:20.187631 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-29 00:02:20.193695 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:20.194354 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-29 00:02:20.196171 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:20.197052 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:20.197774 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-29 00:02:20.240937 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.04 seconds
2022-07-29 00:02:20.257414 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-29 00:02:20.343198 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.343921 (MainThread): On master: BEGIN
2022-07-29 00:02:20.361220 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-29 00:02:20.361890 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.362325 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-29 00:02:20.391963 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-07-29 00:02:20.395143 (MainThread): On master: ROLLBACK
2022-07-29 00:02:20.396511 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.397141 (MainThread): On master: BEGIN
2022-07-29 00:02:20.399106 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:20.399800 (MainThread): On master: COMMIT
2022-07-29 00:02:20.400337 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.400746 (MainThread): On master: COMMIT
2022-07-29 00:02:20.401559 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-29 00:02:20.402774 (MainThread): 00:02:20 | Concurrency: 1 threads (target='dev')
2022-07-29 00:02:20.403518 (MainThread): 00:02:20 | 
2022-07-29 00:02:20.807164 (Thread-1): Began running node model.dbt_.dim_types
2022-07-29 00:02:20.807887 (Thread-1): 00:02:20 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-29 00:02:20.808883 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:20.809249 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-29 00:02:20.809584 (Thread-1): Compiling model.dbt_.dim_types
2022-07-29 00:02:20.887718 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-29 00:02:21.003888 (Thread-1): finished collecting timing info
2022-07-29 00:02:21.087481 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.087999 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-29 00:02:21.090048 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.100560 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.101043 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-29 00:02:21.102411 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.168740 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-29 00:02:21.170069 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.170502 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-29 00:02:21.171632 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:21.172074 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.172357 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-29 00:02:21.615301 (Thread-1): SQL status: SELECT 6 in 0.44 seconds
2022-07-29 00:02:21.631921 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.632392 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-29 00:02:21.634945 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:21.644380 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.644881 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-29 00:02:21.647023 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:21.650722 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-29 00:02:21.651188 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.651652 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-29 00:02:21.676760 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-29 00:02:21.690614 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.691111 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-29 00:02:21.754759 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2022-07-29 00:02:21.772932 (Thread-1): finished collecting timing info
2022-07-29 00:02:21.775577 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b906902-6ec0-422b-aa49-c406f239f767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6c8ee10>]}
2022-07-29 00:02:21.776469 (Thread-1): 00:02:21 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.97s]
2022-07-29 00:02:21.776942 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-29 00:02:21.778461 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-29 00:02:21.779072 (Thread-1): 00:02:21 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-29 00:02:21.780725 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.781099 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-29 00:02:21.781392 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-29 00:02:21.814629 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-29 00:02:21.815853 (Thread-1): finished collecting timing info
2022-07-29 00:02:21.842729 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.843212 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-29 00:02:21.844647 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.854124 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.854595 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-29 00:02:21.855752 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.861648 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-29 00:02:21.863559 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.864007 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-29 00:02:21.865239 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:21.865752 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.866031 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-29 00:02:22.126854 (Thread-1): SQL status: SELECT 922 in 0.26 seconds
2022-07-29 00:02:22.154656 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.155141 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-29 00:02:22.158357 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:22.168882 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.169349 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-29 00:02:22.170958 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:22.175153 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-29 00:02:22.175632 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.175918 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-29 00:02:22.189039 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-29 00:02:22.203449 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.203914 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-29 00:02:22.211898 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-29 00:02:22.224610 (Thread-1): finished collecting timing info
2022-07-29 00:02:22.227777 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b906902-6ec0-422b-aa49-c406f239f767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6cc2b10>]}
2022-07-29 00:02:22.229432 (Thread-1): 00:02:22 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.45s]
2022-07-29 00:02:22.229963 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-29 00:02:22.230473 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-29 00:02:22.231206 (Thread-1): 00:02:22 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-29 00:02:22.232310 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.232696 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-29 00:02:22.232990 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-29 00:02:22.268815 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-29 00:02:22.269883 (Thread-1): finished collecting timing info
2022-07-29 00:02:22.302784 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.303330 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-29 00:02:22.304535 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:22.314554 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.315068 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-29 00:02:22.316446 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:22.322090 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-29 00:02:22.323325 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.323781 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-29 00:02:22.324689 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:22.325076 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.325347 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-29 00:02:29.744450 (Thread-1): SQL status: SELECT 922 in 7.42 seconds
2022-07-29 00:02:29.760624 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.761262 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-29 00:02:29.763152 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:29.773329 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.773921 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-29 00:02:29.775747 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:29.778890 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-29 00:02:29.780744 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.781245 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-29 00:02:29.826366 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-29 00:02:29.833619 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.834219 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-29 00:02:29.984752 (Thread-1): SQL status: DROP TABLE in 0.15 seconds
2022-07-29 00:02:29.994994 (Thread-1): finished collecting timing info
2022-07-29 00:02:29.997633 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b906902-6ec0-422b-aa49-c406f239f767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6d73c50>]}
2022-07-29 00:02:29.998638 (Thread-1): 00:02:29 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 7.77s]
2022-07-29 00:02:29.999245 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-29 00:02:30.093903 (MainThread): Using postgres connection "master".
2022-07-29 00:02:30.094364 (MainThread): On master: BEGIN
2022-07-29 00:02:30.095481 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:30.096097 (MainThread): On master: COMMIT
2022-07-29 00:02:30.096506 (MainThread): Using postgres connection "master".
2022-07-29 00:02:30.096981 (MainThread): On master: COMMIT
2022-07-29 00:02:30.098448 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-29 00:02:30.099865 (MainThread): 00:02:30 | 
2022-07-29 00:02:30.100551 (MainThread): 00:02:30 | Finished running 3 table models in 10.71s.
2022-07-29 00:02:30.101068 (MainThread): Connection 'master' was left open.
2022-07-29 00:02:30.101486 (MainThread): On master: Close
2022-07-29 00:02:30.102183 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-29 00:02:30.102737 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-29 00:02:30.169525 (MainThread): 
2022-07-29 00:02:30.170541 (MainThread): Completed successfully
2022-07-29 00:02:30.171698 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-29 00:02:30.172930 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6c394d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67e8c00550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6cac390>]}
2022-07-29 00:02:30.173634 (MainThread): Flushing usage events
2022-07-29 00:02:47.714334 (MainThread): Running with dbt=0.16.1
2022-07-29 00:02:48.018428 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-29 00:02:48.019453 (MainThread): Tracking: tracking
2022-07-29 00:02:48.038732 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efefbc9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7eff011810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efeeda510>]}
2022-07-29 00:02:48.142728 (MainThread): Partial parsing not enabled
2022-07-29 00:02:48.152193 (MainThread): Parsing macros/core.sql
2022-07-29 00:02:48.174620 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-29 00:02:48.210080 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-29 00:02:48.218058 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-29 00:02:48.362897 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-29 00:02:48.446195 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-29 00:02:48.476855 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-29 00:02:48.486663 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-29 00:02:48.584298 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-29 00:02:48.647059 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-29 00:02:48.669982 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-29 00:02:48.697834 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-29 00:02:48.727391 (MainThread): Parsing macros/adapters/common.sql
2022-07-29 00:02:48.912008 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-29 00:02:48.916905 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-29 00:02:48.931584 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-29 00:02:48.941665 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-29 00:02:48.946231 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-29 00:02:48.954252 (MainThread): Parsing macros/etc/query.sql
2022-07-29 00:02:48.959428 (MainThread): Parsing macros/etc/datetime.sql
2022-07-29 00:02:48.999277 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-29 00:02:49.003792 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-29 00:02:49.012998 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-29 00:02:49.018148 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-29 00:02:49.024499 (MainThread): Parsing macros/relations.sql
2022-07-29 00:02:49.031647 (MainThread): Parsing macros/adapters.sql
2022-07-29 00:02:49.204220 (MainThread): Parsing macros/catalog.sql
2022-07-29 00:02:49.222235 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-29 00:02:49.306769 (MainThread): Partial parsing not enabled
2022-07-29 00:02:49.440775 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:49.441244 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:49.512380 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:49.512833 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:49.542185 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:49.542641 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:49.678152 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:49.678612 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:50.332000 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-29 00:02:51.300266 (MainThread): scipy not found, skipping conversion test.
2022-07-29 00:02:51.313435 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-29 00:02:51.314815 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-29 00:02:51.327120 (MainThread): 
2022-07-29 00:02:51.329117 (MainThread): Acquiring new postgres connection "master".
2022-07-29 00:02:51.329759 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:51.511024 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:51.511942 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-29 00:02:51.896599 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:51.897246 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-29 00:02:51.910078 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-07-29 00:02:51.911077 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:51.911667 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-29 00:02:51.923979 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-29 00:02:51.946676 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-29 00:02:52.024066 (MainThread): Using postgres connection "master".
2022-07-29 00:02:52.024728 (MainThread): On master: BEGIN
2022-07-29 00:02:52.037177 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-29 00:02:52.037681 (MainThread): Using postgres connection "master".
2022-07-29 00:02:52.037985 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-29 00:02:52.063140 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-29 00:02:52.066637 (MainThread): On master: ROLLBACK
2022-07-29 00:02:52.068620 (MainThread): 00:02:52 | Concurrency: 1 threads (target='dev')
2022-07-29 00:02:52.069140 (MainThread): 00:02:52 | 
2022-07-29 00:02:52.084097 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-29 00:02:52.084896 (Thread-1): 00:02:52 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-29 00:02:52.086247 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:52.086791 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-29 00:02:52.087240 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-29 00:02:52.191880 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-29 00:02:52.225314 (Thread-1): finished collecting timing info
2022-07-29 00:02:52.226715 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:52.227234 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-29 00:02:52.229625 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:52.230285 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:52.230722 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-29 00:02:52.234623 (Thread-1): SQL status: SELECT 1 in 0.00 seconds
2022-07-29 00:02:52.240313 (Thread-1): finished collecting timing info
2022-07-29 00:02:52.241340 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-29 00:02:52.243070 (Thread-1): 00:02:52 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.16s]
2022-07-29 00:02:52.243858 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-29 00:02:52.292598 (MainThread): 00:02:52 | 
2022-07-29 00:02:52.293946 (MainThread): 00:02:52 | Finished running 1 test in 0.96s.
2022-07-29 00:02:52.295068 (MainThread): Connection 'master' was left open.
2022-07-29 00:02:52.303598 (MainThread): On master: Close
2022-07-29 00:02:52.310398 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-29 00:02:52.311076 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-29 00:02:52.325858 (MainThread): 
2022-07-29 00:02:52.327063 (MainThread): Completed successfully
2022-07-29 00:02:52.328185 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-29 00:02:52.329797 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efc5c7550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efc593110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7eee61f790>]}
2022-07-29 00:02:52.330743 (MainThread): Flushing usage events
2022-07-29 00:03:07.689328 (MainThread): Running with dbt=0.16.1
2022-07-29 00:03:07.971741 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-29 00:03:07.972737 (MainThread): Tracking: tracking
2022-07-29 00:03:07.989233 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c10223650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c10367e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c10317fd0>]}
2022-07-29 00:03:08.085775 (MainThread): Partial parsing not enabled
2022-07-29 00:03:08.092118 (MainThread): Parsing macros/core.sql
2022-07-29 00:03:08.115637 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-29 00:03:08.201102 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-29 00:03:08.220336 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-29 00:03:08.374855 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-29 00:03:08.467590 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-29 00:03:08.496882 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-29 00:03:08.506221 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-29 00:03:08.602320 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-29 00:03:08.659753 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-29 00:03:08.684423 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-29 00:03:08.713085 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-29 00:03:08.744359 (MainThread): Parsing macros/adapters/common.sql
2022-07-29 00:03:08.929853 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-29 00:03:08.935184 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-29 00:03:08.944753 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-29 00:03:08.955302 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-29 00:03:08.960016 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-29 00:03:08.968220 (MainThread): Parsing macros/etc/query.sql
2022-07-29 00:03:08.973525 (MainThread): Parsing macros/etc/datetime.sql
2022-07-29 00:03:09.015521 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-29 00:03:09.019861 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-29 00:03:09.029490 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-29 00:03:09.035018 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-29 00:03:09.041458 (MainThread): Parsing macros/relations.sql
2022-07-29 00:03:09.048388 (MainThread): Parsing macros/adapters.sql
2022-07-29 00:03:09.127099 (MainThread): Parsing macros/catalog.sql
2022-07-29 00:03:09.152833 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-29 00:03:09.249011 (MainThread): Partial parsing not enabled
2022-07-29 00:03:09.388050 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:03:09.388692 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:09.463266 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:03:09.464041 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:09.495851 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:03:09.496489 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:09.636140 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:03:09.636805 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:10.376042 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-29 00:03:11.307253 (MainThread): scipy not found, skipping conversion test.
2022-07-29 00:03:11.319718 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-29 00:03:11.320860 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-29 00:03:11.332850 (MainThread): 
2022-07-29 00:03:11.333521 (MainThread): 00:03:11 | Concurrency: 1 threads (target='dev')
2022-07-29 00:03:11.333919 (MainThread): 00:03:11 | 
2022-07-29 00:03:11.354895 (Thread-1): Began running node model.dbt_.dim_types
2022-07-29 00:03:11.356242 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:03:11.356682 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.357007 (Thread-1): Compiling model.dbt_.dim_types
2022-07-29 00:03:11.541270 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-29 00:03:11.542605 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.544072 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.546151 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-29 00:03:11.547663 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-29 00:03:11.548943 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:03:11.549541 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.549983 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-29 00:03:11.584158 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-29 00:03:11.585416 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.586735 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.588573 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-29 00:03:11.589345 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-29 00:03:11.590835 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:03:11.591688 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.592154 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-29 00:03:11.626086 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-29 00:03:11.627323 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.628881 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.630634 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-29 00:03:11.631467 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-29 00:03:11.632911 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:03:11.633497 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.633931 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-29 00:03:11.697301 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-29 00:03:11.698521 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.700107 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.702209 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-29 00:03:11.792264 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-29 00:03:11.793892 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-29 00:03:11.841410 (MainThread): 00:03:11 | Done.
2022-07-29 00:03:11.889683 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-29 00:03:11.890305 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:11.890735 (MainThread): 00:03:11 | Building catalog
2022-07-29 00:03:11.998921 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-29 00:03:12.000153 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-29 00:03:12.394014 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:12.394934 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-29 00:03:12.418947 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.02 seconds
2022-07-29 00:03:12.420015 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:12.421544 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-29 00:03:14.253141 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 1.83 seconds
2022-07-29 00:03:14.353443 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-29 00:03:14.369601 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:14.370245 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-29 00:03:14.371793 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:03:14.372792 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:14.373334 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-29 00:03:14.378586 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-29 00:03:14.384037 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-29 00:03:14.613159 (MainThread): 00:03:14 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-29 00:03:14.614168 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c0d925b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c0d9250d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c0d925950>]}
2022-07-29 00:03:14.615062 (MainThread): Flushing usage events
2022-07-29 00:03:15.604826 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-29 00:03:15.606829 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-29 00:03:15.608360 (MainThread): On warehouse.information_schema: Close
2022-07-29 00:03:20.088114 (MainThread): Running with dbt=0.16.1
2022-07-29 00:03:20.410738 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-29 00:03:20.411759 (MainThread): Tracking: tracking
2022-07-29 00:03:20.574106 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f69ebe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6ab5710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b1c350>]}
2022-07-29 00:03:20.584059 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-29 00:03:20.584639 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-29 00:03:20.584943 (MainThread): Press Ctrl+C to exit.


2022-07-29 00:03:20.587099 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b42e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b4c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b4c290>]}
2022-07-29 00:03:20.587937 (MainThread): Flushing usage events
2022-07-29 00:03:21.481458 (MainThread): Encountered an error:
2022-07-29 00:03:21.482956 (MainThread): [Errno 98] Address already in use
2022-07-29 00:03:21.534013 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/serve.py", line 31, in run
    SimpleHTTPRequestHandler  # type: ignore
  File "/usr/local/lib/python3.7/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/usr/local/lib/python3.7/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use

2022-07-30 19:16:20.330448 (MainThread): Running with dbt=0.16.1
2022-07-30 19:16:20.850472 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:16:20.895376 (MainThread): Tracking: tracking
2022-07-30 19:16:21.282043 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e0c05d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e219f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e0a50d0>]}
2022-07-30 19:16:21.554101 (MainThread): Partial parsing not enabled
2022-07-30 19:16:21.602407 (MainThread): Parsing macros/core.sql
2022-07-30 19:16:21.690980 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:16:21.799219 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:16:21.824958 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:16:22.027773 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:16:22.263548 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:16:22.381310 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:16:22.412253 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:16:22.644683 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:16:22.769141 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:16:22.807279 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:16:22.853745 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:16:22.903453 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:16:23.172744 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:16:23.190626 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:16:23.202616 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:16:23.220572 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:16:23.254246 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:16:23.285195 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:16:23.297823 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:16:23.385330 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:16:23.412531 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:16:23.446558 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:16:23.473312 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:16:23.575319 (MainThread): Parsing macros/relations.sql
2022-07-30 19:16:23.606457 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:16:23.899826 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:16:23.936929 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:16:24.373310 (MainThread): Partial parsing not enabled
2022-07-30 19:16:24.967746 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:24.970774 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:25.133836 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:16:25.134720 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:25.223207 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:16:25.223831 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:25.415356 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:16:25.416014 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:26.757440 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:16:28.245453 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:16:28.307626 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:16:28.308837 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:16:28.321584 (MainThread): 
2022-07-30 19:16:28.322905 (MainThread): Acquiring new postgres connection "master".
2022-07-30 19:16:28.323423 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:28.568379 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-30 19:16:28.569073 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-30 19:16:29.116342 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-30 19:16:29.116792 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-30 19:16:29.242806 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.13 seconds
2022-07-30 19:16:29.273767 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.274203 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-30 19:16:29.274460 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-07-30 19:16:29.283878 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.285161 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-07-30 19:16:29.287171 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:29.289834 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.290312 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-07-30 19:16:29.319269 (ThreadPoolExecutor-0_0): SQL status: CREATE SCHEMA in 0.03 seconds
2022-07-30 19:16:29.322186 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-30 19:16:29.322622 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.322904 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-30 19:16:29.341669 (ThreadPoolExecutor-0_0): SQL status: COMMIT in 0.02 seconds
2022-07-30 19:16:29.417393 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-30 19:16:29.418200 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly create_warehouse_warehouse).
2022-07-30 19:16:29.436262 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:16:29.436951 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-30 19:16:29.438297 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:29.438916 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:16:29.439426 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-30 19:16:29.555972 (ThreadPoolExecutor-1_0): SQL status: SELECT 0 in 0.12 seconds
2022-07-30 19:16:29.559868 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-30 19:16:29.709717 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.710185 (MainThread): On master: BEGIN
2022-07-30 19:16:29.724655 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:16:29.729004 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.735563 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-30 19:16:29.818782 (MainThread): SQL status: SELECT 0 in 0.08 seconds
2022-07-30 19:16:29.822968 (MainThread): On master: ROLLBACK
2022-07-30 19:16:29.824380 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.824984 (MainThread): On master: BEGIN
2022-07-30 19:16:29.826823 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:29.829815 (MainThread): On master: COMMIT
2022-07-30 19:16:29.838481 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.838932 (MainThread): On master: COMMIT
2022-07-30 19:16:29.839920 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:16:29.840997 (MainThread): 19:16:29 | Concurrency: 1 threads (target='dev')
2022-07-30 19:16:29.841499 (MainThread): 19:16:29 | 
2022-07-30 19:16:29.876219 (Thread-1): Began running node model.dbt_.dim_types
2022-07-30 19:16:29.877081 (Thread-1): 19:16:29 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-30 19:16:29.878329 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:29.878891 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-30 19:16:29.879453 (Thread-1): Compiling model.dbt_.dim_types
2022-07-30 19:16:29.962892 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-30 19:16:29.964441 (Thread-1): finished collecting timing info
2022-07-30 19:16:30.046734 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.047474 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-30 19:16:30.048912 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:16:30.057726 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.058391 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-30 19:16:30.059591 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:16:30.161477 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-30 19:16:30.163684 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.164282 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-30 19:16:30.166216 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:30.171566 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.172969 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-30 19:16:30.192097 (Thread-1): Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from warehouse.warehouse.source
                          ^

2022-07-30 19:16:30.192577 (Thread-1): On model.dbt_.dim_types: ROLLBACK
2022-07-30 19:16:30.195415 (Thread-1): finished collecting timing info
2022-07-30 19:16:30.198730 (Thread-1): Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from warehouse.warehouse.source
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "warehouse.source" does not exist
LINE 8:     select * from warehouse.warehouse.source
                          ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from warehouse.warehouse.source
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-30 19:16:30.344638 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9668870b-2c0e-47d8-a3d3-a2ad496e8b91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5b7c07d0>]}
2022-07-30 19:16:30.345449 (Thread-1): 19:16:30 | 1 of 3 ERROR creating table model warehouse.dim_types................ [ERROR in 0.47s]
2022-07-30 19:16:30.345931 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-30 19:16:30.352552 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-30 19:16:30.353131 (Thread-1): 19:16:30 | 2 of 3 SKIP relation warehouse.fct_summary........................... [SKIP]
2022-07-30 19:16:30.353561 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-30 19:16:30.359474 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-30 19:16:30.360374 (Thread-1): 19:16:30 | 3 of 3 SKIP relation warehouse.fct_trajectory........................ [SKIP]
2022-07-30 19:16:30.360836 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-30 19:16:30.400347 (MainThread): Using postgres connection "master".
2022-07-30 19:16:30.400827 (MainThread): On master: BEGIN
2022-07-30 19:16:30.401632 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:30.402080 (MainThread): On master: COMMIT
2022-07-30 19:16:30.402356 (MainThread): Using postgres connection "master".
2022-07-30 19:16:30.402610 (MainThread): On master: COMMIT
2022-07-30 19:16:30.404928 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:16:30.406349 (MainThread): 19:16:30 | 
2022-07-30 19:16:30.407049 (MainThread): 19:16:30 | Finished running 3 table models in 2.08s.
2022-07-30 19:16:30.409257 (MainThread): Connection 'master' was left open.
2022-07-30 19:16:30.409864 (MainThread): On master: Close
2022-07-30 19:16:30.415245 (MainThread): Connection 'model.dbt_.dim_types' was left open.
2022-07-30 19:16:30.415928 (MainThread): On model.dbt_.dim_types: Close
2022-07-30 19:16:30.462283 (MainThread): 
2022-07-30 19:16:30.463005 (MainThread): Completed with 1 error and 0 warnings:
2022-07-30 19:16:30.463647 (MainThread): 
2022-07-30 19:16:30.464197 (MainThread): Database Error in model dim_types (models/traffic_models/dim_types.sql)
2022-07-30 19:16:30.464692 (MainThread):   relation "warehouse.source" does not exist
2022-07-30 19:16:30.465149 (MainThread):   LINE 8:     select * from warehouse.warehouse.source
2022-07-30 19:16:30.465699 (MainThread):                             ^
2022-07-30 19:16:30.466161 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-30 19:16:30.466642 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2022-07-30 19:16:30.467485 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e156790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5b8f2e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5b7d6350>]}
2022-07-30 19:16:30.468348 (MainThread): Flushing usage events
2022-07-30 19:23:12.511996 (MainThread): Running with dbt=0.16.1
2022-07-30 19:23:12.881312 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:23:12.901396 (MainThread): Tracking: tracking
2022-07-30 19:23:12.953446 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13e06390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13de0450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13de0b90>]}
2022-07-30 19:23:13.198791 (MainThread): Partial parsing not enabled
2022-07-30 19:23:13.299391 (MainThread): Parsing macros/core.sql
2022-07-30 19:23:13.426075 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:23:13.611772 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:23:13.647544 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:23:13.835638 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:23:14.109187 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:23:14.156738 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:23:14.170567 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:23:14.301924 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:23:14.460200 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:23:14.564122 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:23:14.677573 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:23:14.771342 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:23:15.107587 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:23:15.136282 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:23:15.159959 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:23:15.188210 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:23:15.210748 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:23:15.241851 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:23:15.254867 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:23:15.353310 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:23:15.367628 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:23:15.403518 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:23:15.429979 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:23:15.465382 (MainThread): Parsing macros/relations.sql
2022-07-30 19:23:15.496737 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:23:15.712420 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:23:15.755577 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:23:15.973670 (MainThread): Partial parsing not enabled
2022-07-30 19:23:16.241710 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:23:16.242150 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:16.336472 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:23:16.337005 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:16.386983 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:23:16.387668 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:16.553755 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:23:16.554531 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:17.133851 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13f23810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f17be6b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f115efcd0>]}
2022-07-30 19:23:17.134667 (MainThread): Flushing usage events
2022-07-30 19:23:18.508755 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:23:18.510326 (MainThread): Encountered an error:
2022-07-30 19:23:18.511752 (MainThread): Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found
2022-07-30 19:23:18.608951 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 301, in process_manifest
    process_sources(manifest, project_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 587, in process_sources
    _process_sources_for_node(manifest, current_project, node)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 575, in _process_sources_for_node
    table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/utils.py", line 348, in invalid_source_fail_unless_test
    target_table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 502, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found

2022-07-30 19:26:17.028777 (MainThread): Running with dbt=0.16.1
2022-07-30 19:26:17.313315 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:26:17.332345 (MainThread): Tracking: tracking
2022-07-30 19:26:17.353484 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d10bf890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d10caf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d117d1d0>]}
2022-07-30 19:26:17.439627 (MainThread): Partial parsing not enabled
2022-07-30 19:26:17.445715 (MainThread): Parsing macros/core.sql
2022-07-30 19:26:17.466356 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:26:17.514202 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:26:17.523844 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:26:17.666178 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:26:17.749816 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:26:17.779003 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:26:17.788526 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:26:17.891293 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:26:17.951377 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:26:17.974261 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:26:18.002823 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:26:18.032599 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:26:18.230685 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:26:18.235951 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:26:18.246260 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:26:18.263448 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:26:18.268611 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:26:18.279524 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:26:18.302230 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:26:18.362247 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:26:18.366797 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:26:18.377101 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:26:18.383617 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:26:18.389922 (MainThread): Parsing macros/relations.sql
2022-07-30 19:26:18.397097 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:26:18.484732 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:26:18.498400 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:26:18.584193 (MainThread): Partial parsing not enabled
2022-07-30 19:26:18.735195 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:26:18.735663 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:18.810244 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:26:18.810705 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:18.840214 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:26:18.840646 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:18.975502 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:26:18.975941 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:19.362100 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3ce8c6590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d1bc9b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3ce8c1190>]}
2022-07-30 19:26:19.362915 (MainThread): Flushing usage events
2022-07-30 19:26:20.426206 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:26:20.428512 (MainThread): Encountered an error:
2022-07-30 19:26:20.429149 (MainThread): Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found
2022-07-30 19:26:20.509955 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 301, in process_manifest
    process_sources(manifest, project_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 587, in process_sources
    _process_sources_for_node(manifest, current_project, node)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 575, in _process_sources_for_node
    table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/utils.py", line 348, in invalid_source_fail_unless_test
    target_table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 502, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found

2022-07-30 19:29:47.695588 (MainThread): Running with dbt=0.16.1
2022-07-30 19:29:47.988063 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:29:47.989367 (MainThread): Tracking: tracking
2022-07-30 19:29:48.017122 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf430a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf43edc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf43ed2d0>]}
2022-07-30 19:29:48.106989 (MainThread): Partial parsing not enabled
2022-07-30 19:29:48.113170 (MainThread): Parsing macros/core.sql
2022-07-30 19:29:48.134372 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:29:48.169395 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:29:48.177534 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:29:48.316474 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:29:48.395023 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:29:48.423645 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:29:48.432432 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:29:48.525286 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:29:48.594292 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:29:48.635669 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:29:48.664883 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:29:48.696130 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:29:48.912933 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:29:48.917947 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:29:48.927430 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:29:48.937600 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:29:48.942187 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:29:48.950566 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:29:48.955742 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:29:48.996349 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:29:49.001326 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:29:49.010764 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:29:49.019354 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:29:49.025640 (MainThread): Parsing macros/relations.sql
2022-07-30 19:29:49.032523 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:29:49.103360 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:29:49.114780 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:29:49.253171 (MainThread): Partial parsing not enabled
2022-07-30 19:29:49.401278 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:49.401952 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:49.479731 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:49.480351 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:49.509959 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:49.510589 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:49.675384 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:29:49.676021 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:50.358284 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:29:51.355933 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:29:51.368297 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:29:51.369377 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:29:51.381018 (MainThread): 
2022-07-30 19:29:51.382631 (MainThread): Acquiring new postgres connection "master".
2022-07-30 19:29:51.383069 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:51.567619 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-30 19:29:51.569057 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-30 19:29:51.962810 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-30 19:29:51.963492 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-30 19:29:51.983996 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-30 19:29:52.069706 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-30 19:29:52.071002 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-30 19:29:52.082533 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:29:52.083426 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-30 19:29:52.085743 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:52.088256 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:29:52.091564 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-30 19:29:52.100447 (ThreadPoolExecutor-1_0): SQL status: SELECT 1 in 0.01 seconds
2022-07-30 19:29:52.106749 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-30 19:29:52.212879 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.213605 (MainThread): On master: BEGIN
2022-07-30 19:29:52.232610 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-30 19:29:52.235882 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.236239 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-30 19:29:52.251365 (MainThread): SQL status: SELECT 0 in 0.01 seconds
2022-07-30 19:29:52.256365 (MainThread): On master: ROLLBACK
2022-07-30 19:29:52.258471 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.259065 (MainThread): On master: BEGIN
2022-07-30 19:29:52.263143 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:52.263677 (MainThread): On master: COMMIT
2022-07-30 19:29:52.263979 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.264247 (MainThread): On master: COMMIT
2022-07-30 19:29:52.266441 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:29:52.272404 (MainThread): 19:29:52 | Concurrency: 1 threads (target='dev')
2022-07-30 19:29:52.273495 (MainThread): 19:29:52 | 
2022-07-30 19:29:52.310478 (Thread-1): Began running node model.dbt_.dim_types
2022-07-30 19:29:52.311791 (Thread-1): 19:29:52 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-30 19:29:52.313566 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.314176 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-30 19:29:52.315828 (Thread-1): Compiling model.dbt_.dim_types
2022-07-30 19:29:52.411540 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-30 19:29:52.413297 (Thread-1): finished collecting timing info
2022-07-30 19:29:52.553791 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.554279 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-30 19:29:52.564149 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-30 19:29:52.577337 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.577783 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-30 19:29:52.578856 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:52.771788 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-30 19:29:52.778025 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.779823 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-30 19:29:52.789218 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:29:52.805015 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.807970 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-30 19:29:53.108911 (Thread-1): SQL status: SELECT 6 in 0.30 seconds
2022-07-30 19:29:53.129984 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:53.134410 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-30 19:29:53.139001 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-30 19:29:53.144151 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-30 19:29:53.144859 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:53.145324 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-30 19:29:53.248517 (Thread-1): SQL status: COMMIT in 0.10 seconds
2022-07-30 19:29:53.261147 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:53.262852 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-30 19:29:53.264518 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.276689 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.279320 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd056c54-79b3-4d37-8afe-317904b704d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf19a6f90>]}
2022-07-30 19:29:53.280315 (Thread-1): 19:29:53 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.97s]
2022-07-30 19:29:53.280830 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-30 19:29:53.287539 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-30 19:29:53.288165 (Thread-1): 19:29:53 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-30 19:29:53.289131 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.289499 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-30 19:29:53.289792 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-30 19:29:53.424810 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-30 19:29:53.426296 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.469752 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.470380 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-30 19:29:53.473425 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.495233 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.498810 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-30 19:29:53.502807 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.522330 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-30 19:29:53.525875 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.526301 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-30 19:29:53.528200 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:53.528673 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.528943 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-30 19:29:53.635636 (Thread-1): SQL status: SELECT 922 in 0.10 seconds
2022-07-30 19:29:53.651580 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.653460 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-30 19:29:53.656504 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-30 19:29:53.665002 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-30 19:29:53.667012 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.669504 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-30 19:29:53.681486 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-30 19:29:53.690145 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.693073 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-30 19:29:53.694359 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.724277 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.732692 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd056c54-79b3-4d37-8afe-317904b704d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf1b48f90>]}
2022-07-30 19:29:53.738058 (Thread-1): 19:29:53 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.44s]
2022-07-30 19:29:53.740905 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-30 19:29:53.743661 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-30 19:29:53.744811 (Thread-1): 19:29:53 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-30 19:29:53.751882 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.752979 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-30 19:29:53.754612 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-30 19:29:53.818810 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-30 19:29:53.820408 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.862161 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.864311 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-30 19:29:53.865870 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.875574 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.876212 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-30 19:29:53.877445 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.890375 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-30 19:29:53.892497 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.893000 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-30 19:29:53.893861 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:53.894287 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.894547 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-30 19:29:55.905605 (Thread-1): SQL status: SELECT 922 in 2.01 seconds
2022-07-30 19:29:55.976091 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:55.976541 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-30 19:29:55.981311 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-30 19:29:55.985999 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-30 19:29:55.986455 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:55.986729 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-30 19:29:56.676070 (Thread-1): SQL status: COMMIT in 0.69 seconds
2022-07-30 19:29:56.682704 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:56.686862 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-30 19:29:56.688252 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:56.710695 (Thread-1): finished collecting timing info
2022-07-30 19:29:56.713483 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd056c54-79b3-4d37-8afe-317904b704d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf1b57fd0>]}
2022-07-30 19:29:56.714533 (Thread-1): 19:29:56 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 2.97s]
2022-07-30 19:29:56.715256 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-30 19:29:56.785049 (MainThread): Using postgres connection "master".
2022-07-30 19:29:56.785484 (MainThread): On master: BEGIN
2022-07-30 19:29:56.786563 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:56.787051 (MainThread): On master: COMMIT
2022-07-30 19:29:56.787432 (MainThread): Using postgres connection "master".
2022-07-30 19:29:56.787696 (MainThread): On master: COMMIT
2022-07-30 19:29:56.788924 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:29:56.790521 (MainThread): 19:29:56 | 
2022-07-30 19:29:56.791566 (MainThread): 19:29:56 | Finished running 3 table models in 5.41s.
2022-07-30 19:29:56.792359 (MainThread): Connection 'master' was left open.
2022-07-30 19:29:56.792709 (MainThread): On master: Close
2022-07-30 19:29:56.793231 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-30 19:29:56.793705 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-30 19:29:56.834697 (MainThread): 
2022-07-30 19:29:56.835737 (MainThread): Completed successfully
2022-07-30 19:29:56.836600 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-30 19:29:56.837726 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bec01a090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0be3a2d550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf1b95850>]}
2022-07-30 19:29:56.838466 (MainThread): Flushing usage events
2022-07-30 19:30:15.353058 (MainThread): Running with dbt=0.16.1
2022-07-30 19:30:15.769765 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-30 19:30:15.770716 (MainThread): Tracking: tracking
2022-07-30 19:30:15.788064 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9232e9490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa925eccd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa923137150>]}
2022-07-30 19:30:15.872789 (MainThread): Partial parsing not enabled
2022-07-30 19:30:15.878773 (MainThread): Parsing macros/core.sql
2022-07-30 19:30:15.900171 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:30:15.934843 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:30:15.942981 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:30:16.148426 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:30:16.250913 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:30:16.280293 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:30:16.289089 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:30:16.386332 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:30:16.448171 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:30:16.484143 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:30:16.554453 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:30:16.637112 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:30:16.888599 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:30:16.893532 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:30:16.903641 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:30:16.913894 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:30:16.918318 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:30:16.926217 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:30:16.931764 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:30:16.993137 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:30:16.998008 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:30:17.009386 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:30:17.014780 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:30:17.021185 (MainThread): Parsing macros/relations.sql
2022-07-30 19:30:17.028559 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:30:17.188942 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:30:17.203576 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:30:17.410227 (MainThread): Partial parsing not enabled
2022-07-30 19:30:17.672635 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:30:17.673074 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:17.749197 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:30:17.749651 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:17.778448 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:30:17.778887 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:17.926358 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:17.926884 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:18.701079 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:30:19.823971 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:30:19.835843 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:30:19.837185 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:30:19.848961 (MainThread): 
2022-07-30 19:30:19.850779 (MainThread): Acquiring new postgres connection "master".
2022-07-30 19:30:19.851410 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:20.030384 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-30 19:30:20.031280 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-30 19:30:20.858466 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:30:20.863345 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-30 19:30:20.916101 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.05 seconds
2022-07-30 19:30:20.916608 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:30:20.916936 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-30 19:30:20.943462 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.03 seconds
2022-07-30 19:30:21.006696 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-30 19:30:21.103575 (MainThread): Using postgres connection "master".
2022-07-30 19:30:21.104049 (MainThread): On master: BEGIN
2022-07-30 19:30:21.117642 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:30:21.118155 (MainThread): Using postgres connection "master".
2022-07-30 19:30:21.118971 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-30 19:30:21.170813 (MainThread): SQL status: SELECT 0 in 0.04 seconds
2022-07-30 19:30:21.173901 (MainThread): On master: ROLLBACK
2022-07-30 19:30:21.184472 (MainThread): 19:30:21 | Concurrency: 1 threads (target='dev')
2022-07-30 19:30:21.185792 (MainThread): 19:30:21 | 
2022-07-30 19:30:21.224175 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:21.224829 (Thread-1): 19:30:21 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-30 19:30:21.225828 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:21.226782 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-30 19:30:21.227379 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-30 19:30:21.383676 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-30 19:30:21.405603 (Thread-1): finished collecting timing info
2022-07-30 19:30:21.406799 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:21.407220 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-30 19:30:21.408834 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:30:21.411070 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:21.411571 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-30 19:30:21.513291 (Thread-1): SQL status: SELECT 1 in 0.10 seconds
2022-07-30 19:30:21.514339 (Thread-1): finished collecting timing info
2022-07-30 19:30:21.516068 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-30 19:30:21.518498 (Thread-1): 19:30:21 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.29s]
2022-07-30 19:30:21.523221 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:21.533031 (MainThread): 19:30:21 | 
2022-07-30 19:30:21.534018 (MainThread): 19:30:21 | Finished running 1 test in 1.68s.
2022-07-30 19:30:21.540300 (MainThread): Connection 'master' was left open.
2022-07-30 19:30:21.540691 (MainThread): On master: Close
2022-07-30 19:30:21.551748 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-30 19:30:21.552232 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-30 19:30:21.586559 (MainThread): 
2022-07-30 19:30:21.587088 (MainThread): Completed successfully
2022-07-30 19:30:21.587569 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-30 19:30:21.588147 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9208421d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa920842550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9208a93d0>]}
2022-07-30 19:30:21.588863 (MainThread): Flushing usage events
2022-07-30 19:30:35.590307 (MainThread): Running with dbt=0.16.1
2022-07-30 19:30:35.858644 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-30 19:30:35.859646 (MainThread): Tracking: tracking
2022-07-30 19:30:35.886044 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6a62bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6b177d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6a520d0>]}
2022-07-30 19:30:35.976539 (MainThread): Partial parsing not enabled
2022-07-30 19:30:35.982873 (MainThread): Parsing macros/core.sql
2022-07-30 19:30:36.004864 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:30:36.039704 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:30:36.047854 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:30:36.191324 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:30:36.270875 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:30:36.299067 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:30:36.307906 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:30:36.400684 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:30:36.456402 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:30:36.479251 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:30:36.506844 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:30:36.536888 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:30:36.886158 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:30:36.891090 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:30:36.900710 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:30:36.916512 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:30:36.920986 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:30:36.928908 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:30:36.935011 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:30:36.975510 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:30:36.979957 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:30:36.989147 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:30:36.994361 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:30:37.000615 (MainThread): Parsing macros/relations.sql
2022-07-30 19:30:37.007427 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:30:37.082890 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:30:37.094117 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:30:37.180138 (MainThread): Partial parsing not enabled
2022-07-30 19:30:37.314882 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:30:37.315433 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:37.389126 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:30:37.389596 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:37.418742 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:30:37.419279 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:37.554503 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:37.554960 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:38.351874 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:30:39.452185 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:30:39.466203 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:30:39.467581 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:30:39.480122 (MainThread): 
2022-07-30 19:30:39.481443 (MainThread): 19:30:39 | Concurrency: 1 threads (target='dev')
2022-07-30 19:30:39.482515 (MainThread): 19:30:39 | 
2022-07-30 19:30:39.505613 (Thread-1): Began running node model.dbt_.dim_types
2022-07-30 19:30:39.506858 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:30:39.507408 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.507747 (Thread-1): Compiling model.dbt_.dim_types
2022-07-30 19:30:39.703760 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-30 19:30:39.704835 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.705962 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.707564 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-30 19:30:39.710309 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-30 19:30:39.711691 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:30:39.712307 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.712778 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-30 19:30:39.747311 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-30 19:30:39.748872 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.750105 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.751831 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-30 19:30:39.752577 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-30 19:30:39.753884 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:30:39.754445 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.754888 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-30 19:30:39.787927 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-30 19:30:39.789343 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.790582 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.792371 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-30 19:30:39.793145 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:39.794506 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:39.795086 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.795582 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-30 19:30:39.853904 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-30 19:30:39.855393 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.856661 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.858461 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:39.955784 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:30:39.957263 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:30:40.006480 (MainThread): 19:30:40 | Done.
2022-07-30 19:30:40.068222 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-30 19:30:40.068823 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:40.069229 (MainThread): 19:30:40 | Building catalog
2022-07-30 19:30:40.174512 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-30 19:30:40.176237 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-30 19:30:40.501583 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:40.502222 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-30 19:30:40.516177 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:30:40.518486 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:40.527329 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-30 19:30:40.952512 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.42 seconds
2022-07-30 19:30:40.994277 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-30 19:30:41.010241 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:41.010876 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-30 19:30:41.013479 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:30:41.014195 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:41.014827 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-30 19:30:41.024201 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.01 seconds
2022-07-30 19:30:41.036002 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-30 19:30:41.547880 (MainThread): 19:30:41 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-30 19:30:41.549360 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6baa790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6baaa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6baa1d0>]}
2022-07-30 19:30:41.550070 (MainThread): Flushing usage events
2022-07-30 19:30:42.415008 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-30 19:30:42.420675 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-30 19:30:42.427508 (MainThread): On warehouse.information_schema: Close
2022-07-30 19:30:46.976326 (MainThread): Running with dbt=0.16.1
2022-07-30 19:30:47.251606 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-30 19:30:47.252508 (MainThread): Tracking: tracking
2022-07-30 19:30:47.285024 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88682b71d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8868377d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f886837ea10>]}
2022-07-30 19:30:47.296604 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-30 19:30:47.297168 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-30 19:30:47.297474 (MainThread): Press Ctrl+C to exit.


2022-08-04 00:03:01.944183 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:02.328208 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-04 00:03:02.382009 (MainThread): Tracking: tracking
2022-08-04 00:03:02.429260 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb6c4cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb77e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb81cf50>]}
2022-08-04 00:03:02.563951 (MainThread): Partial parsing not enabled
2022-08-04 00:03:02.616108 (MainThread): Parsing macros/core.sql
2022-08-04 00:03:02.649188 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 00:03:02.690853 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 00:03:02.715553 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 00:03:02.844610 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 00:03:02.932877 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 00:03:02.980539 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 00:03:02.990528 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 00:03:03.104053 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 00:03:03.176162 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 00:03:03.221749 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 00:03:03.268598 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 00:03:03.329441 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 00:03:03.570640 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 00:03:03.594468 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 00:03:03.608898 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 00:03:03.635442 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 00:03:03.657954 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 00:03:03.866544 (MainThread): Parsing macros/etc/query.sql
2022-08-04 00:03:03.923557 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 00:03:04.056102 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 00:03:04.103737 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 00:03:04.151113 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 00:03:04.199744 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 00:03:04.268530 (MainThread): Parsing macros/relations.sql
2022-08-04 00:03:04.300049 (MainThread): Parsing macros/adapters.sql
2022-08-04 00:03:04.382814 (MainThread): Parsing macros/catalog.sql
2022-08-04 00:03:04.403645 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 00:03:04.534165 (MainThread): Partial parsing not enabled
2022-08-04 00:03:04.743070 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:04.743503 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:04.844029 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:04.844492 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:04.917629 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:04.918034 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:05.161299 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:05.161731 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:05.923399 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 00:03:06.813589 (MainThread): scipy not found, skipping conversion test.
2022-08-04 00:03:06.849097 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 00:03:06.850167 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 00:03:06.862083 (MainThread): 
2022-08-04 00:03:06.863208 (MainThread): Acquiring new postgres connection "master".
2022-08-04 00:03:06.863580 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:07.066971 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-04 00:03:07.068172 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 00:03:07.494186 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-04 00:03:07.494608 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-04 00:03:07.553122 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.06 seconds
2022-08-04 00:03:07.645953 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:07.646604 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-04 00:03:07.652191 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:07.652720 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 00:03:07.654340 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:07.654762 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:07.655025 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 00:03:07.801865 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.15 seconds
2022-08-04 00:03:07.824132 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 00:03:07.913305 (MainThread): Using postgres connection "master".
2022-08-04 00:03:07.913743 (MainThread): On master: BEGIN
2022-08-04 00:03:07.927992 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 00:03:07.928899 (MainThread): Using postgres connection "master".
2022-08-04 00:03:07.929596 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 00:03:08.052426 (MainThread): SQL status: SELECT 0 in 0.12 seconds
2022-08-04 00:03:08.059099 (MainThread): On master: ROLLBACK
2022-08-04 00:03:08.060629 (MainThread): Using postgres connection "master".
2022-08-04 00:03:08.061132 (MainThread): On master: BEGIN
2022-08-04 00:03:08.064300 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.064986 (MainThread): On master: COMMIT
2022-08-04 00:03:08.065424 (MainThread): Using postgres connection "master".
2022-08-04 00:03:08.065833 (MainThread): On master: COMMIT
2022-08-04 00:03:08.066858 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 00:03:08.067970 (MainThread): 00:03:08 | Concurrency: 1 threads (target='dev')
2022-08-04 00:03:08.069109 (MainThread): 00:03:08 | 
2022-08-04 00:03:08.110342 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 00:03:08.111076 (Thread-1): 00:03:08 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-04 00:03:08.112732 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.113172 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 00:03:08.113554 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 00:03:08.189503 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 00:03:08.209934 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.286875 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.287342 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-04 00:03:08.288710 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.296853 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.297274 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 00:03:08.298313 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.359987 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-04 00:03:08.379263 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.379682 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-04 00:03:08.380710 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.381209 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.381489 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-04 00:03:08.473811 (Thread-1): SQL status: SELECT 6 in 0.09 seconds
2022-08-04 00:03:08.488449 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.488873 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-08-04 00:03:08.492550 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.501767 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.502254 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-08-04 00:03:08.504889 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.511928 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 00:03:08.512736 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.513072 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 00:03:08.535935 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-08-04 00:03:08.542508 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.542964 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 00:03:08.558964 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-08-04 00:03:08.576840 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.579318 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7240227-6985-4167-be19-73547cc79397', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8e3d150>]}
2022-08-04 00:03:08.580144 (Thread-1): 00:03:08 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.47s]
2022-08-04 00:03:08.592870 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 00:03:08.594024 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 00:03:08.594543 (Thread-1): 00:03:08 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-08-04 00:03:08.595587 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.595943 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-08-04 00:03:08.596221 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 00:03:08.636000 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 00:03:08.637140 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.662338 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.662794 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-08-04 00:03:08.664275 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.673156 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.673589 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 00:03:08.675021 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.680966 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-08-04 00:03:08.682229 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.682638 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-08-04 00:03:08.683501 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.683887 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.684160 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 00:03:08.755920 (Thread-1): SQL status: SELECT 922 in 0.07 seconds
2022-08-04 00:03:08.769959 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.770396 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-08-04 00:03:08.772193 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.781043 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.781475 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-08-04 00:03:08.782791 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.785929 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 00:03:08.786360 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.786624 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 00:03:08.792443 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-08-04 00:03:08.799295 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.799718 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 00:03:08.815455 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-08-04 00:03:08.824658 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.827065 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7240227-6985-4167-be19-73547cc79397', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8e55210>]}
2022-08-04 00:03:08.827905 (Thread-1): 00:03:08 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.23s]
2022-08-04 00:03:08.828887 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 00:03:08.829413 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 00:03:08.830723 (Thread-1): 00:03:08 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-08-04 00:03:08.832453 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.832853 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-08-04 00:03:08.833145 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 00:03:08.866347 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 00:03:08.867406 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.898144 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.898628 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-08-04 00:03:08.899822 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.908202 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.908742 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 00:03:08.909827 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.915005 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-08-04 00:03:08.916344 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.916771 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-08-04 00:03:08.917560 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.918167 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.918489 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 00:03:09.728561 (Thread-1): SQL status: SELECT 922 in 0.81 seconds
2022-08-04 00:03:09.742878 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.743300 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-08-04 00:03:09.746172 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:09.755271 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.755708 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-08-04 00:03:09.758490 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:09.761752 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 00:03:09.762187 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.762471 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 00:03:09.931206 (Thread-1): SQL status: COMMIT in 0.17 seconds
2022-08-04 00:03:09.937989 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.938409 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 00:03:09.973229 (Thread-1): SQL status: DROP TABLE in 0.03 seconds
2022-08-04 00:03:09.981935 (Thread-1): finished collecting timing info
2022-08-04 00:03:09.984272 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7240227-6985-4167-be19-73547cc79397', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af9004e10>]}
2022-08-04 00:03:09.985188 (Thread-1): 00:03:09 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 1.15s]
2022-08-04 00:03:09.985625 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 00:03:10.034105 (MainThread): Using postgres connection "master".
2022-08-04 00:03:10.035710 (MainThread): On master: BEGIN
2022-08-04 00:03:10.038760 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:10.039232 (MainThread): On master: COMMIT
2022-08-04 00:03:10.039497 (MainThread): Using postgres connection "master".
2022-08-04 00:03:10.039740 (MainThread): On master: COMMIT
2022-08-04 00:03:10.041782 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 00:03:10.043226 (MainThread): 00:03:10 | 
2022-08-04 00:03:10.043720 (MainThread): 00:03:10 | Finished running 3 table models in 3.18s.
2022-08-04 00:03:10.044068 (MainThread): Connection 'master' was left open.
2022-08-04 00:03:10.044413 (MainThread): On master: Close
2022-08-04 00:03:10.044916 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-08-04 00:03:10.045236 (MainThread): On model.dbt_.fct_trajectory: Close
2022-08-04 00:03:10.086244 (MainThread): 
2022-08-04 00:03:10.087188 (MainThread): Completed successfully
2022-08-04 00:03:10.087972 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-08-04 00:03:10.089147 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb7ea450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8f47f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8f7b450>]}
2022-08-04 00:03:10.089822 (MainThread): Flushing usage events
2022-08-04 00:03:23.630301 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:23.915071 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-04 00:03:23.916000 (MainThread): Tracking: tracking
2022-08-04 00:03:23.933205 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d906c7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8fa3cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8fa3c10>]}
2022-08-04 00:03:24.018117 (MainThread): Partial parsing not enabled
2022-08-04 00:03:24.025272 (MainThread): Parsing macros/core.sql
2022-08-04 00:03:24.046089 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 00:03:24.081920 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 00:03:24.090077 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 00:03:24.217661 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 00:03:24.296729 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 00:03:24.324833 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 00:03:24.333520 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 00:03:24.426159 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 00:03:24.481169 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 00:03:24.505944 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 00:03:24.537586 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 00:03:24.595352 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 00:03:24.811225 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 00:03:24.816076 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 00:03:24.825284 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 00:03:24.835488 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 00:03:24.840070 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 00:03:24.847763 (MainThread): Parsing macros/etc/query.sql
2022-08-04 00:03:24.852972 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 00:03:24.894457 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 00:03:24.898931 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 00:03:24.908107 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 00:03:24.913337 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 00:03:24.919496 (MainThread): Parsing macros/relations.sql
2022-08-04 00:03:24.926356 (MainThread): Parsing macros/adapters.sql
2022-08-04 00:03:24.995623 (MainThread): Parsing macros/catalog.sql
2022-08-04 00:03:25.005939 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 00:03:25.089902 (MainThread): Partial parsing not enabled
2022-08-04 00:03:25.223264 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:25.223696 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:25.296453 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:25.296938 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:25.326050 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:25.326459 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:25.474264 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:25.474687 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:26.249092 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 00:03:27.921233 (MainThread): scipy not found, skipping conversion test.
2022-08-04 00:03:27.939054 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 00:03:27.940087 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 00:03:27.960196 (MainThread): 
2022-08-04 00:03:27.979216 (MainThread): Acquiring new postgres connection "master".
2022-08-04 00:03:27.979630 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:28.241483 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:28.244195 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-04 00:03:28.743156 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:28.743596 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 00:03:28.766867 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.02 seconds
2022-08-04 00:03:28.767348 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:28.767619 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 00:03:28.776413 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-08-04 00:03:28.799316 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 00:03:28.878333 (MainThread): Using postgres connection "master".
2022-08-04 00:03:28.878777 (MainThread): On master: BEGIN
2022-08-04 00:03:28.890073 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 00:03:28.890563 (MainThread): Using postgres connection "master".
2022-08-04 00:03:28.890858 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 00:03:28.919934 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-08-04 00:03:28.922927 (MainThread): On master: ROLLBACK
2022-08-04 00:03:28.925122 (MainThread): 00:03:28 | Concurrency: 1 threads (target='dev')
2022-08-04 00:03:28.925627 (MainThread): 00:03:28 | 
2022-08-04 00:03:28.959618 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:28.961058 (Thread-1): 00:03:28 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-08-04 00:03:28.964072 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:28.965020 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 00:03:28.965847 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 00:03:29.130943 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 00:03:29.175378 (Thread-1): finished collecting timing info
2022-08-04 00:03:29.176727 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:29.177083 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-08-04 00:03:29.178428 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:29.178860 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:29.179121 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-08-04 00:03:29.239116 (Thread-1): SQL status: SELECT 1 in 0.06 seconds
2022-08-04 00:03:29.240090 (Thread-1): finished collecting timing info
2022-08-04 00:03:29.241132 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-08-04 00:03:29.242661 (Thread-1): 00:03:29 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.28s]
2022-08-04 00:03:29.243696 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:29.270418 (MainThread): 00:03:29 | 
2022-08-04 00:03:29.270932 (MainThread): 00:03:29 | Finished running 1 test in 1.29s.
2022-08-04 00:03:29.271270 (MainThread): Connection 'master' was left open.
2022-08-04 00:03:29.271534 (MainThread): On master: Close
2022-08-04 00:03:29.272234 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-08-04 00:03:29.272649 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-08-04 00:03:29.300107 (MainThread): 
2022-08-04 00:03:29.300670 (MainThread): Completed successfully
2022-08-04 00:03:29.301113 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-04 00:03:29.301680 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d668b190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8f84ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8f84150>]}
2022-08-04 00:03:29.302311 (MainThread): Flushing usage events
2022-08-04 00:03:41.339147 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:41.599181 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-08-04 00:03:41.600077 (MainThread): Tracking: tracking
2022-08-04 00:03:41.629661 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9ffe0490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feea0043bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feea0043410>]}
2022-08-04 00:03:41.711655 (MainThread): Partial parsing not enabled
2022-08-04 00:03:41.717589 (MainThread): Parsing macros/core.sql
2022-08-04 00:03:41.738173 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 00:03:41.772808 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 00:03:41.781209 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 00:03:41.907423 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 00:03:41.984827 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 00:03:42.013036 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 00:03:42.021786 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 00:03:42.114197 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 00:03:42.168746 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 00:03:42.191603 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 00:03:42.219304 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 00:03:42.249064 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 00:03:42.425204 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 00:03:42.430102 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 00:03:42.439466 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 00:03:42.449820 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 00:03:42.454278 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 00:03:42.462039 (MainThread): Parsing macros/etc/query.sql
2022-08-04 00:03:42.467094 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 00:03:42.507273 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 00:03:42.511671 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 00:03:42.520894 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 00:03:42.526015 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 00:03:42.532109 (MainThread): Parsing macros/relations.sql
2022-08-04 00:03:42.538861 (MainThread): Parsing macros/adapters.sql
2022-08-04 00:03:42.618579 (MainThread): Parsing macros/catalog.sql
2022-08-04 00:03:42.639037 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 00:03:42.730314 (MainThread): Partial parsing not enabled
2022-08-04 00:03:42.863128 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:42.863589 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:42.933812 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:42.934224 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:42.963572 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:42.963992 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:43.099577 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:43.100002 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:43.740159 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 00:03:44.650372 (MainThread): scipy not found, skipping conversion test.
2022-08-04 00:03:44.662222 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 00:03:44.663323 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 00:03:44.675402 (MainThread): 
2022-08-04 00:03:44.676516 (MainThread): 00:03:44 | Concurrency: 1 threads (target='dev')
2022-08-04 00:03:44.677374 (MainThread): 00:03:44 | 
2022-08-04 00:03:44.703157 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 00:03:44.705440 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:44.706224 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.706938 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 00:03:44.881633 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 00:03:44.882668 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.883757 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.885336 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 00:03:44.887966 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 00:03:44.889275 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:44.889681 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.890000 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 00:03:44.923334 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 00:03:44.924589 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.925632 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.927083 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 00:03:44.927654 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 00:03:44.928854 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:44.929243 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.929532 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 00:03:44.961674 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 00:03:44.962671 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.963630 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.965257 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 00:03:44.965866 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:44.967000 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:44.967378 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.967662 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 00:03:45.024061 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 00:03:45.025247 (Thread-1): finished collecting timing info
2022-08-04 00:03:45.026303 (Thread-1): finished collecting timing info
2022-08-04 00:03:45.027803 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:45.127581 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 00:03:45.128039 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 00:03:45.170008 (MainThread): 00:03:45 | Done.
2022-08-04 00:03:45.203914 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-08-04 00:03:45.204419 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:45.204736 (MainThread): 00:03:45 | Building catalog
2022-08-04 00:03:45.302969 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.303702 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 00:03:45.605491 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.605917 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 00:03:45.618166 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-08-04 00:03:45.618627 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.618918 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 00:03:45.766372 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.15 seconds
2022-08-04 00:03:45.785346 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 00:03:45.810977 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.811397 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 00:03:45.813118 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:45.813555 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.814221 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 00:03:45.818518 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-08-04 00:03:45.825316 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 00:03:46.070469 (MainThread): 00:03:46 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-08-04 00:03:46.071761 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9ffd0810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9d71d090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9d72fd50>]}
2022-08-04 00:03:46.072867 (MainThread): Flushing usage events
2022-08-04 00:03:47.173964 (MainThread): Connection 'generate_catalog' was properly closed.
2022-08-04 00:03:47.175554 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-08-04 00:03:47.175875 (MainThread): On warehouse.information_schema: Close
2022-08-04 00:03:50.118338 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:50.382326 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-08-04 00:03:50.383206 (MainThread): Tracking: tracking
2022-08-04 00:03:50.401390 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63b1a98590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63b1b7c390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63b1a97190>]}
2022-08-04 00:03:50.410986 (MainThread): Serving docs at 0.0.0.0:7211
2022-08-04 00:03:50.411516 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-08-04 00:03:50.411801 (MainThread): Press Ctrl+C to exit.


